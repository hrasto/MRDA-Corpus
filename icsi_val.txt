so uh
yeah.
the suggestion was to have these guys start to
okay.
why don't you go ahead dave?
okay.
um
so yeah the this past week i've been mainly occupied with um getting some results from the s.r.i. system trained on this short hub five training set for the mean subtraction method.
and um
i ran some tests last night.
but um
the results are suspicious.
um it's um because they're the baseline results are worse than um andreas than results andreas got previously.
and it could have something to do with um
that's on digits?
that's on digits.
it it it could it could have something to do with um downsampling.
huh.
that's that's worth looking into.
um
and um
apart from that i guess the the main thing i have i have to talk is um where i'm planning to go over the next week.
um
so i've been working on integrating this mean subtraction approach into the smartkom system.
and there's this question of well so um in my tests before with h.t.k.i. found it worked it worked the best with about twelve seconds of data used to estimate the mean.
but we'll often have less in the smartkom system.
um
so i think we'll use as much data as we have at a particular time.
and we'll we'll concatenate utterances together um to get as much data as we possibly can from the user.
but um there's a question of how to set up the models so um we could train the models.
if we think twelve seconds is ideal we could train the models using twelve seconds to calculate the mean to mean subtract the training data.
or we could um use some other amount.
so like i did an experiment where i um was using six seconds in test.
um but for i tried twelve seconds in train.
and i tried um um the same in train.
i'm i tried six seconds in train.
and six seconds in train was about point three percent better.
um and um it's not clear to me yet whether that's something significant.
so i want to do some tests and um actually make some plots of um for a particular amount of data and test what happens if you vary the amount of data in train.
uhhuh.
uh guenter i don't know if you followed this stuff but this is uh a uh uh long-term long-term window f.f.t.'s.
yeah.
yeah.
yeah he you talked about it.
we we spoke about it already.
oh okay.
yeah.
so you know what he's doing.
all right.
so i was i actually ran the experiments mostly.
and i i was i was hoping to have the plots with me today.
i just didn't get to it.
but um
yeah i i would be curious about people's feedback on this.
because i'm i i think there are some i think it's it's kind of like a a bit of a tricky engineering problem.
i'm trying to figure out what's the optimal way to set this up.
so um i'll try to make the plots and then put some postscript up on my on my web page.
and i'll mention it in my status report if people want to take a look.
you could clarify something for me.
you're saying point three percent.
you take a point three percent hit when the training and testing links are don't match or something?
is that what it is?
or
well it
i i don't think it it's just for any mismatch you take a hit.
yeah.
in some cases it might be better to have a mismatch.
yeah.
like i think i saw something like like if you only have two seconds in test or um maybe it was something like four seconds you actually do a little better if you um train on six seconds than if you train on four seconds.
right.
um
but the case uh with the point three percent hit was using six seconds in test um comparing train on twelve seconds versus train on six seconds.
and which was worse?
the train on twelve seconds.
okay.
but point three percent uh from what to what?
that's point three percent.
on the the the accuracies went from it was something vaguely like ninety five point six accuracy um improved to ninety five point nine when i
so four point four to four point one.
okay.
so yeah.
so about a about an eight percent uh seven or eight percent relative.
okay.
uh
yeah.
well i think in a you know if if you were going for an evaluation system you'd care.
but if you were doing a live system that people were actually using nobody would notice.
it's uh i think the thing is to get something that's practical that that you could really use.
huh.
that's that's interesting.
all right the uh i see your point.
i guess i was thinking of it as um an interesting research problem.
yeah.
the how to i was thinking that for the a.s.r.u. paper we could have a section saying for smartkom we we in we tried this approach in uh interactive system which i don't think has been done before.
uhhuh.
uhhuh.
and and then there was two research questions from that.
and one is the does it still work if you just use the past history.
uhhuh.
all right.
and the other was this question of um what i was just talking about now.
so i guess that's why i thought it was interesting.
i mean a short time f.f.t. short time cepstrum calculation uh mean mean calculation work that people have in commercial systems they do this all the time.
they the they calculate it from previous utterances and then use it you know.
yeah.
um
but but uh
as you say there hasn't been that much with this long longtime uh spectra work.
uh
oh oh okay.
so that's that's that's standard.
um
yeah.
pretty common.
yeah.
okay.
um
but uh
yes.
no it is interesting.
and the other thing is i mean there's two sides to these really small uh gradations in performance.
um i mean on the one hand in a practical system if something is uh four point four percent error four point one percent error people won't really tell be able to tell the difference.
on the other hand when you're doing uh research you may uh you might find that the way that you build up a change from a ninety five percent accurate system to a ninety eight percent accurate system is through ten or twelve little things that you do that each are point three percent.
so so the they they it's i don't mean to say that they're they're irrelevant.
uh they are relevant.
but um for a demo you won't see it.
uhhuh.
right.
okay.
yeah.
and um
let's let's see.
um
okay.
and then there's um another thing i want to start looking at um is um the choice of the analysis window length.
so i've just been using two seconds.
just because that's what carlos did before.
uh i wrote to him asking about he chose the two seconds.
and it seemed like he chose it a bit informally.
so um
with the with the h.t.k. set-up i should be able to do some experiments on just varying that length.
say between one and three seconds in a few different reverberation conditions.
um say this room and also a few of the artificial impulse responses we have for reverberation.
just um making some plots and seeing how they look.
and um
so
with the the sampling rate i was using one second or two seconds or four seconds is at a power of two um number of samples.
and um i'll i'll for the ones in between i guess i'll just zero pad.
uhhuh.
i guess one thing that might also be an issue uh because part of what you're doing is you're getting a a spectrum over a bunch of different kinds of speech sounds.
um
and so it might matter how fast someone was talking for instance.
oh.
you know if you if if if there's a lot of phones in one second maybe you'll get a a really good sampling of all these different things.
and and uh on the other hand if someone's talking slowly maybe you'd need more.
so
i don't know if you have some samples of faster or slower speech.
huh.
but it might make a difference.
i don't know.
uh yeah.
i don't i don't think the t.i. digits data that i have um is would be appropriate for that.
yeah probably not.
yeah.
but what do you what about if i i fed it through some kind of um speech processing algorithm that changed the speech rate?
yeah but then you'll have the degradation of of uh whatever you do uh added onto that.
but maybe.
yeah.
maybe if you get something that sounds that that's does a pretty job at that.
yeah.
well uh just if you think it's worth looking into.
you could imagine that.
i mean it it is getting a little away from reverberation.
um yeah.
it's just that you're making a choice.
uh i was thinking more from the system aspect if you're making a choice for smartkom that that that it might be that it's it the optimal number could be different depending on
yeah.
right.
could be.
i don't know.
and and the third thing um uh is um barry explained l.d.a. filtering to me yesterday.
and so um mike shire in his thesis um did a a series of experiments um training l.d.a. filters in on different conditions.
and you were interested in having me repeat this for
for this mean subtraction approach?
is is that right?
or for these long analysis windows i guess is the right way to put it.
i guess the the the issue i was the general issue i was bringing up was that if you're have a moving moving window uh a a a set of weights times things that uh move along shift along in time that you have in fact a linear time invariant filter.
and you just happened to have picked a particular one by setting all the weights to be equal.
and so the issue is what are some other filters that you could use uh in that sense of filter.
uhhuh.
and um
as i was saying i think the simplest thing to do is not to train anything but just to do some sort of uh uh hamming or hanning uh kind of window kind of thing.
right.
uhhuh.
just sort of to de emphasize the jarring.
so i think that would sort of be the first thing to do.
but then yeah the l.d.a. uh is interesting because it would sort of say well suppose you actually trained this up to do the best you could by some criterion.
what would the filter look like then?
uhhuh.
uh
and um
that's sort of what we're doing in this aurora stuff.
and uh
it's still not clear to me in the long run whether the best thing to do would be to do that or to have some stylized version of the filter that looks like these things you've trained up because you always have the problem that it's trained up for one condition and it isn't quite right for another.
so
uh that's that's why that's why rasta filter has actually ended up lasting a long time.
people still using it quite a bit because you don't change it.
so
doesn't get any worse.
uh
huh.
anyway.
okay.
so um
actually i was just thinking about what i was asking about earlier which is about having less than say twelve seconds in the smartkom system to do the mean subtraction.
you said in systems where you use cepstral mean subtraction they concatenate utterances.
and do you know how they address this issue of um testing versus training?
can
i think what they do is they do it always online.
go ahead.
i mean that you just take what you have from the past.
that you calculate the mean of this and subtract the mean.
okay.
um
and then you can yeah you you can increase your window while you get while you are getting more samples.
okay.
um
and um so so in in that case what do they do when they're um performing the cepstral mean subtraction on the training data?
so because you'd have hours and hours of training data.
so do they cut it off and start over?
at intervals.
or
so do you have uh you you mean you have files which are hours of hours long?
or
oh well no.
i guess not.
yeah.
but
i mean usually you have in the training set you have similar conditions.
i mean file lengths are i guess the same order or in the same size as for test data or
aren't they?
okay.
but it's
okay.
so if someone's interacting with the system though uh morgan uh morgan said that you would tend to um chain utterances together.
um
well i think what i was i thought what i was saying was that um at any given point you are going to start off with what you had from before.
oh.
from
and so if you're splitting things up into utterances.
so for instance in a dialogue system where you're going to be asking uh you know for some information there's some initial something.
and you know the first time out you you might have some general average.
but you you you don't have very much information yet.
but at after they've given one utterance you've got something.
you can compute your mean cepstra from that.
uhhuh.
and then can use it for the next thing that they say.
uh
so that you know the performance should be better that second time.
um
and i think the heuristics of exactly how people handle that and how they handle their training i'm sure vary from place to place.
but i think the ideally it seems to me anyway that you you would want to do the same thing in training as you do in test.
but that's that's just uh a prejudice.
and i think anybody working on this with some particular task would experiment.
right.
i i guess the question i had was um amount of data was the amount of data that you'd give it to um update this estimate.
because say you if you have say five thousand utterances in your training set um and you you keep the mean from the last utterance.
by the time it gets to the five thousandth utterance
no but those are all different people with different i mean in
so for instance in in the in a telephone task these are different phone calls.
so you don't want to chain it together from a from a different phone call.
okay.
so so so they would
so it's within speaker.
yeah.
within phone call.
if it's a dialogue system it's within whatever this characteristic you're trying to get rid of is expected to be consistent over.
huh.
and it
right?
right.
okay.
so you'd you and so in training you would start over at at every new phone call or at every new speaker.
yeah.
yeah.
okay.
yeah.
now you know maybe you'd use something from the others.
just because at the beginning of a call you don't know anything.
and so you might have some kind of general thing that's your best guess to start with.
but
so i i
you know a lot of these things are proprietary.
so we're doing a little bit of guesswork here.
i mean what do what do people do who really face these problems in the field?
well they have companies.
and they don't tell other people exactly what they do.
but but i mean when you the the hints that you get from what they when they talk about it are that they do they all do something like this.
right.
right.
okay.
i see.
because i so this smartkom task first off it's this t.v. and movie information system.
yeah but you might have somebody who's using it.
and
and then later you might have somebody else who's using it.
yeah.
yeah.
and so you'd want to set some
right.
right.
i i see.
i was i was about to say so if if you ask it what what movies are on t.v. tonight.
yeah.
yeah.
if i look at my wristwatch when i say that it's about two seconds.
yeah.
the way i currently have the mean subtraction um set up the the analysis window is two seconds.
so what you just said about what do you start with raises a question of what do i start with then.
uhhuh.
i guess it because
well.
okay.
so in that situation though maybe what's a little different there is i think you're talking about there's only one
it it it also depends
we're getting a little off track here.
oh.
but but but
right.
uh there's been some discussion about whether the work we're doing in that project is going to be for the kiosk or for the mobile or for both.
and i think for this kind of discussion it matters.
if it's in the kiosk then the physical situation is the same.
it's going to you know the exact interaction of the microphone's going to differ depending on the person and so forth.
but at least the basic acoustics are going to be the same.
so if it's really in one kiosk then i think that you could just chain together and and you know as much as much speech as possible to
because what you're really trying to get at is the is the reverberation characteristic.
yeah.
but in in the case of the mobile uh presumably the acoustic's changing all over the place.
right.
and in that case you probably don't want to have it be endless because you want to have some sort of it's it's not a question of how long do you think it's you can get an approximation to a stationary something given that it's not really stationary.
right.
huh.
right.
so
and i i i guess i just started thinking of another question.
which is for for the very first frame what what do i do.
if i'm if i take if i use that frame to calculate the mean then i'm just going to get nothing.
uhhuh.
right.
um
so i should probably have some kind of default mean for the first couple of frames.
yeah.
yeah.
okay.
yeah.
or subtract nothing.
i mean it's
or subtract nothing.
and and that's that's i guess that's something that's people have figured out how to deal with in cepstral mean subtraction as well?
yeah yeah.
yeah people do something.
they they uh they have some um
uh
in in cepstral mean subtraction for short-term window analysis windows as is usually done you're trying to get rid of some very general characteristic.
and so uh if you have any other information about what a general kind of characteristic would be then you you can do it there.
you can also
you can also reflect the data.
so you take uh
you know.
i'm not sure how many frames you need.
but you take that many from the front and flip it around to as the negative value.
uhhuh.
yeah.
that's
so you can always
yeah.
the other thing is that and and i i remember b.b.n. doing this is that if you have a multi pass system um if the first pass it takes most of the computation the second and the third pass could be very very quick.
just looking at a relatively small small uh space of hypotheses.
huh.
uhhuh.
then you can do your first pass without any subtraction at all.
oh.
and then your second pass uh uh eliminates those most of those hypotheses by uh by having an improved improved version of the analysis.
okay.
okay.
so
okay.
so that was all i had for now.
yeah.
do you want to go barry?
yeah.
okay.
um so for the past uh week or two i've been just writing my uh formal thesis proposal.
um so i'm taking this qualifier exam that's coming up in two weeks.
and i i finish writing a proposal and submit it to the committee.
um
and uh should i should i explain uh more about what what i'm proposing to do and and stuff?
yes briefly.
yeah briefly.
okay.
um
so briefly i'm proposing to do a a new approach to speech recognition using um a combination of uh multi band ideas and ideas um about the uh acoustic phonetic approach to speech recognition.
um so i will be using these graphical models that um that implement the multi band approach to recognize a set of intermediate categories that might involve uh things like phonetic features or other other feature things that are more closely related to the acoustic signal itself.
um and the hope in all of this is that by going multi band and by going into these um intermediate classifications that we can get a system that's more robust to to unseen noises and situations like that.
um
and so
some of the research issues involved in this are um one what kind of intermediate categories do we need to classify.
um another one is um what what other types of structures in these multi band graphical models should we consider in order to um combine evidence from the sub bands.
and uh the third one is how do we how do we merge all the uh information from the individual uh multi band classifiers to come up with word word recognition or or phone recognition things.
um
so basically that's that's what i've been doing.
so you've got two weeks huh?
and
i got two weeks to brush up on um presentation stuff.
and um
oh i thought you were finishing your thesis in two weeks.
but
oh that too.
yeah.
yeah.
are you going to do any dry runs for your thing?
or are you just going to
yes.
yes.
i um i'm i'm going to do some.
would you be interested?
sure.
to help out.
sure.
okay.
thanks.
yeah.
is that it?
that's it.
okay.
uh hhh.
let's see.
so we've got forty minutes left.
and it seems like there's a lot of material.
any suggestions about where we where we should go next?
huh.
uh
do you want to go sunil?
maybe we'll just start with you.
yeah.
but i actually stuck most of this in our last meeting with guenter.
um
but i'll just
um
so the last week uh i showed some results with only speechdat-car.
which was like some fifty six percent.
and uh i didn't
i mean i i found that the results
i mean i wasn't getting that results on the t.i. digit.
so i was like looking into why what is wrong with the t.i. digits.
why why i was not getting it.
and i found that the noise estimation is a reason for the t.i. digits to perform worse than the baseline.
so uh i actually picked
i mean the first thing i did was i just scaled the noise estimate by a factor which is less than one to see if that because i found there are a lot of zeros in the spectrogram for the t.i. digits when i used this approach.
so the first thing i did was i just scaled the noise estimate.
and i found
so the the results that i've shown here are the complete results using the new
well the the new technique is nothing but the noise estimate scaled by a factor of point five.
so it's just an ad hoc.
i mean some intermediate result because it's not optimized for anything.
so the results the trend the only trend i could see from those results was like the the the current noise estimation or the uh noise composition scheme is working good for like the car noise type of thing.
because i've the only only very good result in the t.i. digits is the noise car noise condition for their test a..
which is like the best i could see that
uh for any non stationary noise like babble or subway or any street some restaurant noise it's like it's not performing very well.
so
the so that that's the first thing i uh i could make out from this stuff.
and
yeah i think what is important to see is that there is a big difference between the training modes.
uhhuh.
yeah.
if you have clean training you get also a fifty percent improvement.
yeah.
but if you have muddy condition training you get only twenty percent.
yeah.
yeah.
uhhuh.
uhhuh.
uh and in that twenty percent it's very inconsistent across different noise conditions.
huh.
so i have like a forty five percent for car noise.
and then there's a minus five percent for the babble.
huh.
and there's this thirty three for the station.
and so it's it's not it's not actually very consistent across.
so
the only correlation between the speechdat-car and this performance is the stationarity of the noise that is there in these conditions and the speechdat-car.
uhhuh.
and uh
so
so the overall result is like in the last page.
which is like forty seven.
which is still very imbalanced because there are like fifty six percent on the speechdat-car and thirty five percent on the t.i. digits.
and
uh
the fifty six percent is like comparable to what the french telecom gets.
but the thirty five percent is way off.
i'm sort of confused.
but
this
i'm looking on the second page.
oh.
yep.
and it says fifty percent.
looking in the lower right-hand corner.
fifty percent relative performance.
for the clean training.
is that
and if you if you look
is that fifty percent improvement?
yeah.
yeah.
for that's for the clean training and the noisy testing for the t.i. digits.
so it's improvement over the baseline mel cepstrum.
yeah.
yeah.
but the baseline mel cepstrum under those training doesn't do as well.
i i'm i'm trying to understand why it's it's eighty percent.
that's an accuracy number i guess.
yeah yeah yeah.
right?
so that's not as good as the one up above.
no.
but the fifty is better than the one up above.
yeah.
so i'm confused.
uh actually the noise compensation whatever uh we are put in it works very well for the high mismatch condition.
i mean it's consistent in the speechdat-car.
and in the clean training also it gives it
but this fifty percent is is that the the high mismatch performance equivalent to the high mismatch performance in the speech.
so so since the high mismatch performance is much worse to begin with it's easier to get a better relative improvement.
yeah.
yeah.
i do.
yeah yeah.
so by putting this noise.
yeah.
yeah if we look at the figures on the right we see that the reference system is very bad.
oh.
yeah.
the reference drops like a very fast.
oh oh oh oh oh oh.
like for clean clean training condition.
i see.
yeah.
i see.
nnn.
this is this is t.i. digits we're looking at?
yeah.
this whole page is t.i. digits?
yeah.
oh.
or this is
oh yeah.
it's not written anywhere.
yeah it's t.i. digits.
the first spreadsheet is t.i. digits.
huh.
huh.
how does clean training do for the uh car?
stuff?
the car?
oh.
still it still uh that that's still consistent.
i mean i get the best performance in the case of car which is the third column in the a. condition.
no.
i mean this is added noise.
i mean this is t.i. digits.
i'm sorry.
i meant in in the in the uh multi language uh uh finnish and
uh
this is next next page.
that's the next next spreadsheet is.
huh.
so that is the performance for italian finnish and spanish.
training condition.
oh right.
so clean corresponds to high mismatch.
yeah.
and increase
that's increase
improvement.
improvement.
yeah.
that's percentage increase is the percentage improvement over the baseline.
it's it's a
so that's
which means decrease in word error rate?
yeah.
okay.
so percentage increase means decrease.
okay.
yeah yeah.
yeah.
the the there was a very long discussion about this on on the on the uh amsterdam meeting.
yeah.
how to how to calculate it then.
yeah.
there's there's a
i i i guess you are using finally this the scheme which they
which is there in the spreadsheet.
i'm not changing anything in there.
okay.
huh.
all right.
so
uh
yeah.
so all the h.m. numbers are very good.
in the sense they are better than what the french telecom gets.
so
but the the only number that's still i mean which stephane also got in his result was that medium mismatch of the finnish.
which is very which is a very strange situation where we used the we changed the proto for initializing the h.m.m.
i mean this this is basically because it gets stuck in some local minimum in the training.
that seventy five point seven nine in the finnish mismatch which is that the eleven point nine six what we see.
uhhuh.
huh.
yeah.
so we have to jiggle it somehow?
yeah.
so we start with that different proto and it becomes eighty eight.
which is like some fifty percent improvement.
wait a minute.
start with a different what?
different prototype.
which is like a different initialization for the uh transition probabilities.
it's just that right now the initialization is to stay more in the current state.
which is point four point six right?
yeah.
yeah.
and if it changes to point five point five which is equal for transition and self loop where it becomes eighty eight percent.
well but that involves mucking with the back end.
which is not allowed.
yeah.
we can't do it.
yeah.
yeah.
huh.
so
i mean it uh like
it is well known this this medium match condition of the finnish data has some strange effects.
very
yeah.
it has a very few at uh actually uh i mean words also.
i mean that is
yeah.
that too.
it's a very very small set actually.
yeah.
uhhuh.
there is a a there is a lot of uh there are a lot of utterances with music in with music in the background.
so there is
yeah.
yeah yeah yeah.
huh.
yeah.
uhhuh.
yeah.
it has some music also.
i mean very horrible music like like 4x.
i know.
so maybe for that one you need a much smarter v.a. d?
huh
if it's music.
uh
so
that that's the that's about the results.
and uh
the summary is like
okay.
so there are the other thing what i tried was which i explained in the last meeting is using the channel zero for uh for both dropping and estimating the noise.
and that's like just to get a feel of how good it is.
i guess the fifty six percent improvement in the speechdat-car becomes like sixty seven percent.
like ten percent better.
but that's that's not a that's a cheating experiment.
so
that's just
so
but the but the uh forty seven point nine percent which you have now that's already a remarkable improvement in comparison to the first proposal.
yeah.
so we had forty four percent in the first proposal.
okay.
uhhuh.
yeah.
we have a big
so the major improvement that we got was in all the high mismatch cases.
because all those numbers were in sixties and seventies.
because we never had any noise compensations.
huh.
so that's where the biggest improvement came up.
not much in the well match and the medium match and t.i. digits also right now.
so this is still at three or four percent improvement over the first proposal.
huh.
huh.
yeah so that's good.
yeah.
so
then if we can improve the noise estimation then it should get better.
yeah.
i i started thinking about also
i mean yeah uh i discovered the same problem when i started working on uh on this aurora task almost two years ago.
that you have the problem with this
at the beginning we had only this condition training of the t.i. digits.
yeah.
and uh i i found the same problem.
just taking um what we were used to use i mean uh some type of spectral subtraction you get even worse results than the basis.
yeah.
and uh
yeah.
yeah.
i i tried to find an explanation for it.
so
huh.
so
yes.
stephane also has the same experience of using the spectral subtraction right?
huh.
uhhuh.
yeah.
yeah.
so here here i mean i found that it's if i changed the noise estimate i could get an improvement.
so that's so it's something which i can actually pursue is the noise estimate.
uhhuh.
and
yeah i think what you do is in when when you have the the this multi condition training mode um then you have then you can train models for the speech for the words as well as for the pauses where you really have all information about the noise available.
yeah.
and
it was surprising.
at the beginning it was not surprising to me that you get really the best results on doing it this way.
i mean in comparison to any type of training on clean data and any type of processing.
but it was
so
it it seems to be the best what what what we can do in this moment is multi condition training.
and when we now start introducing some some noise reduction technique we we introduce also somehow artificial distortions.
yeah.
and these artificial distortions uh i have the feeling that they are the reason why why we have the problems in this multi condition training.
that means the h.m.m.'s we trained they are they are based on gaussians.
yeah.
and on modeling gaussians.
and if you
can i move a little bit with this?
yeah.
and if we introduce now this this spectral subtraction or wiener filtering stuff
so usually what you have is maybe um
i'm i'm showing now an envelope.
um
maybe you'll for this time.
so usually you have maybe in clean condition you have something which looks like this.
and if it is noisy it is somewhere here.
and then you try to subtract it or wiener filter or whatever.
and what you get is you have always these problems that you have this these these these zeros in there.
yeah.
and you have to do something if you get these negative values.
i mean this is your noise estimate and you somehow subtract it or do whatever.
uh and then you have
and then i think what you do is you introduce some some artificial distribution in this.
uh
in in the models.
i mean you you train it also this way.
but somehow there is there is no longer a a gaussian distribution.
it is somehow a strange distribution which we introduce with these artificial distortions.
and and i was thinking that that might be the reason why you get these problems in the especially in the multi condition training mode.
uhhuh.
yeah yeah.
yeah.
the the models are not complex enough to absorb that additional variability that you're introducing.
thanks adam.
yeah.
yes.
well that's
yeah.
so
i also have the feeling that um the reason why it doesn't work is yeah that the models are much are um not complex enough.
because i actually i always had a good experience with spectral subtraction.
just a straight spectral subtraction algorithm when i was using neural networks big neural networks which maybe are more able to model strange distributions.
uhhuh.
and
but
yeah.
then i tried the same exactly the same spectral subtraction algorithm on these aurora tasks.
and it simply doesn't work.
huh.
it's even it uh hurts even.
so
we probably should at some point here try the tandem the the the system two kind of stuff with this with the spectral subtraction for that reason.
huh.
uhhuh.
because again it should do a transformation to a domain where it maybe looks more gaussian.
uhhuh.
huh.
yeah.
i i was
just yesterday when i was thinking about it um what what we could try to do or do about it
i mean if you if you get at this in this situation that you get this this negative values and you simply set it to zero or to a constant or whatever if we if we would use there a somehow um a random generator which which has a certain distribution not a certain yeah a special distribution we should see we we have to think about it.
it's
and that we so introduce again some natural behavior in this trajectory.
uhhuh.
uhhuh.
very different from speech.
still i mean it shouldn't confuse the
yeah i mean similar to what what you see really in in the real um noisy situation.
okay.
uhhuh.
or in the clean situation.
but but somehow a a natural distribution.
but isn't that again sort of the idea of the additive thing?
if it as as we had in the j. stuff.
i mean basically if if you have random data um in in the time domain then when you look at the spectrum it's going to be pretty flat.
uhhuh.
and and
uh
so just add something everywhere rather than just in those places.
it's just a constant right?
uhhuh.
yeah.
i think
yeah.
it's it's just especially in these segments.
i mean you introduce um very artificial behavior.
yeah.
and
yeah.
well see if you add something everywhere it has almost no effect up up up on on top.
uhhuh.
and it and it and it has significant effect down there.
that was sort of the idea.
uhhuh.
huh.
yeah.
the that's true.
that those those regions are the cause for this those negative values or whatever you get.
uhhuh.
uhhuh.
yeah.
so
i mean we we could uh we we could think how what what we could try.
yeah.
i mean it it was just an idea.
yeah yeah.
uhhuh.
i mean we
i think when it's noisy people should just speak up.
to
huh.
so
if we look at the france telecom proposal they use some kind of noise addition.
they have a random number generator right?
and they add noise on the trajectory of uh the log energy only right?
oh they do.
oh.
yep.
c.c. zero and log energy also.
yeah.
yeah.
um
but i don't know how much effect it this have.
but they do that.
now?
yeah.
uhhuh.
oh.
huh.
so it it it it it is somehow similar to what
i think because they have log energy.
yeah.
and then just generate random number.
they have some kind of mean and variance.
and they add this number to to the log energy simply.
um
to the
yeah.
the the log energy the after the clean cleaning up.
uhhuh.
so they add a random random noise to it.
to the just the energy or to the mel uh to the mel filter?
no.
only to the log energy.
only
yeah.
oh.
uhhuh.
so it because i mean i think this is most interesting for the mel filters.
uhhuh.
right?
or or f.f.t.'s.
one or the other.
but but they do not apply filtering of the log energy or what
like uh i mean
like like a spectral subtraction.
or
no their filter is not m. domain.
yeah.
so they did filter their time signal.
i
and then what
and then they calculate from this the log energy.
or
yeah.
then after that it is almost the same as the baseline system.
uhhuh.
and then the final log energy that they that they get that to the to that they add some random noise.
yeah but again that's just log energy as opposed to filter bank energy.
yeah.
huh.
so it's not the mel.
you know it's not the mel filter bank output.
yeah.
uhhuh.
these are log energy computed from the time domain signal.
uhhuh.
not from the mel filter banks.
so
huh.
maybe it's just a way to decrease the importance of this particular parameter in the in the world feature vector.
did
if you add noise to one of the parameters you widen the distributions.
huh.
becomes flat.
the variance yeah reduces.
and
so
huh yeah.
eee-sss-uh.
so it could reduce the dependence on the amplitude and so on.
yeah?
yeah.
yeah.
although
maybe.
so is uh is that about it?
uhhuh.
uh
or
so the
okay.
so the other thing is the i'm just looking at a little bit on the delay issue where the delay of the system is like a hundred and eighty millisecond.
so i just just tried another system i mean another filter which i've like shown at the end.
which is very similar to the existing uh filter.
only uh only thing is that the phase is is like a totally nonlinear phase.
because it's a it's not a symmetric filter anymore.
this is for the l.d. a?
yeah.
so so this this is like so this makes the delay like zero for l.d.a.
because it's completely causal.
oh.
so
so i got actually just the results for the italian for that.
and that's like
so the fifty one point o. nine has become forty eight point o. six.
which is like three percent relative degradation.
uhhuh.
so i have like the fifty one point o. nine.
and
so
i don't know it fares for the other conditions.
so it's just like it's like a three percent relative degradation.
with the
but but is there is there a problem with the one hundred eighty milliseconds?
or
well this is
uh
yeah i mean i talked to to uh i uh i talked uh about it with with hynek.
i mean there is
this is
so so basically our our position is that um we shouldn't be unduly constraining the latency at this point.
because we're all still experimenting with trying to make the performance better in the presence of noise.
uh there is a minority in that group who is arguing who are arguing for um uh having a further constraining of the latency.
so we're just continuing to keep aware of what the tradeoffs are and you know what what do we gain from having longer or shorter latencies.
huh.
but since we always seem to at least get something out of longer latencies not being so constrained we're tending to go with that if we're not told we can't do it.
uhhuh.
what where was the um the smallest latency of all the systems last time?
the french telecom.
well france telecom was was was very short latency.
it's
and they had a very good result.
what what was it?
it was thirty five.
it was in the order of thirty milliseconds.
or
yeah.
thirteen?
thirty.
thirty.
thirty four.
yeah.
yeah.
yeah so it's possible to get very short latency.
but again we're the the approaches that we're using are ones that take advantage of
yeah.
i was just curious about where we are compared to you know the shortest that people have done.
but but i think this thirty milliseconds they they did it did not include the the delta calculation.
yeah.
and this is included now.
yeah.
yeah.
yeah.
you know?
yeah.
so if they include the delta it will be an additional forty millisecond.
uhhuh.
yeah.
yeah.
i i don't remember the
they were not using the h.t.k. delta?
no they're using a nine point window.
which is like a four on either side.
nine point.
which is like
okay.
so.
huh.
they didn't include that.
yeah.
uhhuh.
so
okay.
where does the compression in decoding delay comes from?
that's the way the the the frames are packed.
like you have to wait for one more frame to pack.
because it's the c.r.c. is computed for two frames always.
uhhuh.
well that they would need that forty milliseconds also.
right?
no.
they actually changed the compression scheme altogether.
uhhuh.
so they have their own compression and decoding scheme.
and they i don't know what they have.
oh.
but they have coded zero delay for that.
because they i know they changed it.
their compression.
they have their own c.r.c.
their their own error correction mechanism.
oh.
so they don't have to wait more than one more frame to know whether the current frame is in error.
oh okay.
so they changed the whole thing so that there's no delay for that compression and part also.
huh.
uhhuh.
even you have reported actually zero delay for the compression.
i thought maybe you also have some different
huh.
huh.
no i think i i used this scheme as it was before.
okay.
uh.
uhhuh.
okay.
we've got twenty minutes.
so we should probably try to move along.
uh did you want to go next stephane?
i can go next.
yeah.
huh.
oh.
it's
wait a minute.
it's
yeah.
we have to take
wait a minute.
i think i'm confused.
well.
okay.
all right.
so you have one sheet.
this one is you don't need it.
all right.
uh
so you have to take the whole the five.
there should be five sheets.
okay.
i have four now.
because i left one with dave because i thought i was dropping one off and passing the others on.
so no.
we're not.
okay.
thanks.
please give me one.
uh we need one more over here.
okay maybe there's not enough for everybody.
i can share with barry.
but
yeah.
okay.
oh okay.
can we look at this?
yeah.
so
yeah there are two figures showing actually the huh um performance of the current v.a.d.
so it's a neural network based on p.l.p. parameters.
uh which estimate silence probabilities.
and then i just put a median filtering on this.
to smooth the probabilities right?
um
i didn't use the the scheme that's currently in the proposal.
because i don't want to.
in the proposal
well in in the system we want to add like speech frame before every word and a little bit of of uh a couple of frames after also.
uh but to estimate the performance of the v.a.d. we don't want to do that.
because it would artificially increase the um the false alarm rate of speech detection.
right?
um
so
there is normally a figure for the finnish and one for italian.
and maybe someone has two for the italian.
because i'm missing one figure here.
no.
well.
well whatever.
uh yeah so one surprising thing that we can notice first is that apparently the speech miss rate is uh higher than the false alarm rate.
so
so so what is the lower curve and the upper curve?
it means
uhhuh.
yeah there are two curves.
yeah.
one curve's for the close talking microphone which is the lower curve.
uh okay.
and the other one is for the distant microphone.
which has more noise.
so
it's logical that it performs worse.
so as i was saying the miss rate is quite important.
uh which means that we tend to label speech as as a silence.
and
uh i didn't analyze further yet.
but i think it's it may be due to the fricative sounds.
which may be in noisy condition maybe label labelled as silence.
and it may also be due to the alignment.
because
well the reference alignment.
because right now i just use an alignment obtained from from a system trained on channel zero.
and
i checked it a little bit.
but there might be alignment errors.
um yeah.
like the fact that the the models tend to align their first state on silence and their last state on silence also.
so the reference reference alignment would label as speech some silence frame before speech and after speech.
this is something that we already noticed before.
when
huh
so this this could also explain uh the high miss rate maybe.
and and this this curves are the average over the whole database.
uh
so
yeah.
right.
huh.
um
yeah.
and the different points of the curves are for five uh thresholds on the probability uh from point three to point seven.
so that threshold
uhhuh.
yeah.
okay.
okay.
so the detection threshold is very
so the
the v.a. d?
yeah yeah.
yeah.
there first a threshold on the probability that puts all the values to zero or one.
huh.
and then the median filtering.
yeah.
so the median filtering is fixed.
you just change the threshold.
yeah.
it's fixed.
yeah.
yeah.
uhhuh.
so going from channel zero to channel one uh almost double the error rate.
um
yeah.
well so it's a reference performance that we can you know if we want to to work on the v.a.d. we can work on this basis.
uhhuh.
and
okay.
is this is this v.a.d.a.m.l. p?
yeah.
okay.
how how big is it?
it's a very big one.
i don't remember.
so three three hundred and fifty inputs.
uh
six thousand hidden nodes and two outputs.
okay.
yeah.
uhhuh.
middle sized one.
yeah.
uhhuh.
yeah.
uh ppp
i don't know you have questions about that or suggestions?
huh.
so.
it seems the performance seems worse in finnish.
which
well it's not trained on finnish.
uh
it's worse.
it's not trained on finnish.
yeah.
what's it trained on?
i mean the m.l.p.'s not trained on finnish.
right.
what's it trained on?
oh oh.
sorry.
uh it's italian t.i. digits.
yeah.
oh it's trained on italian?
yeah.
yeah.
uhhuh.
okay.
and
that's right.
okay.
and also there are like funny noises on finnish.
more than on italian.
uhhuh.
i mean like music.
yeah.
and um
yeah the yeah it's true.
so yeah.
we were looking at this.
but for most of the noises noises are
um
i don't know if we want to talk about that.
but
well the the car noises are below like five hundred hertz.
and we were looking at the music utterances.
and in this case the noise is more about two thousand hertz.
well the music energy's very low apparently.
yeah.
uh
uh from zero to two two thousand hertz.
so maybe just looking at this frequency range for from five hundred to two thousand would improve somewhat the v.a.d.
huh.
and
yeah.
huh.
so there are like some some some parameters you wanted to use or something?
yeah.
but
yes.
or yeah.
uhhuh.
uh the next
um
so is the is the is the training is the training based on these labels files which you take as reference here?
oh it's there.
when you train the neural net you
yeah.
no.
it's not.
it's it was trained on some alignment obtained
um
uh for the italian data i think we trained the neural network on with embedded training.
so re estimation of the alignment using the neural network i guess.
that's right?
yeah.
we actually trained uh the on the italian training part.
yeah.
we we had another system with
so it was a a phonetic classification system for the italian aurora data.
yeah.
it must be somewhere.
yeah.
for the aurora data that it was trained on it was different.
like for t.i. digits you used a a previous system that you had i guess.
what
no it
yeah yeah.
that's true.
so the alignments from the different database that are used for training came from different system.
yeah.
then we put them together.
well you put them together and trained the v.a.d. on them.
yeah.
huh.
yeah.
huh.
uh
but did you use channel did you align channel one also?
or
i just took their entire italian training part.
yeah.
so it was both channel zero plus channel one.
so
yeah.
so the alignments might be wrong then on channel one right?
on one.
possible.
so we might
yeah.
we can do a realignment.
that's true.
at least want to retrain on these alignments.
which should be better because they come from close talking microphone.
yeah.
the that was my idea.
i mean if if it if it is not the same labeling which is taking the spaces.
yeah.
okay.
yeah possible.
yeah.
huh.
i mean it
yeah.
so the system
so the v.a.d. was trained on maybe different set of labels for channel zero and channel one.
uhhuh.
and
uhhuh.
uhhuh.
was the alignments were were different for certainly different because they were independently trained.
we didn't copy the channel zero alignments to channel one.
uhhuh.
uhhuh.
yeah.
yeah.
but for the new alignments what you generated you just copied the channel zero to channel one right?
right.
yeah.
yeah.
um
and uh hhh
actually when we look at at the v.a.d. for some utterances it's almost perfect.
i mean it just dropped one frame.
the first frame of speech.
or
so there are some utterances where it's almost one hundred percent v.a.d. performance.
huh.
uh
but yeah.
huh
yep.
so the next thing is um i have the spreadsheet for three different system.
but for this you only have to look right now on the speechdat-car performance.
uh because i didn't test
so i didn't test the spectral subtraction on t.i. digits yet.
uh so you have three sheets.
one is the um proposal one system.
actually it's not exactly proposal one.
it's the system that sunil just described.
um
but with uh wiener filtering from um france telecom included.
um so this gives like fifty seven point seven percent uh uh error rate reduction on the speechdat-car data.
huh
and then i have two sheets where it's for a system where
uh
so it's again the same system.
but in this case we have spectral subtraction.
with a maximum overestimation factor of two point five.
uh there is smoothing of the gain trajectory with some kind of uh low pass filter.
which has forty milliseconds latency.
and then after subtraction um i add a constant to the energies.
and i have two cases where the first case is where the constant is twenty five d.b. below the mean speech energy.
and the other is thirty d.b. below.
um
and for these two system we have like fifty five point uh five percent improvement.
and fifty eight point one.
so again it's around fifty six fifty seven.
uh
because i notice the t.i. digits number is exactly the same for these last two.
yeah because i didn't
for the france telecom uh spectral subtraction included in the our system the t.i. digits number are the right one.
but not for the other system.
because i didn't test it yet this system including with spectral subtraction on the t.i. digits data.
i just tested it on speechdat-car.
uhhuh.
uh!
so so that means the only thing
so so so these numbers are simply
this we have to
yeah.
yeah.
yeah.
but this number.
okay.
yes.
so you so you just should look at that fifty eight point o. nine percent and so on.
right.
right.
okay.
good.
uhhuh.
um
yeah.
so this
so by uh by by reducing the noise a a decent threshold like minus thirty d.b. it's like uh you are like reducing the floor of the noisy regions right?
yeah.
yeah.
the floor is lower.
um
uhhuh.
uhhuh.
i'm sorry.
so when you say minus twenty five or minus thirty d.b. with respect to what?
to the average um speech energy.
which is estimated on the world database.
okay.
so basically you're creating a signal to noise ratio of twenty five or thirty d.b.
yeah.
but it's not
i i i think what you do is this.
uh
it it's
when when you have this after you subtracted it i mean then you get something with this uh where you set the values to zero.
and then you simply add an additive constant again.
yeah.
so you shift it somehow.
this this whole curve is shifted again.
right.
but did you do that before the thresholding to zero?
it's
or
but it's after the thresholding.
so
oh.
so you'd really want to do it before.
maybe
right?
maybe we might do it before.
yeah.
yeah.
because then the then you would have less of that phenomenon.
yeah.
hhh.
i think.
uh
yeah.
but still when you do this and you take the log after that it it reduce the the variance.
yeah.
it it
but
right.
huh
yeah that will reduce the variance.
that'll help.
but maybe if you does do it before you get less of these funny looking things he's drawing.
uhhuh.
um
so before it's like adding this to the to the original.
but but
we would
right at the point where you've done the subtraction.
okay.
um essentially you're adding a constant into everything.
uhhuh.
but the way stephane did it it is exactly the way i have implemented in the phone.
so
oh yeah.
better do it different then.
yeah.
um
yeah.
just you you just you just set it for a particular signal to noise ratio that you want.
yeah i i made similar investigations like stephane did here.
just uh adding this constant and and looking how dependent is it on the value of the constant.
yeah.
yeah.
uhhuh.
and then must choose them somehow to give on average the best results for a certain range of the signal to noise ratios.
yeah.
uhhuh.
uhhuh.
so
oh it's clear.
i should have given other results.
also it's clear when you don't add noise it's much worse.
like around five percent worse i guess.
uhhuh.
and if you add too much noise it get worse also.
and it seems that right now this this is a constant that does not depend on on anything that you can learn from the utterance.
it's just a constant noise addition.
um
and i i think
i i'm sorry.
i think
then then i'm confused.
i thought
you're saying it doesn't depend on the utterance.
but i thought you were adding an amount that was twenty five d.b. down from the signal energy.
yeah.
so the way i did that i just measured the average speech energy of the all the italian data.
oh!
and then i i have i used this as mean speech energy.
uhhuh.
oh it's just a constant amount over all.
yeah.
and
what i observed is that for italian and spanish when you go to thirty and twenty five d.b. uh it it's good.
okay.
oh.
it stays in this range.
it's uh the
well the performance of the this algorithm is quite good.
but for finnish you have a degradation already when you go from thirty five to thirty.
and then from thirty to twenty five.
and i have the feeling that maybe it's because just finnish has a mean energy that's lower than than the other databases.
and due to this the thresholds should be
yeah.
the the the noise addition should be lower.
but in i mean in the real thing you're not going to be able to measure what people are doing over half an hour or an hour or anything right?
and
so you have to come up with this number from something else.
yeah.
so
uh but you are not doing it now language dependent?
or
it's not.
it's just something that's fixed.
no.
it's overall.
yeah.
okay.
uhhuh.
um
yeah so i
but what he is doing language dependent is measuring what that number reference is that he comes down twenty five down from.
no.
it no.
because i did it i started working on italian.
no?
i obtained this average energy.
yeah.
and then i used this one.
for all the languages.
yeah.
okay.
so it's sort of arbitrary.
i mean so if if
yeah.
yeah.
yep.
um
yeah.
yeah so the next thing is to use this as as maybe initialization.
and then use something online.
uhhuh.
something more adaptive.
but and i expect improvement at least in finnish because uh the way
yeah.
okay.
well.
um
for italian and spanish it's this value works good but not necessarily for finnish.
huh
but unfortunately there is like this forty millisecond latency.
and
um
yeah so i would try to somewhat reduce this.
i already know that if i completely remove this latency so um it um there is a three percent hit on italian.
uhhuh.
does latency
sorry.
go ahead.
yeah.
your your smoothing was uh over this so to say the the factor of the wiener.
and then it's uh
what was it?
this
uhhuh.
this smoothing it was over the subtraction factor so to say.
it's a smoothing over the the gain of the subtraction algorithm.
was this done
uhhuh.
and and you are looking into the future into the past.
right.
and smoothing.
uhhuh.
so to smooth this thing.
yeah.
um
and did did you try simply to smooth um to smooth the the to to smooth stronger the the envelope?
um
no.
i did not.
huh.
huh.
because i mean it should have a similar effect if you
yeah.
i mean you you have now several stages of smoothing so to say.
you start up.
as far as i remember you you smooth somehow the envelope.
you smooth somehow the noise estimate.
uhhuh.
and and later on you smooth also this subtraction factor.
huh.
uh no.
it's it's just the gain that's smoothed actually.
uh actually i i do all the smoothing.
but it's smoothed
uh.
oh it it was you.
yeah yeah.
uh yeah.
yeah.
yeah.
yeah.
no in this case it's just the gain.
uhhuh.
and
but the way it's done is that um for low gain there is this nonlinear smoothing actually.
for low gains um i use the smoothed uh smoothed version.
uh.
but for high gain it's i don't smooth.
uhhuh.
i just uh it experience shows you if if you do the
the best is to do the smoothing as early as possible.
uhhuh.
so when you start up
i mean you start up with the with the somehow with the noisy envelope.
uhhuh.
and best is to smooth this somehow.
uhhuh.
uh yeah.
i could try this.
um
and
so before estimating the s.n.r. smooth the envelope.
yeah.
yeah.
uhhuh.
uhhuh.
but
yeah.
then i i would need to find a way to like smooth less also when there is high energy.
because i noticed that it it helps a little bit to like smooth more during low energy portions and less during speech.
yes.
because if you smooth then you kind of distort the speech.
yeah.
yeah.
right.
um
uhhuh.
yeah i think when
you you could do it in this way that you say if you if i'm
you have somehow a noise estimate.
uhhuh.
and if you say i'm i'm with my envelope.
i'm close to this noise estimate.
yeah.
and then you you would like to have a stronger smoothing.
uhhuh.
so you could you could base it on your estimation of the signal to noise ratio on your actual
uhhuh.
uhhuh.
huh.
yeah.
or some silence probability from the vad if you have
um
yeah.
but i don't trust the current vad.
so
yeah.
uh so not not right now maybe.
well maybe.
maybe.
the vad later will be much better.
yeah.
so
i see.
so is that it?
uh fff i think that's it.
yeah.
uh
so to summarize the performance of these speechdat-car results is similar than than yours so to say.
yeah.
yeah.
so the fifty eight is like the some fifty six point.
you have you have fifty six point four.
and and and dependent on this additive constant it is better or or worse.
yeah.
that's true.
yeah.
slightly better?
uhhuh.
yeah.
yeah.
uhhuh.
and yeah.
the condition where it's better than your approach it's it just because maybe it's better on well matched and that the weight on well matched is is bigger.
yeah.
yeah you you caught up.
because
yep.
that's true.
if you don't weigh differently the different condition you can see that your well the the two stage wiener filtering is maybe better.
or
it's better for high mismatch right?
yeah.
yeah.
it's better for high mismatch.
uhhuh.
but a little bit worse for well matched.
so over all it gets yeah worse for the well matched condition.
so
uhhuh.
so we need to combine these two.
uh
that's that's the best thing is like the french telecom system is optimized for the well matched condition.
uhhuh.
they
yeah.
so they know that the weighting is good for the well matched.
and so there's
everywhere the well matched's performance is very good for the french telecom.
yeah.
uhhuh.
uhhuh.
we are we may also have to do something similar.
uhhuh.
well our tradition here has always been to focus on the mismatched.
um the
because it's more interesting.
my mine was it too i mean.
yeah.
before i started working on this aurora.
so
yeah.
yeah.
yeah.
okay.
carmen?
do you uh
well i only say that the this is a summary of the of all the v.t.s. experiments.
and say that the result in the last um for italian the last experiment for italian are bad.
i make a mistake when i write.
up at d. i copy one of the bad result.
so you
and
there you know this
um
well.
if we put everything we improve a lot the spectral use of the v.t.s.
but the final result are not still huh good like the wiener filter for example.
i don't know.
maybe it's it's possible to to have the same result.
that's somewhere.
i don't know exactly.
huh.
because i have huh worse result in medium mismatch and high mismatch.
you you have a better
yeah.
you have some results that are good for the high mismatch.
and yeah.
i are more or less similar but but are worse.
and
still i don't have the result for t.i. digits.
the program is training.
maybe for this weekend i will have result t.i. digits.
and i can complete that like this.
well.
uh.
right.
one thing that i note are not here in this result but are speak are spoken before with sunil i i improve my result using clean l.d.a. filter.
uhhuh.
uhhuh.
if i use uh the l.d.a. filter that are training with the noisy speech that hurts the my results.
so what are these numbers here?
are these with the clean or with the noisy?
this is with the clean.
okay.
with the noise i have worse result that if i doesn't use it.
uhhuh.
but that may be because with this technique we are using really really clean speech.
the speech the representation that go to the h.t.k. is really clean speech.
because it's from the dictionary.
the code book.
and maybe from that.
i don't know.
uhhuh.
because i think that you did some experiments using the two the two l.d.a. filter clean and and noise.
it's
and it doesn't matter too much.
um
yeah i did that.
but it doesn't matter on speechdat-car.
but it matters uh a lot on t.i. digits.
it's better to use clean.
using the clean filter.
yeah.
uh it's much better when you we used the clean derived l.d.a. filter.
uhhuh.
maybe you can do also this.
yeah.
to use clean speech.
uh
yeah i'll try.
but yeah.
sunil in in your result it's
i i'll try the
no i i my result is with the noisy noisy l.d.a.
it's with the noisy one.
yeah.
yeah.
oh.
it's with the noisy.
so
yeah.
it's it's not the clean l.d.a.
um
it's in in the front sheet i have like like the summary.
yeah.
and and your result is with the
it's with the clean l.d.a.
oh.
this is your results are all with the clean l.d.a. result.
yeah with the clean l.d.a.
yeah.
okay.
is that the reason?
and in your case it's all all noisy.
all noisy.
yeah.
yeah.
but
and
uh
yeah.
uh
but i observe my case it's in uh uh at least on speechdat-car it doesn't matter.
but t.i. digits it's like two or three percent absolute uh better.
on t.i. digits this matters.
absolute.
uh
so if
so you really might want to try the clean i think.
yeah i i i will have to look at it.
yeah that's true.
yeah.
yeah that could be sizeable right there.
and this is everything.
yeah.
maybe you you are leaving in in about two weeks carmen.
okay.
no?
yeah.
yeah.
so i mean if if if i would put it put on the head of a project manager i i i would say uh um i mean there is not so much time left now.
be my guest.
i mean if um
what what i would do is i i i would pick the best consolation which you think.
and create create all the results for the whole database that you get to the final number as as sunil did it.
and prepare at the
and um
and maybe also to to write somehow a document where you describe your approach and what you have done.
yeah.
i was thinking to do that next week.
yeah.
yeah.
yeah.
i'll i'll borrow the head back and and agree.
yeah.
i i i will do that next week.
yeah.
that's that's
right.
in fact actually i i guess the uh the spanish government uh requires that anyway.
they want some kind of report from everybody who's in the program.
uhhuh.
so
and of course i'd we'd we'd like to see it too.
so
okay.
yeah.
so um
what's do you think we uh should do the digits or skip it?
or what are what do you think?
uh we have them now?
yeah.
got them.
uh why why don't we do it?
okay.
just just take a minute.
i can send
yet.
would you pass those down?
oh.
sorry.
okay.
um
so i guess i'll go ahead.
um
okay.
so uh he's not here.
so
so you get to
yeah i will try to explain the thing that i did this this week during this week.
yeah.
well uh you know that i work i begin to work with a new feature to detect voice unvoice.
uhhuh.
what i trying two m l p to to the with this new feature and the fifteen feature uh from the uh base system.
the the mel cepstrum?
no satly the the mel cepstrum
the new base system the new base system.
oh the
okay.
yeah we
the aurora system.
yeah the aurora system with the new filter v a d or something like that.
okay.
and i'm trying two m l p one one that only have three output.
voice unvoice and silence.
uhhuh.
and other one that have fifty six output.
the probabilities of the allophone.
and i tried to do some experiment of recognition with that.
and only have result with with the m l p with the three output.
and i put together the fifteen features and the three m l p output.
and well the result are a little bit better but more or less similar.
uh i i'm i'm slightly confused.
huh.
what what feeds the uh the three output net?
voice unvoice and
no no what feeds it?
what features does it see?
the feature the input?
the inputs are the fifteen the fifteen uh bases feature.
uhhuh.
the with the new code.
and the other three features are r the variance of the difference between the two spectrum.
uhhuh.
the variance of the auto correlation function except the the first point because half the height value is r zero.
uhhuh.
uhhuh.
uhhuh.
uhhuh.
and also r zero.
the first coefficient of the auto correlation function.
that is like the energy with these three feature
right.
also these three feature.
you wouldn't do like r one over r zero or something like that?
i mean usually for voiced unvoiced you'd do yeah you'd do something you'd do energy.
yeah.
but then you have something like spectral slope which is you get like r one over r zero or something like that.
uh yeah.
what are the r s?
i'm sorry i missed it.
r correlations.
no.
oh.
r no.
auto correlation?
yes yes the variance of the auto correlation function that uses that.
well that's the variance.
but if you just say what is
i mean to first order um
yeah one of the differences between voiced unvoiced and silence is energy.
yeah.
another one is but the other one is the spectral shape.
i'll
the spectral shape.
yeah.
yeah.
and so r one over r zero is what you typically use for that.
no.
i don't use that.
i can't use
no.
i'm saying that's what people typically use.
huh.
see because it because this is this is just like a single number to tell you um does the spectrum look like that or does it look like that.
uhhuh.
oh.
r r zero.
right?
uhhuh.
so if it's if it's um if it's low energy uh but the but the spectrum looks like that or like that it's probably silence.
uhhuh.
uh but if it's low energy and the spectrum looks like that it's probably unvoiced.
yeah.
so if you just if you just had to pick two features to determine voiced unvoiced you'd pick something about the spectrum like uh r one over r zero um and r zero.
uhhuh.
okay.
or you know you'd have some other energy measure.
and
like in the old days people did like uh zero crossing counts.
yeah yeah.
right?
well i can also use this.
yeah.
um
because the result are a little bit better.
but we have in a point that everything is more or less the similar more or less similar.
yeah.
but um
it's not quite better.
right but it seemed to me that what you were what you were getting at before was that there is something about the difference between the original signal or the original f f t and with the filter which is what
and the variance was one take uh on it.
yeah.
i used this too.
right.
but it it could be something else.
suppose you didn't have anything like that.
then in that case if you have two nets
all right and this one has three outputs and this one has
uhhuh.
whatever.
fifty six or something.
uhhuh.
if you were to sum up the probabilities for the voiced and for the unvoiced and for the silence here we've found in the past you'll do better at voiced-unvoiced-silence than you do with this one.
so just having the three output thing doesn't doesn't really buy you anything.
yeah.
the issue is what you feed it.
yeah.
i have
yeah.
so uh
so you're saying take the features that go into the voiced-unvoiced-silence net and feed those into the other one as additional inputs rather than having a separate
no
well that's another way.
yeah.
that wasn't what i was saying.
but yeah that's certainly another thing to do.
no i was just trying to say if you if you bring this into the picture over this what more does it buy you?
huh.
and what i was saying is that the only thing i think that it buys you is um based on whether you feed it something different.
and something different in some fundamental way.
and so the kind of thing that that she was talking about before was looking at something uh um something uh about the difference between the the uh um log f f t uh log power uh and the log magnitude uh f spectrum uh and the um uh filter bank.
yeah.
and so the filter bank is chosen in fact to sort of integrate out the effects of pitch.
and she's saying
you know trying
so the particular measure that she chose was the variance of this of this difference.
uhhuh.
but that might not be the right number.
maybe.
right?
i mean maybe there's something about the variance that's that's not enough.
or maybe there's something else that that one could use.
but i think that for me the thing that that struck me was that uh you want to get something back here.
so here's here's an idea.
uh what about it you skip all the all the really clever things and just fed the log magnitude spectrum into this?
uh i'm sorry.
this is
you have the log magnitude spectrum and you were looking at that and the difference between the filter bank and and computing the variance.
yeah.
uhhuh.
uhhuh.
that's a clever thing to do.
what if you stopped being clever?
ha!
and you just took this thing in here because it's a neural net and neural nets are wonderful.
and figure out what they can what they most need from things and i mean that's what they're good at.
yeah.
so i mean you're you're you're trying to be clever and say what's the statistic that should we should get about this difference.
but uh in fact you know maybe just feeding this in or or feeding both of them in.
huh.
you know another way saying let it figure out what's the what is the interaction.
especially if you do this over multiple frames.
uhhuh.
then you have this over time and and both kinds of measures.
and uh you might get uh something better.
uhhuh.
um
so so don't uh don't do the division.
but let the net have everything.
that's another thing you could do.
yeah.
yeah.
yeah.
um
i mean it seems to me if you have exactly the right thing then it's better to do it without the net.
because otherwise you're asking the net to learn this.
you know say if you wanted to learn how to do multiplication.
uhhuh.
i mean you could feed it a bunch of you could feed two numbers that you wanted to multiply into a net.
and have a bunch of non linearities in the middle.
and train it to get the product of the output and it would work.
but it's kind of crazy.
because we know how to multiply.
and you you'd be you know much lower error usually if you just multiplied it out.
but suppose you don't really know what the right thing is.
and that's what these sort of dumb machine learning methods are good at.
so
um
anyway.
it's just a thought.
how long does it take carmen to train up one of these nets?
oh not too much.
yeah.
huh one day or less.
huh.
yeah.
it's probably worth it.
what are what are your uh frame error rates for for this?
uh fifty six.
uh no the frame error rate?
fifty six i think.
is that maybe that's accuracy?
percent.
the accuracy.
fifty six percent accurate for voice unvoice
uhhuh.
no for
yes
i don't remember for voice unvoice.
oh okay.
maybe for the other one.
yeah voiced unvoiced hopefully would be a lot better.
okay.
for voiced.
i don't
should be in nineties somewhere.
better.
maybe for voice unvoice.
right.
this is for the other one.
i should i can't show that.
okay.
but i think that fifty five was for the when the output are the fifty six phone.
uhhuh.
that i look in the with the other nnn the other m l p that we have are more or less the same number.
silence will be better but more or less the same.
i think at the frame level for fifty six that was the kind of number we were getting for for uh um reduced band width uh stuff.
i think that i i i think that for the other one for the three output is sixty two three more or less.
uhhuh.
that's all?
it's
yeah.
that's pretty bad.
yeah because it's noise also.
aha!
oh yeah.
and we have
aha!
yeah.
i know.
yeah.
okay.
but even in
oh yeah in training.
still
uh
well actually so this is a test that you should do then.
um if you're getting fifty six percent over here
uh that's in noise also.
right?
yeah yeah yeah.
oh okay.
if you're getting fifty six here try adding together the probabilities of all of the voiced phones here and all of the unvoiced phones.
will be
and see what you get then.
yeah.
i bet you get better than sixty three.
well i don't know.
but i i i think that we i have the result more or less maybe.
i don't know.
i don't i'm not sure but i remember that i can't show that.
okay but that's a that is a a good check point.
you should do that anyway.
yeah.
okay?
given this this uh regular old net that's just for choosing for other purposes uh add up the probabilities of the different subclasses and see see how well you do.
uh and that you know anything that you do over here should be at least as good as that.
uhhuh.
okay.
i will do that.
the targets for the neural net uh they come from forced alignments?
but
uh no.
timit canonical mappings.
timit.
uh.
oh so this is trained on timit.
okay.
yeah.
yeah noisy timit.
okay.
yeah this for timit.
but noisy timit?
noisy timit.
right.
we have noisy timit with the noise of the the t i digits.
and now we have another noisy timit also with the noise of uh italian data base.
i see.
yeah.
well there's going to be it looks like there's going to be a noisy uh some large vocabulary noisy stuff too.
somebody's preparing.
really?
yeah.
i forget what it'll be.
resource management.
wall street journal.
something.
some some read task actually that they're preparing.
huh!
for what
for aurora?
yeah.
oh!
yeah so the uh
uh the issue is whether people make a decision now based on what they've already seen or they make it later.
and one of the arguments for making it later is let's make sure that whatever techniques that we're using work for something more than than connected digits.
huh.
so
when are they planning when would they do that?
huh i think late uh i think in the summer sometime.
huh.
so
okay thanks.
this is the work that i did during this date.
uhhuh.
and also huh i hynek last week say that if i have time i can to begin to to study.
well seriously the france telecom proposal
uhhuh.
to look at the code
and something like that.
to know exactly what they are doing
because maybe that we can have some ideas.
uhhuh.
but not only to read the proposal.
look look carefully what they are doing with the program and something like that.
and i begin to to work also in that.
but the first thing that i don't understand is that they are using r the uh log energy that this quite
i don't know why they have some constant in the expression of the lower energy.
i don't know what that means.
they have a constant in there you said?
yeah.
oh at the front it says uh log energy is equal to the rounded version of sixteen over the log of two.
this
yeah.
uh
then maybe i can understand.
uh times the
well this is natural log.
and maybe it has something to do with the fact that this is
is that some kind of base conversion?
i i have no idea.
or
yeah that's what i was thinking.
but but um then there's the sixty four.
uh i don't know.
because maybe they're the threshold that they are using on the basis of this value
experimental results.
mcdonald's constant.
i don't know exactly.
because well i thought maybe they have a meaning.
but i don't know what is the meaning of take exactly this value.
yeah it's pretty funny looking.
so they're taking the number inside the log and raising it to sixteen over log base two.
i don't know.
yeah i
um right.
sixteen over two.
does it have to do with those sixty fours?
or
um
if we ignore the sixteen the natural log of one over the natural log of two times the
i don't know.
uhhuh.
well maybe somebody will think of something.
but this is uh
it may just be that they they want to have for very small energies
they want to have some kind of a
yeah the the effect i don't i can understand the effect of this.
no?
because it's to to do something like that.
no?
well it says since you're taking a natural log it says that when when you get down to essentially zero energy this is going to be the natural log of one which is zero.
uhhuh.
so it'll go down to uh to the natural log being
so the lowest value for this would be zero.
so you're restricted to being positive.
and this sort of smooths it for very small energies.
uh why they chose sixty four and something else that was probably just experimental.
yeah.
and the the the constant in front of it i have no idea.
um
well i i will look to try if i move this parameter in their code what happens?
maybe everything is maybe they tres hole are on basis of this.
uh i mean it they they probably have some particular fixed point arithmetic that they're using.
i don't know.
yeah i was just going to say maybe it has something to do with hardware.
and then it just
something they were doing.
yeah.
yeah.
i mean that they're probably working with fixed point or integer or something.
i think you're supposed to on this stuff anyway.
and and so maybe that puts it in the right realm somewhere.
well it just yeah puts it in the right range.
or
yeah.
i think given at the level you're doing things in floating point on the computer i don't think it matters would be my guess.
uhhuh.
but
i this more or less anything
yeah.
okay and when did stephane take off?
he took off
i think that stephane will arrive today or tomorrow.
oh he was gone these first few days.
and then he's here for a couple days before he goes to salt lake city.
uhhuh.
okay.
he's i think that he is in las vegas or something like that.
yeah.
yeah.
so he's he's going to i. cassp which is good.
i i don't know if there are many people who are going to i. cassp.
yeah.
so so i thought make sure somebody go.
yeah.
do have have people sort of stopped going to i. cassp in recent years?
um people are less consistent about going to i. cassp.
and i think it's still it's still a reasonable forum for students to to present things.
uh it's i think for engineering students of any kind i think it's it's if you haven't been there much it's good to go to.
uh to get a feel for things a range of things.
not just speech.
uh
but i think for for sort of dyed in the wool speech people um i think that i c s l p and eurospeech are much more targeted.
uhhuh.
uh
and then there's these other meetings like h l t and and uh a s r u.
uhhuh.
so there's there's actually plenty of meetings that are really relevant to to uh computational uh speech processing of one sort or another.
uhhuh.
uhhuh.
um so
i mean i mostly just ignored it because i was too busy and didn't get to it.
so uh
want to talk a little bit about what we were talking about this morning?
oh!
just briefly or or anything else?
um uh yeah.
so i i guess some of the progress i i've been getting a getting my committee members for the quals.
and um so far i have morgan and hynek mike jordan.
and i asked john ohala and he agreed.
cool.
yeah yeah.
so i'm i i just need to ask um malek.
one more.
um tsk
then uh i talked a little bit about um continuing with these dynamic um acoustic events.
and um we're we're we're thinking about a way to test the completeness of a a set of um dynamic uh events.
uh completeness in the in the sense that um if we if we pick these x number of acoustic events do they provide sufficient coverage for the phones that we're trying to recognize or or the the words that we're going to try to recognize later on?
and so morgan and i were uh discussing um uh a form of a cheating experiment where we get um we have uh um a chosen set of features or acoustic events.
and we train up a hybrid um system to do phone recognition on timit.
so the idea is if we get good phone recognition results using um these set of acoustic events then um that that says that these acoustic events are sufficient to cover a set of phones.
at least found in timit.
um so it would be a a measure of are we on the right track with with the the choices of our acoustic events?
um so that's going on.
and also just uh working on my uh final project for jordan's class.
uh which is
actually let me
hold that thought.
yeah.
let me back up while we're still on it.
okay sure.
the the other thing i was suggesting though is that given that you're talking about binary features uh maybe the first thing to do is just to count.
and uh count co occurrences and get probabilities for a discrete h m m.
because that'd be pretty simple.
because it's just say if you had ten ten events uh that you were counting uh each frame would only have a thousand possible values for these ten bits.
and uh so you could make a table that would say if you had thirty nine phone categories that would be a thousand by thirty nine.
and just count the co occurrences and divide them by the the uh uh uh uh count the co occurrences between the event and the phone and divide them by the number of occurrences of the phone.
and that would give you the likelihood of the of the event given the phone.
and um then just use that in a very simple h m m.
and uh you could uh do phone recognition then and uh wouldn't have any of the issues of the uh training of the net or
i mean it'd be on the simple side.
but
uhhuh.
uh um
you know if uh uh the example i was giving was that if if you had um onset of voicing and and end of voicing as being two kinds of events then if you had those all marked correctly and you counted co occurrences you should get it completely right.
uhhuh.
so
um but you'd get all the other distinctions you know randomly wrong.
i mean there'd be nothing to tell you that.
so um uh
if you just do this by counting then you should be able to find out in a pretty straightforward way whether you have a sufficient uh set of events to to do the kind of level of of uh classification of phones that you'd like.
so that was that was the idea.
and then the other thing that we were discussing was was um okay how do you get the your training data?
uhhuh.
because uh the switchboard transcription project uh uh you know was half a dozen people or so working off and on over a couple years.
and
uh similar similar amount of data to what you're talking about with timit training.
so it seems to me that the only reasonable starting point is uh to automatically translate the uh current timit markings into the markings you want.
and uh it won't have the kind of characteristic that you'd like of catching funny kind of things that maybe aren't there from these automatic markings.
uhhuh.
but but
uh it's uh
it's probably a good place to start.
yeah.
yeah.
yeah and a short short amount of time.
just to again just to see if that information is sufficient to uh determine the phones.
uhhuh.
huh.
so
yeah you could even then to to get an idea about how different it is you could maybe take some subset and you know go through a few sentences mark them by hand.
and then see how different it is from you know the canonical ones.
right.
just to get an idea a rough idea of if it really even makes a difference.
you can get a little feeling for it that way.
yeah.
yeah that is probably right.
i mean uh my my guess would be that this is since timit's read speech that this would be less of a big deal.
uhhuh.
if you went and looked at spontaneous speech it'd be more more of one.
right.
right.
and the other thing would be say if you had these ten events you'd want to see well what if you took two events or four events or ten events or
and you know and
and hopefully there should be some point at which having more information doesn't tell you really all that much more about what the phones are.
uhhuh.
you could define other events as being sequences of these events too.
uh you could.
but the thing is what he's talking about here is a uh a translation to a per frame feature vector.
so there's no sequence in that.
i think.
i think it's just a
unless you did like a second pass over it or something after you've got your
yeah but we're just talking about something simple here yeah to see if
yeah.
yeah yeah.
yeah.
i'm adding complexity.
yeah.
just
you know the idea is with a with a very simple statistical structure could you could you uh at least verify that you've chosen features that are sufficient?
yeah.
okay and you were saying something starting to say something else about your your class project or
oh.
yeah um
yeah.
so for my class project i'm um i'm tinkering with uh support vector machines.
something that we learned in class.
and uh um basically just another method for doing classification.
and so i'm going to apply that to um compare it with the results by um king and taylor who did um these.
um
using recurrent neural nets they recognized um a set of phonological features.
um
and made a mapping from the m f c c's to these phonological features.
so i'm going to do a similar thing with with support vector machines.
and see if
so what's the advantage of support vector machines?
what
um so support vector machines are are good with dealing with a less amount of data.
huh.
and um so if you if you give it less data it still does a reasonable job in learning the the patterns.
huh.
um and um
i guess it
yeah.
they're sort of succinct.
and and they uh
yeah.
does there some kind of a distance metric that they use?
or how do they for what do they do for classification?
um.
right.
so the the simple idea behind a support vector machine is um you have you have this feature space.
right?
uhhuh.
and then it finds the optimal separating plane um between these two different um classes.
uhhuh.
uhhuh.
and um and so um
what it at the end of the day what it actually does is it picks those examples of the features that are closest to the separating boundary.
and remembers those.
uhhuh.
and and uses them to recreate the boundary for the test set.
so given these um these features or or these these examples um critical examples which they call support support vectors then um given a new example if the new example falls um away from the boundary in one direction then it's classified as being a part of this particular class.
oh.
and otherwise it's the other class.
so why save the examples?
why not just save what the boundary itself is?
uhhuh.
um huh
let's see.
uh
yeah that's a good question.
i
that's another way of doing it.
yeah.
huh.
right so so it i mean i i guess it's
sort of an equivalent.
you know it it goes back to nearest neighbor sort of thing.
uhhuh.
right.
um if is it uh
when is nearest neighbor good?
well nearest neighbor good is good if you have lots and lots of examples.
um but of course if you have lots and lots of examples then it can take a while to to use nearest neighbor.
there's lots of look ups.
so a long time ago people talked about things where you would have uh a condensed nearest neighbor.
where you would you would you would pick out uh some representative examples which would uh be sufficient to represent to to correctly classify everything that came in.
oh.
uhhuh.
i i think i think support vector stuff sort of goes back to to that kind of thing.
i see.
um
so rather than doing nearest neighbor where you compare to every single one you just pick a few critical ones.
and
yeah.
huh.
and the
you know um neural net approach uh or gaussian mixtures for that matter are sort of fairly brute force kinds of things where you sort of you predefine that there is this big bunch of parameters.
and then you you place them as you best can to define the boundaries.
and in fact as you know these things do take a lot of parameters
and and uh if you have uh only a modest amount of data you have trouble uh learning them.
um so i i guess the idea to this is that it it is reputed to uh be somewhat better in that regard.
uhhuh.
right.
it can be a a reduced um parameterization of of the the model by just keeping certain selected examples.
huh.
yeah.
so
but i don't know if people have done sort of careful comparisons of this on large tasks or anything.
maybe maybe they have.
i don't know.
yeah i don't know either.
yeah.
do you get some kind of number between zero and one at the output?
actually you don't get a you don't get a nice number between zero and one.
you get you get either a zero or a one.
um uh
there are there are
well basically it's it's um you you get a distance measure at the end of the day.
and then that distance measure is is um is translated to a zero or one.
um
but that's looking at it for for classification for binary classification.
that's for classification.
and you get that for each class you get a zero or a one.
right?
right.
right.
but you have the distances to work with.
you have the distances to work with.
because actually mississippi state people did use support vector machines for uh uh speech recognition and they were using it to estimate probabilities.
yeah.
yeah.
yeah they they had a had a way to translate the distances into into probabilities with the with the simple um uh sigmoidal function.
yeah.
and did they use sigmoid or a softmax type thing?
and didn't they like exponentiate or something?
um yeah.
and then divide by the sum of them?
there's some there's like one over one plus the exponential or something like that.
or
oh it
oh so it is a sigmoidal.
yeah.
okay.
all right.
did the did they get good results with that?
i mean they're okay.
i i don't i don't think they were earth earth shattering.
but i think that uh this was a couple years ago.
huh.
i remember them doing it at some meeting.
and and um i don't think people were very critical because it was interesting just to to try this.
and you know it was the first time they tried it.
so so the you know the numbers were not incredibly good.
huh.
but there's you know it was reasonable.
uhhuh.
i i don't remember anymore.
i don't even remember what the task was.
it was broadcast news or something.
i don't know.
huh.
right.
uh so barry if you just have zero and ones how are you doing the speech recognition?
oh!
i'm not i'm not planning on doing speech recognition with it.
i'm just doing detection of phonological features.
oh okay.
so uh for example this this uh feature set called the uh sound patterns of english um is just a bunch of um binary valued features.
let's say is this voicing or is this not voicing?
is this sonorants not sonorants?
and stuff like that.
okay.
so
did you find any more mistakes in their tables?
oh!
uh i haven't gone through the entire table yet.
yeah.
yesterday i brought chuck the table.
and i was like wait this is is the mapping from n to to this phonological feature called um coronal.
is is should it be shouldn't it be a one.
or should it should it be you know coronal instead of not coronal as it was labelled in the paper.
so i haven't hunted down all the all the mistakes yet.
uhhuh.
but
but as i was saying people do get probabilities from these things.
okay.
and and uh we were just trying to remember how they do.
but people have used it for speech recognition and they have gotten probabilities.
so they have some conversion from these distances to probabilities.
okay.
right yeah.
there's you have you have the paper.
right?
the mississippi state paper?
uhhuh.
uhhuh.
yeah if you're interested you could look.
and
okay okay.
yeah.
yeah.
i can i can show you i
yeah.
so in your in in the thing that you're doing uh you have a vector of ones and zeros for each phone?
our
uhhuh.
uh is this the class project?
or
yeah.
okay.
um
is that what you're
right right right.
so for every phone there is there is a um a vector of ones and zeros uh corresponding to whether it exhibits a particular phonological feature or not.
uhhuh.
uhhuh.
and so when you do your
i'm what is the task for the class project?
to come up with the phones?
um
or to come up with these vectors to see how closely they match the phones?
oh.
right.
um to come up with a mapping from um m f c c's or some feature set um to uh to whether there's existence of a particular phonological feature.
or
uhhuh.
and um
yeah.
basically it's to learn a mapping from from the m f c c's to uh phonological features.
is it did that answer your question?
i think so.
okay.
i guess i mean uh i'm not sure what you what you're what you get out of your system.
do you get out a uh a vector of these ones and zeros and then try to find the closest matching phoneme to that vector?
uhhuh.
or
oh.
no no.
i'm not i'm not planning to do any any phoneme mapping yet.
just it's it's basically it's it's really simple basically a detection of phonological features.
uhhuh.
i see.
yeah.
and um because the uh
so king and and taylor um did this with uh recurrent neural nets.
yeah.
uhhuh.
and this their their idea was to first find a mapping from m f c c's to uh phonological features.
and then later on once you have these phonological features then uh map that to phones.
uhhuh.
so i'm i'm sort of reproducing phase one of their stuff.
huh.
so they had one recurrent net for each particular feature?
right.
right right right.
i see.
i did they compare that i mean what if you just did phone recognition and did the reverse lookup?
uh
so you recognize a phone and which ever phone was recognized you spit out it's vector of ones and zeros.
uhhuh.
i expect you could do that.
i mean uh
uh.
that's probably not what he's going to do on his class project.
yeah.
yeah.
no.
so um have you had a chance to do this um thing we talked about yet with the uh
yeah.
um
insertion penalty?
uh no actually i was going a different
that's a good question too.
but i was going to ask about the the um changes to the data in comparing p l p and mel cepstrum for the s r i system.
uh
well what i've been
changes to the data.
i'm not sure i
right.
so we talked on the phone about this that that there was still a difference of a of a few percent.
yeah.
right.
and you told me that there was a difference in how the normalization was done.
and i was asking if you were going to do redo it uh for p l p with the normalization done as it had been done for the mel cepstrum.
uhhuh.
uh right.
no i haven't had a chance to do that.
what i've been doing is uh trying to figure out
okay.
it just seems to me like there's a um
well it seems like there's a bug.
because the difference in performance is it's not gigantic.
but it's big enough that it it seems wrong.
yeah.
i agree.
and
but i thought that the normalization difference was one of the possibilities.
yeah but i don't i'm not
right?
yeah.
i guess i don't think that the normalization difference is going to account for everything.
so what i was working on is um just going through and checking the headers of the wavefiles.
okay.
to see if maybe there was a um a certain type of compression or something that was done that my script wasn't catching.
so that for some subset of the training data uh the the the features i was computing were junk.
okay.
which would you know cause it to perform okay.
but uh you know the the models would be all messed up.
so i was going through and just double checking that kind of think first to see if there was just some kind of obvious bug in the way that i was computing the features.
uhhuh.
i see.
okay.
looking at all the sampling rates to make sure all the sampling rates were what eight k what i was assuming they were.
yeah.
um
yeah that makes sense to check all that.
yeah.
so i was doing that first before i did these other things just to make sure there wasn't something
although really uh uh a couple three percent uh difference in word error rate uh could easily come from some difference in normalization i would think.
but
yeah.
and i think i'm trying to remember but i think i recall that andreas was saying that he was going to run sort of the reverse experiment.
uh which is to try to emulate the normalization that we did.
but with the mel cepstral features.
sort of you know back up from the system that he had.
i thought he said he was going to
i have to look back through my my email from him.
yeah he's probably off at at uh his meeting now.
yeah.
he's gone now.
um
yeah.
yeah.
but
yeah.
the i think they should be roughly equivalent.
but
um
i mean again the cambridge folk found the p l p actually to be a little better.
right.
uh so it's um
i mean the other thing i wonder about was whether there was something just in the the bootstrapping of their system which was based on
but maybe not since they
yeah see one thing that's a little bit um
i was looking i've been studying and going through the logs for the system that um andreas created.
and um his uh the way that the s r i system looks like it works is that it reads the wavefiles directly.
uh and does all of the cepstral computation stuff on the fly.
right.
right.
and so there's no place where these where the cepstral files are stored anywhere that i can go look at and compare to the p l p ones.
so whereas with our features he's actually storing the cepstrum on disk and he reads those in.
right.
but it looked like he had to give it uh
even though the cepstrum is already computed he has to give it uh a front end parameter file.
which talks about the kind of uh computation that his mel cepstrum thing does.
uhhuh.
so i i don't know if that it probably doesn't mess it up.
it probably just ignores it if it determines that it's already in the right format or something.
but the the the two processes that happen are a little different.
so
yeah.
so anyway there's stuff there to sort out.
yeah.
yeah.
so
okay.
let's go back to what you thought i was asking you.
yeah.
no and i didn't have a chance to do that.
ha!
oh!
you had the same answer anyway.
yeah.
yeah i've been um i've been working with um jeremy on his project.
and then i've been trying to track down this bug in uh the icsi front end features.
uhhuh.
so one thing that i did notice yesterday i was studying the um the uh rasta code.
uhhuh.
and it looks like we don't have any way to um control the frequency range that we use in our analysis.
we basically it looks to me like we do the f f t um and then we just take all the bins.
and we use everything.
we don't have any set of parameters where we can say you know only process from you know a hundred and ten hertz to thirty seven fifty.
um
at least i couldn't see any kind of control for that.
yeah i don't think it's in there.
i think it's in the uh uh uh the filters.
so the f f t is on everything.
but the filters
um
for instance ignore the the lowest bins and the highest bins.
and what it does is it it copies
the the filters?
which filters?
um
the filter bank which is created by integrating over f f t bins.
uhhuh.
um
when you get the mel when you go to the mel scale?
right.
yeah.
it's bark scale.
and it's it it um it actually copies the uh um the second filters over to the first.
so the first filters are always
and you can you can specify a different number of uh features different number of filters.
i think.
as i recall.
so you can specify a different number of filters and whatever um uh you specify the last ones are going to be ignored.
so that that's a way that you sort of change what the what the bandwidth is.
you can't do it without i think changing the number of filters.
i saw something about uh that looked like it was doing something like that but i didn't quite understand it.
but
so maybe
yeah so the idea is that the very lowest frequencies and and typically the veriest highest frequencies are kind of junk.
uhhuh.
and so um
you just for continuity you just approximate them by by the second to highest and second to lowest.
uhhuh.
it's just a simple thing we put in.
and and so if you
but so the but that's a fixed uh thing?
there's nothing that lets you
yeah i think that's a fixed thing.
but see see my point?
if you had if you had ten filters then you would be throwing away a lot at the two ends.
uhhuh.
and if you had if you had fifty filters you'd be throwing away hardly anything.
uhhuh.
um i don't remember there being an independent way of saying we're just going to make them from here to here.
use this analysis bandwidth or something.
but i i i don't know.
it's actually been awhile since i've looked at it.
yeah i went through the feacalc code and then looked at you know just calling the rasta libs and thing like that.
and i didn't i couldn't see any place where that kind of thing was done.
but um i didn't quite understand everything that i saw.
yeah see i don't know feacalc at all.
so
uhhuh.
but it calls rasta with some options.
and um
right.
but i i think in
i don't know.
i guess for some particular database you might find that you could tune that and tweak that to get that a little better.
but i think that in general it's not that critical.
i mean there's
yeah.
you can you can throw away stuff below a hundred hertz or so.
and it's just not going to affect phonetic classification at all.
another thing i was thinking about was um is there a
i was wondering if there's maybe um certain settings of the parameters when you compute p l p which would basically cause it to output mel cepstrum.
so that in effect what i could do is use our code.
but produce mel cepstrum.
and compare that directly to
well it's not precisely.
yeah.
huh.
i mean
um um
what you can do is um you can definitely change the the filter bank from being uh a uh trapezoidal integration to a a a triangular one.
uhhuh.
which is what the typical mel mel cepstral uh filter bank does.
uhhuh.
and some people have claimed that they got some better performance doing that.
so you certainly could do that easily.
but the fundamental difference
i mean there's other small differences
there's a cubic root that happens.
right?
yeah.
but you know as opposed to the log in the other case.
i mean the fundamental difference that we've seen any kind of difference from before which is actually an advantage for the p l p uh i think is that the the smoothing at the end is auto regressive instead of being cepstral uh from cepstral truncation.
so um it's a little more noise robust.
huh.
um and that's that's why when people started getting databases that had a little more noise in it like like uh um broadcast news and so on.
that's why cambridge switched to p l p i think.
uhhuh.
so um
that's a difference that i don't think we put any way to get around.
since it was an advantage.
uhhuh.
um uh
but we did uh we did hear this comment from people at some point that um it uh they got some better results with the triangular filters rather than the trapezoidal.
so that is an option in rasta.
huh.
uh and you can certainly play with that.
but i think you're probably doing the right thing to look for bugs first.
yeah just it just seems like this kind of behavior could be caused by you know some of the training data being messed up.
i don't know.
could be.
you know you're sort of getting most of the way there but there's a
so i started going through and looking
one of the things that i did notice was that the um log likelihoods coming out of the log recognizer from the p l p data were much lower much smaller.
than for the mel cepstral stuff and that the average amount of pruning that was happening was therefore a little bit higher for the p l p features.
uhhuh!
so since he used the same exact pruning thresholds for both i was wondering if it could be that we're getting more pruning.
oh!
he he he used the identical pruning thresholds even though the the range of of the
yeah.
oh well that's that's a pretty good point right there.
right.
right.
yeah.
yeah.
so
i would think that you might want to do something like uh you know look at a few points to see where you are starting to get significant search errors.
that's
right.
well what i was going to do is i was going to take um a couple of the utterances that he had run through.
then run them through again.
but modify the pruning threshold and see if it you know affects the score.
yeah.
yeah.
so
but i mean you could uh if if if that looks promising you could you know uh run the overall test set with a with a few different uh pruning thresholds for both.
uhhuh.
right.
and presumably he's running at some pruning threshold that's that's uh you know gets very few search errors.
but is is relatively fast.
uhhuh.
right i mean yeah generally in these things you you turn back pruning really far.
and
so i i didn't think it would be that big a deal.
because i was figuring well you have it turned back so far that you know it
but you may be in the wrong range for the p l p features for some reason.
yeah.
yeah.
yeah.
and the uh the the run time of the recognizer on the p l p features is longer.
which sort of implies that the networks are bushier.
you know there's more things it's considering.
which goes along with the fact that the matches aren't as good.
so uh you know it could be that we're just pruning too much.
so
yeah.
yeah maybe just be different kind of distributions.
and and
uhhuh.
yeah.
so that's another possible thing.
uhhuh.
they they should really shouldn't.
there's no particular reason why they would be exactly behave exactly the same.
uhhuh right.
right.
so
so
there's lots of little differences.
so
yeah.
yeah.
trying to track it down.
yeah.
i guess this was a little bit off topic i guess.
yeah.
because i was i was thinking in terms of this as being a a a a core item that once we once we had it going we would use for a number of the front end things also.
uhhuh.
so
um
want to
that's as far as my stuff goes.
what's what's on
yeah.
yeah.
well i tried this mean subtraction method.
um
due to avendano i'm taking um six seconds of speech.
um i'm using two second f f t analysis frames stepped by a half second.
so it's a quarter length step.
and i i take that frame and four the four i take
sorry.
i take the current frame and the four past frames and the four future frames.
and that adds up to six seconds of speech.
and i calculate um the spectral mean of the log magnitude spectrum over that n.
i use that to normalize the the current center frame by mean subtraction.
and i then then i move to the next frame and i i do it again.
well actually i calculate all the means first.
and then i do the subtraction.
and um the i tried that with h d k.
the aurora setup of h d k training on clean t i digits.
and um it it helped.
um in a phony reverberation case
um where i just used the simulated impulse response um the error rate went from something like eighty it was from something like eighteen percent to um four percent.
and on meeting recorder far mike digits mike on channel f it went from um forty one percent error to eight percent error.
on on the real data not with artificial reverb?
right.
uhhuh.
and that that was um trained on clean speech only.
which i'm guessing is the reason why the baseline was so bad.
and
that's actually a little side point is i think that's the first results that we have uh uh uh of any sort on the far field uh on on the far field data uh for recorded in in meetings.
oh um actually um adam ran the s r i recognizer.
did he?
on the near field on the
on the far field also.
he did one p z m channel and one p d a channel.
oh did he?
oh.
i didn't recall that.
what kind of numbers was he getting with that?
i i'm not sure.
i think it was about five percent error for the p z m channel.
five.
i think.
yeah.
so why were you getting forty one here?
is this
um
i i'm i'm guessing it was the the training data.
uh clean t i digits is like pretty pristine training data.
and if they trained the s r i system on this t v broadcast type stuff
i think it's a much wider range of channels.
and it
no.
but wait a minute.
i i i i think he
what am i saying here?
yeah.
so that was the s r i system.
maybe you're right.
yeah.
because it was getting like one percent so it's still this kind of ratio.
it was it was getting one percent or something on the near field.
wasn't it?
uhhuh.
or it wa it was around one.
yeah.
yeah i think it was getting around one percent for the near for the for the close mike.
yeah.
huh.
so it was like one to five?
okay.
so it's still this kind of ratio.
it's just
yeah.
it's a lot more training data.
so
so probably it should be something we should try then is to is to see if is at some point just to take to transform the data.
and then and then uh use use it for the s r i system.
you you mean um
so you're so you have a system which for one reason or another is relatively poor.
yeah.
and and uh you have something like forty one percent error.
uh and then you transform it to eight by doing doing this this work.
um
so here's this other system which is a lot better.
but there's still this kind of ratio.
it's something like five percent error with the the distant mike.
and one percent with the close mike.
okay.
so the question is how close to that one can you get if you transform the data using that system?
right so so i guess this s r i system is trained on a lot of broadcast news or switchboard data.
yeah.
is that right?
do you know which one it is?
it's trained on a lot of different things.
um
it's trained on uh a lot of switchboard call home.
uhhuh.
um a bunch of different sources.
some digits.
there's some digits training in there.
okay.
one thing i'm wondering about is what this mean subtraction method um will do if it's faced with additive noise.
huh.
because i i it's because i don't know what log magnitude spectral subtraction is going to do to additive noise.
yeah.
well it's it's not exactly the right thing.
that's that's the
uhhuh.
but uh but you've already seen that because there is added noise here.
that's that's yeah that's true.
yeah.
that's a good point.
so um
okay so it's then then it's it's it's reasonable to expect it would be helpful if we used it with the s r i system.
and
yeah.
i mean as helpful
i mean
so that's the question.
yeah we're often asked this when we work with a system that that isn't isn't sort of industry industry standard great.
uhhuh.
uh and we see some reduction in error using some clever method.
then you know will it work on a on a on a good system?
so uh
you know this other one's it was a pretty good system.
i think you know one one percent word error rate on digits is uh digit strings is not uh you know stellar.
but but given that this is real digits as opposed to uh sort of laboratory
uhhuh.
well
and it wasn't trained on this task either.
and it wasn't trained on this task.
actually one percent is sort of you know sort of in a reasonable range.
people would say yeah i could i can imagine getting that.
uhhuh.
and uh so the the four or five percent or something is is is quite poor.
uhhuh.
uh you know if you're doing a uh a sixteen digit uh credit card number you'll basically get it wrong almost all the time.
so so uh um
huh.
a significant reduction in the error for that would be great.
huh okay.
and and then uh
yeah.
so
yeah.
cool.
sounds good.
yeah.
all right um i actually have to run.
so i don't think i can do the digits.
but um i guess i'll leave my microphone on?
uh yeah.
yeah.
thank you.
yep.
good.
yeah.
okay so let's get started.
nancy said she's coming.
and that means she will be.
um my suggestion is that robert and johno sort of give us a report on last week's adventures uh to start.
so everybody knows there were these guys uh from uh uh actually from uh d f k i uh part of the german smartkom project who were here for the week.
and i think got a lot done.
yeah i think so too.
um
the we got to the point where we can now speak into the smartkom system and it'll go all the way through and then say something like roman numeral one am smarticus.
it actually says roemisch einz am smarticus.
which means it's just using a german synthesis module for english sentences.
okay.
okay.
so uh
it doesn't know i.
okay.
um the uh
oh.
am spartacus.
i am i am smarticus is what it's saying.
verstehe.
right.
i
okay.
the uh synthesis is just a question of um hopefully it's just a question of exchanging a couple of files once we have them.
and um it's not going to be a problem because we decided to stick to the so called concept to speech approach.
so i'm i'm i'm going backwards now.
so synthesis is where you sort of make this uh make these sounds.
and concept to speech is feeding into this synthesis module giving it what needs to be said and the whole syntactic structure.
so it can pronounce things better presumably then just with text to speech.
uhhuh.
and uh johno learned how to write x m l tags.
uh and did write the tree adjoining grammar for some some sentences.
no?
right?
yeah for a couple
yeah.
so uh the way the uh the dialogue manager works is it dumps out what it wants to know or what it wants to tell the person to a uh in x m l.
and there's a conversion system for different uh to go from x m l to something else.
and so the knowledge base for the system that generates the syntactic structures for the generation is uh in a lisp like the knowledge base is in a lisp like form.
and then the thing that actually builds these syntactic structures is something based on prolog.
so you have a basically a goal.
and it you know says okay well i'm going to try to do the greet the person goal.
uhhuh.
so it just starts uh it binds some variables and it just decides to you know do some subscold.
basically it just means build the tree.
and then it passes the tree onto uh the the generation module.
okay.
but i think that the point is that out of the twelve possible utterances that the german system can do we've already written the the syntax trees for three or four.
we
yeah.
so the syntax trees are very simple.
it's like most of the sentences in one tree.
uhhuh.
and instead of you know breaking down to like small units and building back up they basically took the sentences and basically cut them in half or you know into thirds or something like that and made trees out of those.
and so uh uh tilman wrote a little tool that you could take lisp notation and generate an x m l uh tree uh what do structure from the from the lisp.
and so basically you just say you know noun goes to you know
nah i don't i've never been good at those.
so there's like the v p goes to n and those things in lisp and it will generate for you.
okay.
n v.
yeah.
okay.
all right.
and because we're sticking to that structure the synthesis module doesn't need to be changed so all that fancy stuff.
and the texas speech version of it which is actually the simpler version is going to be done in october which is much too late for us.
so
this way we we worked around that.
the uh the system um i can show you the system.
i actually want at least maybe you should be able to start it on your own if you want to play around with it in in the future.
right now it's brittle and you need to start it up and then make twenty changes on on on on seventeen modules before they actually can stomach it anything.
and send in a a a couple of side queries on some dummy center set up program so that it actually works.
because it's designed for this seevit thing where you have the gestural recognition running with this siemens virtual touch screen which we don't have here.
uhhuh.
and so we're doing it via mouse.
but the whole system was designed to work with this thing.
and it was it was a lot of engineering stuff.
no science in there whatsoever.
but it's working now.
and um that's the good news.
so everything else actually did prove to be language independent except for the parsing and the generation.
why i had i did need to generate different trees than the german ones.
mainly because you know like uh the gerund in in german is automatically taken care of with just a regular verb.
you have to switch it on.
uhhuh.
so i'd uh have to add am walking.
or i'd have to add a little stem for the am when i build the built the tree.
okay.
okay.
yeah i noticed that um that some of the examples they had had you know non english word orders and so on.
you know and then all that good stuff.
so
yeah.
all right.
like
so it might be worth keith you looking at this.
yeah.
um
well tilman
i i still don't i still don't really understand like
i mean we sort of say um
you know i i still don't exactly understand sort of the information flow uh in in this thing or what the modules are and so on.
so you know like just that such and such module uh um decides that it wants to achieve the goal of greeting the user.
and then magically it sort of
yeah
i mean how does it know which syntactic structure to pull out and all that?
i
yeah.
so i think it's not worth going over in the group.
uh
but sort of when you get free and you have the time uh either robert or johno or i can walk you through it.
sure.
yeah.
soon.
okay.
and you can ask all the questions about how this all fits together.
that's fine.
it's eee messy.
but once you understand it you understand it.
it's it's there's nothing really complicated about it.
okay.
no.
and i remember one thing that that came up in the talk last wednesday um was this
i i think he talked about the idea of like um he was talking about these lexicalized uh uh tree adjoining grammars where you sort of for each word you um
okay you know how to do it?
for each lexical item the lexical entry says what all the uh trees are that it can appear in.
and of course that's not that's the opposite of constructional.
that's you know that's that's h p s g or whatever.
right.
right.
you know?
now we're we're not committed for our research to do any of those things.
yeah.
uhhuh.
so uh we are committed for our funding.
right.
okay?
to uh
make our stuff fit to that.
yeah to
no to just get the get the demos they need.
uhhuh.
okay?
so between us all we have to get the demos they need.
if it turns out we can also give them lots more than that by you know tapping into other things we do that's great.
you should probably move the microphone closer to your face.
uhhuh.
but it turns out not to be in any of the contracts.
there's like a little the twisty thing you can move it with.
okay.
and deliberately.
so the reason i'd like you to understand uh what's going on in this demo system is not because it's important to the research.
it's just for closure.
so that if we come up with a question of could we fit this deeper stuff in there or something.
you know what the hell we're talking about fitting in.
right.
okay.
so it's just uh in the same actually with the rest of us.
we just need to really understand what's there.
is there anything we can make use of?
uh is there anything we can give back beyond the sort of minimum requirements?
but none of that has a short time fuse.
okay.
so the the demo requirements for this fall are sort of taken care of as of later this week or something.
and then so it's probably fifteen months or something until there's another serious demo requirement.
that doesn't mean we don't think about it for fifteen months.
oh okay.
but it means we can not think about it for six months.
right.
right yeah.
so the plan for this summer uh really is to step back from the applied project.
right.
keep the keep the context open but actually go after the basic issues.
huh.
oh okay.
and
so the idea is there's this uh other subgroup that's worrying about formalizing the getting a notation.
but sort of in parallel with that uh the hope is in particularly you will work on constructions in english and german for this domain.
uhhuh.
but not worry about parsing them or fitting them into smartkom or any of the other anything any other constraints for the time being.
yeah.
okay.
got it.
it's hard enough to get it semantically and syntactically right and then and get the constructions in their form and stuff.
yeah.
and i i don't want you feeling that you have to somehow meet all these other constraints.
right okay.
um and similarly with the parsing uh we're going to worry about parsing uh the general case.
you know construction parser for general constructions.
and if we need a cut down version for something or whatever we'll worry about that later.
okay.
so i'd like to for the summer turn into science mode.
okay.
and i assume that's also uh your plan as well.
right.
so i mean the the point is that like the meetings um so far that i've been at have been sort of been geared towards this demo.
yeah.
yeah.
but but we're
and then that's going to go away pretty soon.
right.
okay.
and then we'll sort of shift gears fairly substantially.
yeah.
it's
yeah.
it's got
huh?
yeah.
what i what i think is is a good idea that i can can show to anyone who's interested.
we can even make a sort of an internal demo.
and i i show you what i do.
uhhuh.
i speak into it and you hear it talk.
okay.
and i can sort of walk through the information.
so this is like in half hour or forty five minutes.
just fun.
okay.
and so you when somebody on the streets comes up to you and asks you what is smartkom so you can sort of give a sensible answer.
right.
okay.
so we could set that up as actually an institute wide thing?
just give a talk in the big room and and so people know what's going on.
when you're ready.
absolutely.
yeah i mean that's the kind of thing
that's the level at which you know we can just invite everybody and say this is a project that we've been working on and here's a demo version of it and stuff like that.
yeah.
okay.
well we we do want to have all the bugs out where you have to sort of pipe in extra x m l messages from left and right before you're
uhhuh.
indeed.
yeah.
okay.
makes sense.
but so that
it's clear then i think.
actually roughly starting uh let's say next meeting because this meeting we have one other thing to tie up besides the trip report.
yeah.
okay.
but uh starting next meeting i think we want to flip into this mode where
uh i mean there are a lot of issues.
what's the ontology look like.
uhhuh.
you know what do the constructions look like.
what's the execution engine look like.
huh lots of things.
uhhuh.
but more focused on uh an idealized version than just getting the demo out.
now before we do that let's get back in
oh!
but it's still i think useful for you to understand the demo version enough so that you can can see what what it is that that uh it might eventually get retro fitted into or something.
yeah.
okay.
right.
and johno's already done that uh looked at the uh the looked at the smartkom stuff.
uh
to some
uh what what part of the smartkom stuff?
well the parser and that stuff.
oh yeah yeah.
okay anyway.
so the trip
the report on these
the last we we sort of interrupted you guys telling us about what happened last week.
yeah.
it's all right.
um well it was just amazing to to see uh how how instable the whole thing is.
maybe you're done then.
and if you just take the
and i i got the feeling that we are the only ones right now who have a running system.
i don't know what the guys in kaiserslautern have running.
because
the version that is the full version that's on the server does not work.
and you need to do a lot of stuff to make it work.
and so it's
and even tilman and ralf sort of said yeah there never was a really working version that uh did it without all the shortcuts that they built in for the uh october version.
so we're actually maybe ahead of the system gruppe by now.
the system the integration group.
and it was uh it was fun to some extent.
but the uh the outcome that is sort of of scientific interest is that i think both ralf and tilman
um i know that they enjoyed it here.
and they they they liked uh a lot of the stuff they saw here.
what what we have been thinking about.
and they're more than willing to to um cooperate by all means.
and um part of my responsibility is uh to use our internal group ware server at e m l.
make that open to all of us and them.
so that whatever we discuss in terms of parsing and and generating and constructions we we sort of uh put it in there.
and they put what they do in there.
and maybe we can even um get some overlap get some synergy out of that.
and um
the uh
if i find someone at in e m l that is interested in that um i i may even think that we could look take constructions and and generate from them.
because the tree adjoining grammars that that tilman is using is as you said nothing but a mathematical formalism.
and you can just do anything with it whether it's syntactic trees h p s g like stuff or whether it's construction.
so if you ever get to the generation side of constructing things and there might be something of interest there.
but in the moment we're of course definitely focused on the understanding um pipeline.
any other uh visit reports sort of stories?
uh we so we now know i think what the landscape is like.
uhhuh.
and so we just push on and and uh do what we need to do.
and one of the things we need to do is the um and this i think is relatively tight tightly constrained is to finish up this belief net stuff.
so
uh and i was going to switch to start talking about that unless there're other more general questions.
okay so here's where we are on the belief net stuff as far as i understand it.
um going back i guess two weeks ago uh robert had laid out this belief net missing only the connections.
right?
that is so he'd put all all the dots down.
and we went through this and i think more or less convinced ourselves that at least the vast majority of the nodes that we needed for the demo level we were thinking of were in there.
yeah we may run across one or two more.
but of course the connections weren't.
so uh bhaskara and i went off and looked at some technical questions about were certain operations sort of legitimate belief net computations and was there some known problem with them or had someone already uh solved you know how to do this and stuff.
and so bhaskara tracked that down.
the answer seems to be uh no no one has done it but yes it's a perfectly reasonable thing to do if that's what you set out to do.
and so the current state of things is that again starting now um we'd like to actually get a running belief net for this particular subdomain done in the next few weeks.
so bhaskara is switching projects as of the first of june.
and uh he's going to leave us an inheritance which is a uh hopefully a belief net that does these things.
and there're two aspects to it one of which is you know technical.
getting the coding right and making it run and uh stuff like that.
and the other is the actual semantics.
okay what all you know what are the considerations and how and what are the ways in which they relate?
so he he doesn't need help from this group on the technical aspects or if he does uh we'll do that separately.
uhhuh.
but in terms of what are the decisions and stuff like that that's something that we all have to work out.
is is that right?
i mean that's that's both you guys' understanding of where we are?
absolutely.
okay.
so i guess um is there like a latest version of the belief net of the proposed belief net?
like
we had um decided
like
um well no we didn't decide.
we wanted to look into maybe getting it the visualization a bit clearer.
but i think if we do it um sort of a paper version of all the nodes and then the connections between them that should suffice.
uhhuh.
yeah.
that should be fine.
yeah i
yeah i mean that's a separate problem.
we do in the long run want to do better visualization and all that stuff.
yeah.
that's separable.
yeah.
i did look into that uh in terms of you know exploding the nodes out and down
yep.
right.
javabayes does not support that.
i can imagine a way of hacking at the code to do that.
it'd probably take two weeks or so to actually go through and do it.
not not at this point.
and i went through all the other packages on kevin murphy's page.
right.
and i couldn't find the necessary mix of free and uh with the gui and with this thing that we want.
well we can if it's if we can pay
yeah if you know it's paying a thousand dollars or something we can do that.
okay?
okay.
so so don't view free as as a absolute constraint.
okay so then i'll go back and look at the ones on the list that
okay.
yeah.
but
and you can ask kevin.
yeah the one that uh people seem to use is uh hugin or whatever.
huh.
but
hugin.
yeah that's free.
how
i don't think it's
is it free?
because i've seen it advertised in places so i it seems to
uh it may be free to academics.
like i i don't know.
i have a i have a copy that i i downloaded.
okay.
so at one point it was free.
okay.
uh but i noticed people do use hugin.
so um
how do you spell that?
why?
h u g i n.
and bhaskara can give you a pointer.
so then in any case um
but paying a
you know if if it's uh
probably for university it's it's going to be real cheap anyway.
but um you know if it's fifty thousand dollars we aren't going to do it.
i'm mean we have no need for that.
i i also would suggest not to spend two weeks in in in changing the the javabayes code.
no.
he's not going to do that.
yeah.
okay.
i i will send you a pointer to a java applet that does that.
it's sort of a fish eye.
you you have a node and you click on it and it shows you all the connections.
and then if you click on something else that moves away that goes into the middle.
huh.
and maybe there is an easy way of interfacing those two.
if that doesn't work it's not a problem we we need to solve right now.
what i'm what my job is i will um give you the input in terms of of the internal structure.
maybe node by node or something like this.
or should i collect it all?
uhhuh.
and
doesn't matter.
um just any like like sort of rough representation of the entire belief net is probably best.
okay.
and um you're going to be around again always tuesdays and thursdays afternoon-ish as usual?
yeah.
or will that change?
i mean yeah.
i can like i
um this week i guess um kind of i have a lot of projects and stuff but after that i will generally be more free.
so yes i might i can be around.
and i mean generally if you email me also i can be around on other days.
yeah.
okay.
yeah and this is not a crisis that i mean you do everybody who's a student should you know do their work get their courses all in good shape.
and and and and then we'll dig dig down on this.
yeah that's
yeah.
okay.
no that's good.
that means i have i i can spend this week doing it.
so
okay.
how do you go about this process of deciding what these connections are?
i know that there's an issue of how to weight the different things too and stuff.
right.
right?
i mean do you just sort of guess and see if it sort of
well there there there there're two different things you do.
it's
one is you design and the other is you learn.
okay?
so uh what we're going to do initially is is do design and if you will guess.
okay.
okay?
uh that is you know use your best knowledge of of the domain to uh hypothesize what the dependencies are and stuff.
right.
okay.
if it's done right and if you have data then there are techniques for learning the numbers given the structure.
yeah.
and there are even techniques for learning the structure although that takes a lot more data.
and it's not as and so forth and so on.
so uh
but for the limited amount of stuff we have for this particular exercise i think we'll just design it.
all right.
yeah.
hopefully as time passes we'll get more and more data from heidelberg and from people actually using it and stuff.
okay.
so but this is the long run.
yeah.
but to solve our problems uh a mediocre design will do i think in the beginning.
yeah.
that's right.
yeah oh and by the way speaking of data um are there
i could swore uh i could swear i saw it sitting on someone's desk at some point.
but is there a um a transcript of any of the sort of initial interactions of people with the with the system?
uhhuh.
because you know i'm still sort of itching to to look at what look at the stuff and see what people are saying.
yeah.
yeah make yourself a note.
so and and of course keith would like the german as well as the english.
so whatever you guys can get.
the german?
oh yeah of course german.
yeah.
yeah the your native language.
right?
you remember that one.
okay.
that's important yeah.
so he'll get you some data.
yeah
okay.
yeah i mean i i sort of um found the uh uh the audio of some of those.
huh.
and um it kind of sounded like i didn't want to trudge through that you know.
it was just strange.
but
yep.
we probably will not get those to describe because they were trial runs.
oh yeah.
okay.
um but uh that's but we have data in english and german already.
so
okay.
yeah i mean
transcribed.
i will send you that.
okay.
okay.
so while we're still at this sort of top level anything else that we ought to talk about today?
how was your thingy?
oh um i just wanted to uh like mention as an issue um you know last meeting i wasn't here.
because i went to a linguistics colloquium on the fictive motion stuff.
oh right.
and that was pretty interesting.
and you know i mean
seems to me that that will fairly obviously be of relevance to uh to what we're doing here.
because you know people are likely to give descriptions.
like you know what's that thing uh right where you start to go up the hill or something like that.
you know meaning a few feet up the hill or whatever from some reference point and all that stuff.
so i mean i'm sure in terms of you know people trying to state locations or you know all that kind of stuff this is going to be very relevant.
so
um now that was the talk was about english versus japanese um which obviously the japanese doesn't affect us directly except that um some of the construction
he'd what he talked about was that you know in english we say things like you know your bike is parked across the street.
and we use these prepositional phrases you know well if you were to move across the street you would be at the bike.
but um in in japanese the the more conventionalized tendency is to use a sort of a description of where one has crossed to the river there is a tree.
um and you know you can actually say things like um there's a tree where one has crossed the river but no one has ever crossed the river or something like that.
so the idea is that this really is you know that's supposed show that's it's really fictive and so on.
but um
but the point is that that kind of construction is also used in english you know like right where you start to go up the hill or just when you get off the train or something like that to uh to indicate where something is.
huh.
so we'll have to think about
so
how much is that used in german?
um the uh well i i was on a uh on a on a different sidetrack.
i mean the the deep map project which um is undergoing some renovation at at the moment.
oh okay.
but this is a a three language project german english japanese.
okay.
and um we have a uh uh i have taken care that we have the the japanese generation and stuff.
and so i looked into uh spatial description.
so we can generate spatial descriptions how to get from a to b and and information on objects in german english and japanese.
uhhuh.
and there is a huge uh project on spatial descriptions uh differences in spatial descriptions.
well if if you're interested in that.
so how how
i mean it does sort of go all the way down to the conceptual level to some extent.
okay.
so
um
so where is this huge project?
it's kleist.
it's the uh bielefeld generation of uh spatial descriptions and whatever.
uhhuh.
well that may be another thing that keith wants to look at.
okay.
but um i i think we should leave japanese constructions maybe outside of the scope for for now.
yeah.
but um definitely it's interesting to look at at cross the bordered there.
uhhuh.
are are you going to pay any attention to the relative position of of the direction relative relative to the speaker?
for example there are some differences between hebrew and english.
we can say um park in front of the car as you come you drive behind the car.
in hebrew it means park behind the car because to follow the car is defined as it faces you.
uhhuh.
intrinsic.
yeah.
while in english front of the car is the absolute front of the car.
okay.
right so the canonical direction of motion determines where the front is.
so
right.
right.
okay.
so is german uh closer to to uh uh uh uh to i mean uh
uhhuh.
i don't think it it's related to syntax though so it may be entirely different.
um as a matter of fact
no it's not.
right.
yeah.
um did you ever get to look at the the paper that i sent you on the on that problem in english and german?
i think
carroll ninety three.
um i there is a a study on the differences between english and german on exactly that problem.
huh.
so it's they actually say the monkey in front of the car.
where's the monkey?
uhhuh.
and um they found statistically very significant differences in english and german.
so i i i
it might be since there are only a finite number of ways of doing it that that german might be more like hebrew in that respect.
the solution they proposed was that it was due to syntactic factors.
huh.
that but it wasn't was
that syntactic factors do do play a role there whether you're more likely you know to develop uh choices that lead you towards using uh intrinsic versus extrinsic reference frames.
right.
uhhuh.
right.
huh.
i mean it seems to me that you can get both in in english depending
you know like in front of the car could you know like
here's the car sideways to me in between me and the car or something's in front of the car or whatever.
i could see that.
absolutely.
but
but anyway so you know i mean this was this was a a very good talk on those kinds of issues and so on.
so uh
i can also give you uh a pointer to a paper of mine which is the the ultimate taxonomy of reference frames.
all right.
so
cool.
i'm the only person in the world who actually knows how it works.
oh.
oh.
not really.
great.
no i've not seen that.
it's called a
what do you mean um reference frames?
uh uh
it's it's spatial reference frames.
you actually have only
um if you want to have a
this is usually um
i should there should be an l though.
well actually you have only have two choices.
you can either do a two point or a three point.
which is you you're familiar with with the origo?
where that's the center origo is the center of the frame of reference.
and then you have the reference object and the object to be localized.
huh.
huh.
okay?
uhhuh.
in some cases the origo is the same as the reference object.
this was like
so that would be origin in english.
the origin.
right?
yeah.
right.
origo is a terminus technikus in that sense.
that's even used in the english literature.
origo.
oh okay.
all right.
i never heard it.
okay.
okay.
and um so
this video tape is in front of me.
i'm the origo and i'm also the reference object.
uhhuh.
uhhuh.
those are two point.
right.
uhhuh.
and three point relations is if something has an intrinsic front side like this chair then your shoe is behind the chair.
yeah.
uhhuh.
and reference object and um no from from my point of view your shoe is left of the chair.
right.
you you can actually say things like um it's behind the tree from me or something like that i think in in in certain circumstances in english.
right?
yeah.
as sort of from where i'm standing it would appear that
yeah.
so
looks a little bit like reichenbach for time.
yeah it sounds like it.
doesn't it?
it's a lot like it.
yeah.
yeah.
and then and then here you
um
on this scale you have it either be ego or allocentric.
uhhuh.
and that's that's basically it.
so egocentric two point egocentric three point or you can have allocentric.
so as seen from the church the town hall is right of that um fire station.
oh okay.
uhhuh it's hardly ever used but it's
i'd love to see it if you if you have a copy kind of.
yeah.
i see this is this is getting into ami's thing.
yeah.
uh
uhhuh.
he's he's very interested in that.
here
okay.
so
uh yeah.
me too.
well why don't you just put it on the web page?
there's this e d u
right?
yeah.
it's or or just
yeah.
it's also all on my my home page at e m l.
or a link to it.
it's called an anatomy of a spatial description.
just
but i'll send that link.
okay great.
maybe just put a link on.
yep.
yeah.
yep.
by the way there something that i didn't know until about a week ago or so is apparently there are separate brain areas for things within reach and things that are out of reach.
so there's there's uh all this linguistic stuff about you know near and far or yon and and so forth.
huh.
uhhuh.
so this is all this is there's this linguistic facts.
but apparently the uh here's the way the findings go.
that you know they do m r i and and if you're uh got something within reach then there's one of your areas lights up.
and if something's out of reach uh a different one.
but here's the the amazing result um they say.
you get someone with a with a deficit so that they have a perfectly normal ability at distance things.
so the typical task is subdivision.
so there's a a line on the wall over there and you give them a laser pointer and you say where's the midpoint.
and they do fine.
if you give them the line and they have to touch it.
they can't.
there's just that part of the brain isn't functioning so they can't do that.
here's the real experiment.
the same thing on the wall you give them a laser where is it.
they do it.
uhhuh.
give them a stick long stick and say do it.
they can't do it.
so there's a remapping of distant space into nearby space.
right.
so they doubled
because it's within reach now?
the the end the end of this
it's not within reach and you use the within reach uh mechanism.
yeah.
oh wow!
yeah.
circuits.
so i'll i'll dig you up this reference.
right.
that's cool.
and so this this is uh
first of all it explains something that i've always wondered about.
and i'll do this this test on you guys as well.
so
uh i have had an experience not often but a certain number of times when for example i'm working with a tool a screwdriver or something for a long time i start feeling the tip directly.
not indirectly.
but you actually can feel the tip.
yeah.
yeah.
and people who are uh accomplished violinists and stuff like that claim they also have this kind of thing where you get a direct sensation of physical sensation of the end affector.
yeah.
what's going on at the end of the tool.
yeah.
the the the the extension.
huh?
what's going on at the end of the tool or whatever.
yeah.
within
right.
huh?
the extension of of your hand right.
yeah.
right.
have you had this?
the
i i think so.
i mean it's not exactly the same thing but but it it it's getting close to that.
what does it feel like?
yeah.
oh it feels like your as if your uh neurons had extended themselves out to this tool and you're feeling forces on it and so forth.
and and you deal directly with it.
i once i i was playing you know with those um uh devices that allow you to manipulate objects when it's dangerous to get close.
right yeah yeah yeah.
oh okay.
so you can insert your hand something.
yeah.
and there's a correspondence between
yeah.
so i played with it.
after a while you don't feel the difference anymore.
yeah right.
i i mean it's kind of
uhhuh.
very kind of you stop back and suddenly it goes away.
and you have to kind of work again to recapture it but yeah.
yeah.
right.
yeah so anyway so so this was the first actual experimental evidence i'd seen that was consistent with this anecdotal stuff.
that's cool.
and of course it makes a lovely uh story about why languages uh make this distinction.
of course there are behavioral differences too.
things you can reach are really quite different than things you can't.
yeah.
but there seems to be an really deep embodied neural difference.
and this is um
so in addition to the
this is more proximal distal.
yeah uh exactly.
so in addition to ego and allocentric uh which appear all over the place you also apparently have this proximal distal thing which is very deeply uh embedded.
well dan montello sort of
he he does the uh uh the cognitive map world down in santa barbara.
and he he always talks about these.
he he already well
probably most likely without knowing this this evidence uh is talking about these small scale spaces that you can manipulate versus large scale environmental spaces.
yeah.
well there's there's uh been a lot of behavioral things on this but that was the first neuro physiological thing i saw.
anyway yeah so we'll we'll look at this.
and so all of these issues now are now starting to come up.
so now we're now done with demos.
we're starting to do science.
right?
and so these issues about uh reference and spatial reference discourse reference uhuh uhuh all this sort of stuff uh deixis which is part of what you were talking about.
uhhuh.
uhhuh.
um so all of this stuff is coming up essentially starting now.
so we got to do all this.
so there's that.
and then there's also a set of system things that come up.
so okay we're not using their system that means we need our system.
uhhuh.
right?
it it follows.
yeah.
and so uh in addition to the business about just getting the linguistics right and the formalism and stuff we're actually going to build something.
and uh johno is point person on the parser analyzer whatever that is.
and we're going to start on that in parallel with the um the grammar stuff.
but to do that we're going to need to make some decisions like ontology.
all right.
so um and so this is another thing where we're going to you know have to get involved and make relatively early i think make some decisions on uh is there an ontology a p i that that
there's a sort of standard way of getting things from ontologies.
and we build the parser and stuff around that.
or is there a particular ontology that we're going to standardize on and if so for example is there something that we can use there?
does uh either the uh smartkom project or one of the projects at e m l have something that we can just pull out for that?
uh so there are going to be some some some things like that which are not science but system.
but we aren't going to ignore those.
because we're we're not only going the plan is not only to lay out this thing but to actually uh build some of it.
and how much we build and and so forth.
i
uh part of it if it works right is
it looks like we're now in a position that the construction analyzer that we want for this applied project can be the same as the construction analyzer that nancy needs for the child language modeling.
so it's always been out of phase.
but it now seems that um there's a good shot at that.
so we've talked about it.
and the hope is that we can make these things the same thing.
okay.
and of course it's only in both cases it's only one piece of a bigger system.
uhhuh.
but it would be nice if that piece were exactly the same piece.
it was just this uh construction analyzer.
right.
and so we think we think we have a shot at at that.
okay.
so the
so
to to come full circle on that this formalization task
okay?
is trying to get the formalism into into a shape where it can actually uh
yeah.
be of use to someone who's trying to do this.
right?
well yeah.
where it actually is is covers the whole range of things.
and the the the the thing that got mark into the worst trouble is he had a very ambitious thing he was trying to do.
and he insisted on trying to do it with a limited set of mechanisms.
it turned out inherently not to cover the space.
okay.
and it just it was just terribly frustrating for him.
yeah.
and he seemed fully committed to both sides of this irreconcilable thing.
i see.
right.
and
uh johno is much more pragmatic.
okay.
huh?
good to know.
is this is true.
is it not?
yes.
okay.
so there's you know sort of yeah deep really deep emotional commitment to a certain theory being uh complete.
oh okay.
you don't have a hidden purist streak?
oh no.
okay.
well it hasn't it it certainly hasn't been observed in any case.
just checking.
no sir.
all right.
um now you do but that's okay.
uh so for for
because i don't have to implement anything.
exactly right.
i have a problem then.
exactly.
it's so whether i do depends on whether i'm talking to him or him probably.
huh.
yeah right.
right.
which meeting i'm in.
why actually uh the thing is you you do but the thing you have to implement is so small that uh
it's okay to be purist within that context.
within that.
yes.
yeah.
good.
and uh it's and still i think you know get something done.
cool!
but to try to do something upscale and purist particularly if if um what you're purist about doesn't actually work is real hard.
yay.
uhhuh.
yeah.
okay.
and then the other thing is while we're doing this uh robert's going to pick a piece of this space.
it's possible yeah.
okay.
okay?
uh for his absentee thesis.
i think you all know that that you can just in germany almost just send in your thesis.
just a drive up.
ca-chuk.
yeah right.
um
there you go.
okay.
there there's a drive in thesis uh joint over in saarbruecken.
exactly.
drive through yeah.
it costs a lot.
the the amount you put in your credit card and as well.
but uh
but anyway.
so uh that's um also got to be worked out hopefully over the next few weeks.
so that that it becomes clear uh what piece uh robert wants to jump into.
and while we're at this level uh there's at least one new doctoral student in computer science who will be joining the project either next week or the first of august depending on the blandishments of microsoft.
okay.
so uh and her name is eva.
okay.
it really is.
nobody believed that.
yeah i thought it had to be a joke of your part you know.
like johno made it up.
yeah.
i'm sure.
is this person someone who's in first year this year?
no first year coming.
or
so she's she's now out here.
she's moved.
okay.
and she'll be a student as of then.
and probably she'll pick up from you on the belief net stuff.
so she'll be chasing you down and stuff like that.
okay.
document.
uh
right?
uh against all traditions.
and actually i talked today to a uh undergraduate who wants to do an honors thesis on this.
uh
someone from the class?
no.
interestingly enough.
we always get these people who are not in the class who
some of some of them yeah.
it's interesting.
so anyway.
uh but uh she's another one of these ones with a three point nine average and so forth and so on.
uhhuh.
uh so um i've i've given her some things to read.
so we'll see how this goes.
oh there's yet another one of the incoming first incoming first year graduate students who's expressed interest.
so we'll see how that goes.
um
anyway
so i think as far as this group goes um it's certainly worth continuing for the next few weeks to get closure on the uh belief net and the ideas that are involved in that and what are what are the concepts.
we'll see whether it's going to make sense to have this be separate from the other bigger effort with the formalization stuff or not.
i'm not sure.
it partly depends on what your thesis turns out to be and how that goes.
so we'll see.
and then ami you can decide you know how much time you want to put into it.
and uh it's beginning to take shape.
okay.
so uh and
right
i think you will find that if you want to look technically at some of the your traditional questions in this light uh keith who's building constructions will be quite happy to uh see what you know you envision as the issues and the problems and um how they might uh get reflected in constructions.
sure.
i suspect that's right.
yeah.
yeah.
i i may have to go to switzerland for in june or beginning of july for between two weeks and four weeks but uh after that or before that.
okay.
fine.
and um if it's useful we can probably arrange for you to drop by and visit either at heidelberg or at the german a i center while you're in in the neighborhood.
right.
yeah uh
actually i'm invited to do some consulting with a bank in geneva which has an affiliation with a research institute in geneva which i forgot the name of.
yeah.
yep.
do
well we're connected to uh
yeah.
there's a there's a a very significant connection between
we'll we'll go through this.
yeah.
i c s i and e p f l which is the uh it's the
germany's got two big technical institutes.
there's one in in zurich.
uhhuh.
e t h.
and then there's one the french speaking one in lausanne.
okay?
oh so in switzerland.
which is uh e p f l.
so find out who they are associated with in geneva.
great.
probably we're connected to them.
right.
great.
i'll let you know.
i'll send you email.
okay.
yeah.
and so anyway we uh we can undoubtedly get ami uh to give a talk at uh e m l or something like that while he's in in uh
huh.
uh i i think the one you you gave here a couple of weeks ago would be of interest there too.
sure yeah.
a lot of interest actually either place d f k i or uh
yeah so and and if there is a book that you'll be building up some audience for it.
yeah.
right.
and you'll get feedback from these guys.
great yeah.
because they've actually these d f k i guys have done as much as anyone over the last decade in trying to build them.
so we'll set that up.
cool.
okay.
so uh unless we want to start digging into the uh the belief net and the decisions now which would be fine it's probably
i i
it's probably better if i come next week with the um version o point nine of the structure.
okay.
so how about if you two guys between now and next week come up with something that is partially proposal and partially questions.
saying here's what we think we understand here are the things we think we don't understand.
and that we as a group will try to to finish it.
what i'd like to do is shoot for finishing all this next monday.
sure.
okay?
uh these are the decisions
i don't think we're going to get lots more information.
it's a design problem.
uhhuh.
yeah.
you know we
yeah.
and let's come up with a first cut at what this should look like and then finish it up.
okay.
does that make sense?
okay.
and um the the semester will be over next week but then you have projects for one more week to come?
no i i think i'll be done everything by this uh by the end of this week.
same with you?
no.
nnn
this well i've i have projects but then the my professor of one of my classes also has a final that he's giving us.
and he's giving us five days to do it which means it going to be hard.
yeah.
yeah.
oh is it a take home final?
yeah.
who's doing this?
aikin alex yeah.
yeah figured.
that would have been my guess.
huh.
right.
um but anyway yeah.
pretty soon.
okay.
okay so i guess that's
so the seventeenth will definitely be the last day like it or not for me.
right.
right.
so let's do this.
and then well there's going to be some separate
these guys are talking.
uh we have a group on the formalization.
uh nancy and johno and i are going to talk about parsers.
so there're various kinds of uh
okay.
of course nothing gets done even in a meeting of seven people.
right?
so um two or three people is the size in which actual work gets done.
right.
huh.
yeah.
so we'll do that.
great.
oh the other thing we want to do is catch up with uh ellen and see what she's doing.
because the um image schemas are going to be um an important
yeah.
quite relevant yeah.
we we want those.
right?
yeah.
and we want them formalized and stuff like that.
oh yeah.
yeah.
so let me let me make a note to do that.
okay.
yeah i'm actually probably going to be in contact with her uh pretty soon anyway because of various of us students were going to have a reading group about precisely that sort of thing over the summer.
okay.
oh right.
right right right.
that's great.
so
yeah i i shweta mentioned that although she said it's a secret.
okay.
hi.
right.
the faculty aren't faculty aren't supposed to know.
no faculty.
wednesday's much better for me yeah.
but um
i'm sufficiently clueless that i count as a
yeah right.
it's as if we didn't tell anyone at all.
bhaskara.
right.
so uh
all right.
um so i wanted to discuss digits briefly but that won't take too long.
oh good.
right.
okay agenda items.
uh we have digits.
what else we got?
new version of the presegmentation.
new version of presegmentation.
um do we want to say something about the an update of the uh transcript?
yeah why don't you summarize the
update on transcripts.
and i guess that includes some the filtering for the the a s i refs too.
huh.
filtering for what?
for the references that we need to go from the the fancy transcripts to the sort of brain dead.
it'll it'll be basically it'll be a recap of a meeting that we had jointly this morning.
uhhuh.
with don as well.
uhhuh.
got it.
anything else more pressing than those things?
so so why don't we just do those?
you said yours was brief so
okay.
okay well the uh as you can see from the numbers on the digits we're almost done.
the digits goes up to about four thousand.
um and so uh we probably will be done with the t i digits in um another couple weeks um depending on how many we read each time.
so there were a bunch that we skipped.
you know someone fills out the form and then they're not at the meeting and so it's blank.
um but those are almost all filled in as well.
and so once we're it's done it would be very nice to train up a recognizer and actually start working with this data.
so we'll have a corpus that's the size of t i digits?
and so
one particular test set of t i digits.
test set okay.
so i i extracted there was a file sitting around which people have used here as a test set.
it had been randomized and so on.
uhhuh.
and that's just what i used to generate the order of these particular ones.
great.
great.
um.
so i'm impressed by what we could do is take the standard training set for t i digits train up with whatever you know great features we think we have uh for instance and then test on uh this test set.
and presumably uh it should do reasonably well on that.
and then presumably we should go to the distant mike and it should do poorly.
yeah.
right.
and then we should get really smart over the next year or two and it that should get better.
and increase it by one or two percent yeah.
yeah yeah.
um but in order to do that we need to extract out the actual digits.
right.
um so that the reason it's not just a transcript is that there're false starts and misreads and miscues and things like that.
and so i have a set of scripts and x waves where you just select the portion hit r.
um it tells you what the next one should be and you just look for that.
you know so it it'll put on the screen the next set is six nine nine two two.
and you find that and hit the key and it records it in a file in a particular format.
so is this
and so the the question is should we have the transcribers do that or should we just do it?
well some of us.
i've been i've done eight meetings something like that just by hand.
just myself rather.
so it will not take long.
um
uh what what do you think?
my feeling is that
we discussed this right before coffee.
and i think it's a it's a fine idea partly because um it's not unrelated to their present skill set.
but it will add for them an extra dimension it might be an interesting break for them.
and also it is contributing to the uh composition of the transcript.
because we can incorporate those numbers directly and it'll be a more complete transcript.
so i'm i think it's fine that part.
there is there is
so you think it's fine to have the transcribers do it?
uhhuh.
yeah okay.
there's one other small bit which is just entering the information which at which is at the top of this form onto the computer to go along with the where the digits are recorded automatically.
good.
yeah.
and so it's just you know typing in name times time date and so on.
um which again either they can do but it is you know firing up an editor or again i can do.
or someone else can do.
and that you know i'm not that that one i'm not so sure if it's into the the things that i wanted to use the hours for.
because the the time that they'd be spending doing that they wouldn't be able to be putting more words on.
huh.
but that's really your choice it's your
so are these two separate tasks that can happen?
or do they have to happen at the same time before
no they don't have this you have to enter the data before you do the second task but they don't have to happen at the same time.
so it's it's just i have a file which has this information on it and then when you start using my scripts for extracting the times it adds the times at the bottom of the file.
okay.
and so um i mean it's easy to create the files and leave them blank and so actually we could do it in either order.
oh okay.
um it's it's sort of nice to have the same person do it just as a double check to make sure you're entering for the right person.
but either way.
yeah.
yeah just by way of uh uh a uh order of magnitude uh um we've been working with this aurora uh data set.
and uh the best score on the nicest part of the data that is where you've got training and test set that are basically the same kinds of noise and so forth uh is about.
uh
i think the best score was something like five percent uh error per digit.
so that
per digit.
per digit.
you're right.
so if you were doing ten digit uh recognition you would really be in trouble.
uhhuh.
so so the the point there and this is uh car noise uh uh things but but real real situation.
well real
um the uh there's one microphone that's close that they have as as this sort of thing close versus distant.
uh but in a car.
instead of instead of having a projector noise it's it's car noise.
uh but it wasn't artificially added to get some some artificial signal to noise ratio.
it was just people driving around in a car.
so that's that's an indication
that was with many sites competing and this was the very best score and so forth.
so more typical numbers like
although the models weren't that good.
right?
i mean the models are pretty crappy?
you're right.
i think that we could have done better on the models.
but the thing is that we got this this is the kind of typical number for all of the uh uh things in this task all of the um languages.
and so i i think we'd probably the models would be better in some than in others.
huh.
um so uh anyway just an indication once you get into this kind of realm even if you're looking at connected digits it can be pretty hard.
huh.
it's going to be fun to see how we compare at this.
yeah.
how did we do on the t i digits?
very exciting.
well the prosodics are so much different.
it's going to be strange.
i mean the prosodics are not the same as t i digits for example.
yeah.
so i'm i'm not sure how much of effect that will have.
what do you mean the prosodics?
how do
um just what we were talking about with grouping.
that with these the grouping there's no grouping at all and so it's just the only sort of discontinuity you have is at the beginning and the end.
so what are they doing in aurora?
are they reading actual phone numbers?
aurora i don't know.
i don't know what they do in aurora.
or a a digit at a time or
uh i'm not sure how
because it's
no no i mean it's connected it's connected uh digits.
connected.
yeah.
so there's also the not just the prosody but the cross the cross word modeling is probably quite different.
but
but right.
but in t i digits they're reading things like zip codes and phone numbers and things like that.
how
right.
so it's going to be different.
how do we do on t i digits?
i don't remember.
i mean very good.
right?
yeah i mean we were in the
one and a half percent two percent something like that?
uh i no i think we got under a percent.
oh really?
but it was but it's but i mean the very best system that i saw in the literature was a point two five percent or something that somebody had at at bell labs or uh but but uh sort of pulling out all the stops.
okay.
it strikes me that there are more each of them is more informative because it's so random.
all right.
huh.
but i think a lot of systems sort of get half a percent or three quarters a percent.
right.
and we're we're in there somewhere.
but that i mean it's really it's it's close talking mikes no noise clean signal just digits i mean everything is good.
yeah.
it's the beginning of time in speech recognition.
yes exactly.
yeah.
and we've only recently got it to anywhere near human.
it's like the single cell you know it's the beginning of life.
prehistory.
yeah.
and it's still like an order of magnitude worse than what humans do.
right.
yeah.
so
when when they're wide awake yeah.
yeah.
um.
after coffee.
after coffee right.
not after lunch.
okay so um what i'll do then is i'll go ahead and enter this data.
and then hand off to jane and the transcribers to do the actual extraction of the digits.
yeah.
yeah.
one question i have that that i mean we wouldn't know the answer to now but might do some guessing but i was talking before about doing some modeling of uh uh marking of articulatory features with overlap and so on.
huh.
and and um
on some subset.
one thought might be to do this uh on on the digits or some piece of the digits.
uh it'd be easier uh and so forth.
the only thing is i'm a little concerned that maybe the kind of phenomena in
the reason for doing it is because the the argument is that certainly with conversational speech the stuff that we've looked at here before um just doing the simple mapping from um the phone to the corresponding features that you could look up in a book uh isn't right.
it isn't actually right.
in fact there's these overlapping processes where some voicing some up and then some you know some nasality is comes in here and so forth.
and you do this gross thing saying well i guess it's this phone starting there.
so uh that's the reasoning.
but it could be that when we're reading digits because it's it's for such a limited set that maybe maybe that phenomenon doesn't occur as much.
i don't know.
anybody do you have any anybody have any opinion about that?
and that people might articulate more.
and you that might end up with more a closer correspondence.
uhhuh.
yeah.
yeah that's i i agree.
that it's just
sort of less predictability.
and you have to
uhhuh.
yeah.
well it's definitely true that when people are reading even if they're re reading what they had said spontaneously that they have very different patterns.
it's a well would this corpus really be the right one to even try that on?
mitch showed that and some dissertations have shown that.
right.
so the fact that they're reading first of all whether they're reading in a room of people or you know just the fact that they're reading will make a difference.
yeah.
and depends what you're interested in.
see i don't know.
so maybe the thing will be do to take some very small subset mean not have a big program but take a small set uh subset of the conversational speech and a small subset of the digits.
and look and and just get a feeling for it.
um just take a look really.
that could could be an interesting design too because then you'd have the the comparison of the uh predictable speech versus the less predictable speech.
because i don't think anybody is i at least i don't know of anybody uh
well i don't know the answers.
hey
yeah.
and maybe you'd find that it worked in in the case of the of the uh non predictable.
yeah.
have to think about the particular acoustic features to mark too because i mean some things they wouldn't be able to mark like uh you know uh tense lax.
uhhuh.
some things are really difficult.
you know?
just listening.
i think we can get ohala in to give us some advice on that.
well
yeah.
also i thought you were thinking of a much more restricted set of features that
yeah but i i i i was like he said i was going to bring john in and ask john what he thought.
yeah sure.
sure.
yeah.
right.
but i mean you want you want it be restrictive but you also want it to to to have coverage.
right.
you know you should it should be such that if you if you uh if you had um all of the features determined that you that you were uh have chosen that that would tell you uh in the steady state case uh the phone.
yeah.
so um
okay.
even i guess with vowels that would be pretty hard.
wouldn't it?
to identify actually you know which one it is.
it would seem to me that the points of articulation would be more
uh i mean that's i think about articulatory features i think about points of articulation which means uh rather than vowels.
yeah.
points of articulation?
what do you mean?
so is it uh bilabial or dental or is it you know palatal.
uhhuh.
which which are all like where where your tongue comes to rest.
uvular.
place of place of articulation.
place place.
place.
yeah.
place.
thank you what whatever i said that's
okay.
yeah.
i really meant place.
okay i see.
yeah.
okay we got our jargon then okay.
yeah.
uh
well it's also there's really a difference between the pronunciation models in the dictionary and the pronunciations that people produce.
and so you get some of that information from steve's work on the on the labeling.
right.
right.
and it really
i actually think that data should be used more.
that maybe although i think the meeting context is great that he has transcriptions that give you the actual phone sequence.
and you can go from not from that to the articulatory features but that would be a better starting point for marking the gestural features then data where you don't have that.
because we you want to know both about the way that they're producing a certain sound and what kinds of you know what kinds of phonemic differences you get between these transcribed sequences and the dictionary ones.
well you might be right that might be the way at getting at what i was talking about.
but the particular reason why i was interested in doing that was because i remember when that happened and john ohala was over here and he was looking at the spectrograms of the more difficult ones.
uh he didn't know what to say about what is the sequence of phones there.
they came up with some compromise.
because that really wasn't what it look like.
right.
it didn't look like a sequence of phones.
right.
it look like this blending thing happening here and here and here.
right.
yeah so you have this feature here and overlap yeah.
there was no name for that.
yeah.
but right.
yeah.
but it still is there's a there are two steps.
one you know one is going from a dictionary pronunciation of something like going to see you tomorrow.
and or gonta.
right.
yeah.
it could be going to or going to or gonta you know.
right.
and yeah.
going to see you tomorrow uh guh see you tomorrow.
and that it would be nice to have these intermediate or these some these reduced pronunciations that those transcribers had marked or to have people mark those as well.
uhhuh.
because it's not um that easy to go from the dictionary word the dictionary phone pronunciation to the gestural one without this intermediate or a syllable level kind of representation.
well i don't think morgan's suggesting that we do that though.
do you mean
yeah.
yeah i mean i'm at the moment of course we're just talking about what to provide as a tool for people to do research who have different ideas about how to do it.
so for instance you might have someone who just has a has words with states and has uh uh comes from articulatory gestures to that.
and someone else might actually want some phonetic uh intermediate thing.
so i think it would be be best to have all of it if we could.
but um
but what i'm imagining is a score like notation where each line is a particular feature.
yeah.
right?
so you would say you know it's voiced through here and so you have label here and you have nasal here.
yeah.
and they they could be overlapping in all sorts of bizarre ways that don't correspond to the timing on phones.
i mean this is the kind of reason why
i remember when at one of the switchboard workshops that uh when we talked about doing the transcription project dave talkin said can't be done.
right.
he was he was what what he meant was that this isn't you know a sequence of phones.
and when you actually look at switchboard that's not what you see and you know and it
and in in fact the inter annotator agreement was not that good.
right?
on the harder ones?
yeah i mean it
it depends how you look at it and
i i understand what you're saying about this kind of transcription exactly.
yeah.
because i've seen you know where does the voicing bar start and so forth.
yeah.
all i'm saying is that it is useful to have that the transcription of what was really said and which syllables were reduced.
uh if you're going to add the features.
it's also useful to have some level of representation which is is a reduced it's a pronunciation variant that currently the dictionaries don't give you.
uhhuh.
because if you add them to the dictionary and you run recognition you you add confusion.
right.
so people purposely don't add them.
right.
so it's useful to know which variant was was produced at least at the phone level.
so it would be it would be great if we had either these kind of labelings on the same portion of switchboard that steve marked or steve's type markings on this data with these.
right.
that's all i mean.
exactly.
yeah.
exactly.
yeah no i i don't disagree with that.
and steve's type is fairly it's not that slow.
uh uh
i don't know exactly what the timing was but
yeah i don't disagree with it.
the the only thing is that what you actually will end end up with is something
it's all compromised.
right?
so the string that you end up with isn't actually what happened.
but it's it's the best compromise that a group of people scratching their heads could come up with to describe what happened.
uhhuh.
and it's more accurate than phone labels.
but and it's more accurate than the than the dictionary or if you've got a pronunciation uh lexicon that has three or four.
the word.
yeah.
this might be have been the fifth one that you that you pruned or whatever.
so it's like a continuum.
right.
right.
it's you're going all the way down.
so sure.
right.
that's what i meant is
yeah.
yeah.
and in some places it would fill in so the kinds of gestural features are not everywhere.
yeah.
well
right.
so there are some things that you don't have access to either from your ear or the spectrogram.
uhhuh.
but you know what phone it was and that's about all you can all you can say.
right.
and then there are other cases where nasality voicing
it's basically just having multiple levels of of information and marking on the signal.
right.
right.
yeah.
right.
well the other difference is that the the features are not synchronous.
right?
they overlap each other in weird ways.
uhhuh.
uhhuh.
so it's not a strictly one dimensional signal.
right.
so i think that's sort of qualitatively different.
right.
you can add the features in uh but it'll be underspecified.
huh.
there'll be no way for you to actually mark what was said completely by features.
well not with our current system but you could imagine designing a system that the states were features rather than phones.
and if you're
well we we've probably have a separate um discussion of uh of whether you can do that.
that's well isn't that i thought that was well but that wasn't that kind of the direction?
yeah.
i thought
yeah so i mean what what where this is i mean i i want would like to have something that's useful to people other than those who are doing the specific kind of research i have in mind.
so it should be something broader.
but the but uh where i'm coming from is uh we're coming off of stuff that larry saul did with with um uh john dalan and muzim rahim in which uh they uh have um a a multi band system that is uh trained through a combination of gradient learning and e m to um estimate uh the uh value for for for a particular feature.
okay?
and this is part of a larger image that john dalan has about how the human brain does it in which he's sort of imagining that individual frequency channels are coming up with their own estimate of of these these kinds of something like this.
might not be you know exact features that jakobson thought of or something.
but i mean you know some something like that.
some kind of low level features which are not fully you know phone classification.
and the the this particular image of how how it's done is that then given all of these estimates at that level there's a level above it then which is is making some kind of sound unit classification such as you know phone.
and and you know you could argue what what a sound unit should be and and so forth.
but that that's sort of what i was imagining doing.
um and but it's still open within that whether you would have an intermediate level in which it was actually phones or not.
you wouldn't necessarily have to.
um but again i wouldn't want to wouldn't want what we we produced to be so know local in perspective that it it was matched what we were thinking of doing one week.
and and and you know what you're saying is absolutely right.
that that if we can we should put in uh another level of of description there if we're going to get into some of this low level stuff.
well you know um i mean if we're talking about having the annotators annotate these kinds of features it seems like
you know you
the the question is do they do that on meeting data?
or do they do that on switchboard?
that's what i was saying.
maybe meeting data isn't the right corpus.
well it seems like you could do both.
i mean i was thinking that it would be interesting to do it with respect to parts of switchboard anyway in terms of
uhhuh.
uh partly to see if you could generate first guesses at what the articulatory feature would be based on the phone representation at that lower level.
uhhuh.
it might be a time gain.
but also in terms of comparability of um
uhhuh.
well because the yeah and then also if you did it on switchboard you would have the full continuum of transcriptions.
what you gain.
yep.
you'd have it from the lowest level the acoustic features then you'd have the you know the phonetic level that steve did.
uhhuh.
yeah that that's all i was thinking about.
and
it is telephone band so the bandwidth might be
yeah.
and you could tell that
it'd be a complete set then.
and you get the relative gain up ahead.
it's so it's a little different.
yeah.
so i mean you know we'll see how much we can uh get the people to do and how much money we'll have and all this sort of thing.
uhhuh.
uhhuh.
but it it might be good to do what jane was saying uh you know seed it with guesses about what we think the features are based on you know the phone or steve's transcriptions or something to make it quicker.
but
might be do both.
all right so based on the phone transcripts they would all be synchronous but then you could imagine nudging them here and there.
adjusting.
yeah exactly.
scoot the voicing over a little because
yeah.
right.
well i think what i mean i'm i'm a little behind in what they're doing now and uh the stuff they're doing on switchboard now.
but i think that steve and the gang are doing something with an automatic system first and then doing some adjustment.
as i as i recall.
so i mean that's probably the right way to go anyway is to is to start off with an automatic system with a pretty rich pronunciation dictionary that that um you know tries to label it all.
and then people go through and fix it.
so in in our case you'd think about us starting with maybe the regular dictionary entry?
and then
well regular dictionary i mean this is a pretty rich dictionary.
or would we?
it's got got a fair number of pronunciations in it.
or you could start from the if we were going to do the same set of sentences that steve had done we could start with those transcriptions.
but
uhhuh.
yeah.
that's actually what i was thinking is
so i was thinking
right.
yeah.
the problem is when you run uh if you run a regular dictionary um even if you have variants in there which most people don't you don't always get out the actual pronunciations.
yeah.
so that's why the human transcriber's giving you the that pronunciation.
yeah.
actually maybe they're using phone recognizers.
oh.
and so
they they i thought that they were.
is that what they're doing?
they are.
we should catch up on what steve is
oh okay.
uh i think that would be a good good idea.
yeah.
yeah so i think that we also don't have i mean
we've got a good start on it.
but we don't have a really good meeting recorder or recognizer or transcriber or anything yet.
so
yeah.
so i mean another way to look at this is to is to uh do some stuff on switchboard which has all this other stuff to it.
and then um as we get further down the road and we can do more things ahead of time we can do some of the same things to the meeting data.
uhhuh.
okay.
yeah.
and i'm and these people might they they are most of them are trained with i p a.
yeah.
they'd be able to do phonetic level coding or articulatory.
are they busy for the next couple years or
well you know i mean they they they're interested in continuing working with us.
so i mean i and this would be up their alley so we could when the when you meet with with john ohala and find you know what taxonomy you want to apply then they'd be good to train onto it.
yeah.
yeah.
anyway this is not an urgent thing at all.
yeah.
just it came up.
it'd be very interesting though to have that data.
yeah.
i wonder how would you do a forced alignment?
i think so too.
might
interesting idea.
to to i mean you'd want to iterate somehow.
yeah.
it's interesting thing to think about.
it might be
huh.
i was thinking it might be
i mean you'd you'd want models for spreading.
of the acoustic features?
yeah.
uhhuh.
uhhuh.
yeah.
well it might be neat to do some phonetic features on these non word words.
are are these kinds of words that people never the huh s and the huh s and the huh and the uh?
these no i'm serious.
there are all these kinds of functional uh elements.
i don't know what you call them.
but not just fill pauses but all kinds of ways of interrupting and so forth.
uhhuh.
and some of them are yeah uhhuh s and huh s and huh.
huh okay uh grunts.
uh
that might be interesting.
he's got lip lip smacks.
in the meetings.
we should move on.
yeah.
uh new version of uh presegmentation?
uh oh yeah um i worked a little bit on the on the presegmentation to to get another version which does channel specific uh speech-nonspeech detection.
and what i did is i used some normalized features which uh look into the which is normalized energy uh energy normalized by the mean over the channels and by the minimum over the other.
within each channel.
and to to huh to yeah to normalize also loudness and and modified loudness and things and that those special features actually are in my feature vector.
oh.
and and therefore to be able to uh somewhat distinguish between foreground and background speech in in the different in each channel.
and uh i tested it on on three or four meetings and it seems to work well yeah fairly well i i would say.
there are some problems with the lapel mike.
of course.
yeah.
wow that's great.
uh yeah.
and
so i i understand that's what you were saying about your problem with minimum.
yeah.
and
yeah and and i had i had uh specific problems with
i get it.
so new use ninetieth quartile rather than minimum.
yeah.
yeah.
wow!
yeah yeah then i i did some some some things like that.
interesting.
as there there are some some problems in when in the channel there they the the speaker doesn't doesn't talk much or doesn't talk at all.
then the yeah there are there are some problems with with with with normalization and then uh there the system doesn't work at all.
so i'm i'm glad that there is the the digit part where everybody is forced to say something.
right.
so that's that's great for for my purpose.
and the thing is i i then the evaluation of of the system is a little bit hard as i don't have any references.
well we did the hand
the one by hand.
yeah that's the one one where i do the training on so i can't do the evaluation on.
so the thing is can the transcribers perhaps do some some some meetings in in terms of speech-nonspeech in in the specific channels?
uh.
well won't you have that from their transcriptions?
well i have
well okay so now we need
no because we need is really tight.
yeah.
so um
i think i might have done what you're requesting though i did it in the service of a different thing.
oh great.
i have thirty minutes that i've more tightly transcribed with reference to individual channels.
okay.
okay that's great.
that's great too.
and i could and
yeah so
hopefully that's not the same meeting that we did.
and
no actually it's a different meeting.
good.
so um so the you know we have the they transcribe as if it's one channel with these with the slashes to separate the overlapping parts.
okay.
and then we run it through then it then i'm going to edit it and i'm going to run it through channelize which takes it into dave gelbart's format.
yeah.
yeah.
and then you have all these things split across according to channel.
and then that means that if a person contributed more than once in a given overlap during that time bend that that two parts of the utterance end up together it's the same channel.
and then i took his tool and last night for the first thirty minutes of one of these transcripts i tightened up the um boundaries on individual speakers' channels.
okay.
okay.
because his his interface allows me to have total flexibility in the time tags across the channels.
yeah.
yeah.
and um so.
so current this week.
so yeah yeah that that that's great but what would be nice to have some more meetings not just one meeting to to be sure that that there is a system.
yes.
might not be what you need.
yeah so if we could get a couple meetings done with that level of precision i think that would be a good idea.
okay.
oh okay.
yeah.
uh how how much
so the meetings vary in length.
what are we talking about in terms of the number of minutes you'd like to have as your as your training set?
it seems to me that it would be good to have a few minutes from from different meetings.
so
but i'm not sure about how much.
okay now you're saying different meetings because of different speakers or because of different audio quality or both or
both.
both.
okay.
different different number of speakers different speakers different conditions.
yeah we don't have that much variety in meetings yet.
uh i mean we have this meeting and the feature meeting and we have a couple others that we have uh couple examples of but but uh
yeah
yeah.
uhhuh.
even probably with the gains differently will affect it you mean
uh not really as
potentially.
oh because you use the normalization?
because of the normalization yeah.
okay.
oh okay.
yeah.
we can try running
we haven't done this yet because um uh andreas is is going to move over the s r i recognizer.
basically i ran out of machines at s r i.
because we're running the evals.
okay.
and i just don't have machine time there.
but once that's moved over uh hopefully in a a couple days then we can take um what jane just told us about as the presegmented the the segmentations that you did at level eight or at some threshold that jane right and try doing forced alignment um on the word strings.
oh shoot!
yeah.
yeah.
the presegment.
yeah.
yeah.
with the recognizer?
yeah.
and if it's good then that will that may give you a good boundary.
of course if it's good we don't then we're we're fine.
yeah.
but i don't know yet whether these segments that contain a lot of pauses around the words will work or not.
i i would quite like to have some manually transcribed references for for the system as i'm not sure if if it's really good to compare with with some other automatic found boundaries.
yeah.
right.
well no if we were to start with this and then tweak it manually would that that would be okay?
yeah.
right.
yeah sure.
they might be okay.
okay.
it you know it really depends on a lot of things.
yeah.
but i would have maybe a transcriber uh look at the result of a forced alignment and then adjust those.
yeah.
to adjust them or
yeah.
yeah yeah.
that might save some time.
if they're horrible it won't help at all.
yeah great.
but they might not be horrible.
yeah.
so but i'll let you know when we uh have that.
okay great.
how many minutes would you want from
i mean we could easily get a section you know like say a minute or so from every meeting that we have so from the newer ones that we're working on everyone that we have.
and then
should provide this.
if it's not the first minute of of the meeting that that's okay with me.
but in in the first minute uh often there are some some strange things going on which which aren't really well for which which aren't really good.
so what what i'd quite like perhaps is to have some five minutes of of of different meetings.
so
somewhere not in the very beginning five minutes okay.
yeah.
and then i wanted to ask you just for my information.
then would you be
because i don't quite
so would you be training then um the segmenter so that it could on the basis of that segment the rest of the meeting?
so if i give you like five minutes is the idea that this would then be applied to uh to providing tighter time bands?
i i could do a a retraining with that yeah.
wow interesting.
that's but but i hope that i i don't need to do it.
okay.
uhhuh.
so uh it can be do in an unsupervised way.
excellent.
so
excellent okay.
i'm i'm not sure.
but for for for those three meetings which i which i did it seems to be quite well.
but there are some some as i said some problems with the lapel mike.
but perhaps we can do something with with cross correlations to to get rid of the of those.
and
yeah.
that's that's what i that's my future work.
well well what i want to do is to to look into cross correlations for for removing those false overlaps.
wonderful.
are the um wireless different than the wired mikes at all?
i mean have you noticed any difference?
i'm i'm not sure um if if there are any wired mikes in those meetings or uh i have have to have a look at them.
but i'm i'm i think there's no difference between.
so it's just the lapel versus everything else?
yeah.
yeah.
okay so then if that's five minutes per meeting we've got like twelve minutes twelve meetings roughly that i'm that i've been working with then.
of of of the meetings that you're working with how many of them are different
no.
are there any of them that are different than these two meetings?
well oh in terms of the speakers or the conditions or the
yeah speakers.
sorry.
so
um we have different combinations of speakers.
yeah that
i mean just from what i've seen uh there are some where um you're present or not present.
and then then you have the difference between the networks group and this group.
yeah i know some of the n s a meetings yeah.
yeah.
so i didn't know in the group you had if you had
yeah.
so you have the networks meeting?
yep we do.
yeah.
do you have any of jerry's meetings in your pack uh
um no.
no.
we could
i mean you you recorded one last week or so.
i could get that new one in this week i get that new one in.
yep.
we're going to be recording them every monday.
yeah.
because i think he really needs variety.
so
and and having as much variety for speaker certainly would be a big part of that i think.
great.
yeah.
yeah.
okay so if i
okay included include
okay then uh if i were to include all together samples from twelve meetings that would only take an hour.
and i could get the transcribers to do that.
right?
i mean what i mean is that would be an hour sampled and then they'd transcribe those that hour.
right?
that's what i should do?
yeah.
and
right.
but you're
that's that's
i don't mean transcribe.
i mean i mean adjust.
so they get it into the multi channel format and then adjust the timebands so it's precise.
so that should be faster than the ten times kind of thing.
absolutely.
yeah.
i did i did um uh so last night i did uh
oh gosh.
well last night i did about half an hour in three hours which is not terrific.
yeah.
but um
yeah.
anyway it's an hour and a half per
well that's probably
so
well i can't calculate on my on my feet.
do the transcribers actually start with uh transcribing new meetings or are they
well um they're still working they still have enough to finish that i haven't assigned a new meeting.
okay.
but the next
i was about to need to assign a new meeting.
and i was going to take it from one of the new ones.
okay.
and i could easily give them jerry feldman's meeting.
no problem.
and then
okay.
so they're really running out of data i mean that's good.
uhhuh.
uh that first set.
um okay.
they're running out of data unless we make the decision that we should go over and start uh transcribing the other set.
so
there the first the first half.
and so i was in the process of like editing them but this is wonderful news.
yeah.
okay.
all right.
we funded the experiment with uh
also we were thinking maybe applying that that to getting the
yeah that'll be very useful to getting the overlaps to be more precise all the way through.
so this blends nicely into the update on transcripts.
yes it does.
okay.
so um um liz and and don and i met this morning in the barco room with the lecture hall.
yeah please.
go ahead.
and this afternoon.
and this afternoon.
it drifted into the afternoon uh concerning this issue of um the well there's basically the issue of the interplay between the transcript format and the processing that they need to do for the s r i recognizer.
and um well so i mentioned the process that i'm going through with the data.
so you know i get the data back from the well uh metaphorically get the data back from the transcriber.
and then i check for simple things like spelling errors and things like that.
and um i'm going to be doing a more thorough editing with respect to consistency of the conventions.
but they're they're generally very good.
and then i run it through uh the channelize program to get it into the multi channel format okay.
and the what we discussed this morning i would summarize as saying that um these units that result in a a particular channel and a particular timeband at at that level um vary in length.
and um their recognizer would prefer that the units not be overly long.
but it's really an empirical question whether the units we get at this point through just that process i described might be sufficient for them.
so as a first pass through a first chance without having to do a lot of hand editing what we're going to do is i'll run it through channelize give them those data after i've done the editing process and be sure it's clean.
and i can do that pretty quickly with just that minimal editing without having to hand break things.
uhhuh.
and then we'll see if the units that we're getting uh with the at that level are sufficient.
and maybe they don't need to be further broken down.
and if they do need to be further broken down then maybe it just be piece wise maybe it won't be the whole thing.
so that's that's what we were discussing this morning as far as i among
right.
also we discussed some adaptational things.
then lots of
right.
so it's like
uh you know i hadn't uh incorporated a convention explicitly to handle acronyms for example.
but if someone says p z m it would be nice to have that be directly interpretable from the transcript what they said.
uhhuh.
or uh tcl t c l i mean.
it's like
it's and so um i've i've incorporated also convention with that.
but that's easy to handle at the post editing phase.
and i'll mention it to transcribers for the next phase.
but that's okay.
and then a similar uh convention for numbers.
so if they say one eighty three versus one eight three.
um and also i'll be um encoding as i do my post editing the things that are in curly brackets which are clarificational material.
and uh to incorporate uh keyword at the beginning.
so it's going to be either a gloss or it's going to be a vocal sound like a laugh or a cough or so forth.
or a non vocal sound like a door slam and that can be easily done with a you know just a one little additional thing in the in the general format.
yeah we we just needed a way to strip you know all the comments all the things the that linguist wants but the recognizer can't do anything with.
um but to keep things that we mapped to like reject models or you know uh mouth noise or cough.
and then there's this interesting issue jane brought up.
which i hadn't thought about before.
but i was realizing as i went through the transcripts that there are some noises like
um
well the good example was an in breath where a transcriber working from the mixed signal doesn't know whose breath it is.
right.
and they've been assigning it to someone that may or may not be correct.
and what we do is if it's a breath sound you know a sound from the speaker we map it to a noise model like a mouth noise model in the recognizer.
and yeah it probably doesn't hurt that much once in a while to have these.
but if they're in the wrong channel that's not a good idea.
and then there's also things like door-slams that's really in no one's channel.
they're like it's in the room.
right.
yeah.
and uh jane had this nice uh idea of having like an extra uh couple tiers
an extra channel.
yeah.
yeah.
i've been i've been adding that to the ones i've been editing.
and we were thinking that is useful also when there's uncertainties.
so if they hear a breath and they don't know who breath it is it's better to put it in that channel than to put it in the speaker's channel.
because maybe it was someone else's breath.
or uh
so i think that's a good you can always clean that up post processing.
yeah.
so a lot of little details.
but i think we're coming to some kind of closure on that.
so the idea is then uh don can take uh jane's post processed channelized version and with some scripts you know convert that to to a reference for the recognizer.
and we can can run these.
so when that's ready you know as soon as that's ready and as soon as the recognizer is here we can get twelve hours of force aligned and recognized data.
and you know start working on it.
and
so we're i don't know a a week or two away i would say from.
uh if if that process is automatic once we get your post process transcript.
uhhuh.
and that doesn't the amount of editing that it would require is not very much either.
i'm just hoping that the units that are provided in that way will be sufficient.
because i would save a lot of uh time dividing things.
yeah some of them are quite long.
just from
i don't know.
how long were
you did one
i saw a couple around twenty seconds.
and that was just without looking too hard for it.
so i would imagine that there might be some that are longer.
right.
well
one question.
would that be a single speaker or is that multiple speakers overlapping?
no no but if we're going to segment it
like if there's one speaker in there that says okay or something right in the middle it's going to have a lot of dead time around it.
right it's not the it's not the fact that we can't process a twenty second segment it's the fact that there's twenty seconds in which to place one word in the wrong place.
so it's not
yeah.
you know if if someone has a very short utterance there.
yeah.
and that's where we might want to have this individual you know have your pre process input.
yep.
yeah.
sure.
and i just don't know.
i i i thought that perhaps the transcribers could start then from the those multi channel uh speech-nonspeech detections if they would like to.
that's very important.
i have to run it.
right.
in in doing the hand marking?
yeah.
yeah that's what i was thinking too.
right.
so that's probably what will happen.
yeah.
but we'll try it this way and see.
yeah.
i mean it's probably good enough for force alignment.
if it's not then we're really then we definitely
yeah.
uh but for free recognition i'm it'll probably not be good enough.
we'll probably get lots of errors because of the cross talk and noises and things.
yep.
good.
i think that's probably our agenda or starting up there.
oh i wanted to ask one thing.
yeah?
the microphones the new microphones.
when do we get uh
uh they said it would take about a week.
oh exciting.
k.
k.
you ordered them already?
uhhuh.
great.
so what happens to our old microphones?
they go where old microphones go.
um
do we give them to someone?
or
well the only thing we're going to have extra for now
we don't have more receivers.
we just have
right we
right.
so the only thing we'll have extra now is just the lapel.
not not the bodypack.
just the lapel.
just the lapel itself.
um and then one of the one of those.
since what i decided to do on morgan's suggestion was just get two new microphones um and try them out.
uhhuh.
and then if we like them we'll get more.
okay.
yeah.
since they're they're like two hundred bucks a piece.
we won't uh at least try them out.
so it's a replacement for this headset mike.
yep yep.
yeah.
and they're going to do the wiring for us.
what's the um style of the headset?
it's um it's by crown.
and it's one of these sort of mount around the ear thingies.
and uh when i when i mentioned that we thought it was uncomfortable he said it was a common problem with the sony.
and this is how apparently a lot of people are getting around it.
huh.
and i checked on the web and every site i went to raved about this particular mike.
it's apparently comfortable and stays on the head well.
so we'll see if it's any good.
but uh i think it's promising.
you said it was used by aerobics instructors?
yup.
yep yep so it was it was advertised for performers.
that says a lot.
for the for the record adam is not a paid employee or a consultant of crown.
and
excuse me?
oh.
excuse me?
right.
i said for the record adam is is not a paid consultant or employee of crown.
that's right.
however he may be solicited after these meetings are distributed.
well we're using the crown p z ms.
don't worry about finishing your dissertation.
yeah.
these are crown.
aren't they?
the p z ms are crown.
all right.
aren't they?
yeah.
yeah i thought they were.
you bet you bet.
and they work very well.
yes.
so if we go to a workshop about all this this it's going to be a meeting about meetings about meetings.
okay.
so
and then it we have to go to the planning session for that workshop.
oh yeah what which will be the meeting about the meeting about the meeting.
because then it would be a meeting about the meeting about the meeting about meetings.
yeah.
just start saying m four.
yeah okay.
yeah.
m to the fourth.
should we do the digits?
yep go for it.
okay.
so uh you see don the unbridled excitement of the work that we have on this project.
okay.
it's just uh
um
uh you know it doesn't seem like a bad idea to have that information.
and i'm surprised i sort of i'm surprised i forgot that.
but uh i think that would be a good thing to add.
yeah i i'd i think it's
after i just printed out a zillion of them.
yeah well that's
um so i i do have a an agenda suggestion.
uh we i think the things that we talk about in this meeting uh tend to be a mixture of uh procedural uh mundane things and uh research points.
and um i was thinking i think it was a meeting a couple of weeks ago that we we spent much of the time talking about the mundane stuff because that's easier to get out of the way.
and then we sort of drifted into the research and maybe five minutes into that andreas had to leave.
so uh i'm suggesting we turn it around.
and and uh sort of we have anybody has some mundane points that we could send an email later uh hold them for a bit and let's talk about the the research y kind of things.
um so um the one one thing i know that we have on that is uh we had talked a a couple weeks before um uh about the uh the stuff you were doing with with uh um uh attempting to locate events.
we had a little go around trying to figure out what you meant by events.
but i think you know what we had meant by events i guess was uh points of overlap between speakers.
but i i gather from our discussion a little earlier today that you also mean uh interruptions with something else.
like some other noise.
yeah.
yes?
uhhuh yeah.
you mean that as an event also?
so at any rate you were you've you've done some work on that.
right.
and um then the other thing would be it might be nice to have a preliminary discussion of some of the other uh research uh areas that uh we're thinking about doing.
um i think especially since you you haven't been in in these meetings for a little bit.
maybe you have some discussion of some of the the plausible things to look at now that we're starting to get data.
uh and one of the things i know that also came up uh is some discussions that that uh that uh jane had with lokendra.
uh about some some some um uh work about
i i i i i don't want to try to say because i i'll say it wrong.
but anyway some some potential collaboration there about about the about the working with these data.
oh.
so
sure.
so uh
you want to just go around?
uh well i don't know if we if this is sort of like everybody has something to contribute sort of thing.
i think there's just just a couple a couple people primarily.
um but um
uh why don't
actually i think that that last one i just said we could do fairly quickly.
so why don't you you start with that?
okay.
shall i shall i just start?
yeah just explain what it was.
okay.
um so uh he was interested in the question of you know relating to his to the research he presented recently um of inference structures.
and uh the need to build in um this this sort of uh mechanism for understanding of language.
and he gave the example in his talk about how um
i'm remembering it just off the top of my head right now.
but it's something about how um joe slipped you know john had washed the floor or something like that.
and i don't have it quite right.
but that kind of thing.
where you have to draw the inference that okay there's this time sequence.
but also the the the causal aspects of the uh floor and and how it might have been the cause of the fall.
and that um it was the other person who fell than the one who cleaned it.
and it these sorts of things.
so i looked through the transcript that we have so far and um identified a couple different types of things of that type.
and um one of them was something like uh during the course of the transcript um um we had gone through the part where everyone said which channel they were on and which device they were on.
and um the question was raised well should we restart the recording at this point.
and and dan ellis said well we're just so far ahead of the game right now we really don't need to.
now how would you interpret that without a lot of inference?
so the inferences that are involved are things like okay so how do you interpret ahead of the game.
you know so it's the it's what you what you what you draw you know the conclusions that you need to draw are that space is involved in recording.
huh metaphorically.
that um that we have enough space.
and he continues like we're so ahead of the game because now we have built in down sampling.
so you have to sort of get the idea that um ahead of the game is speaking with respect to space limitations.
that um that in fact down sampling is gaining us enough space.
and that therefore we can keep the recording we've done so far.
but there are a lot of different things like that.
so do you think his interest is in using this as a data source?
or training material?
or what?
well i i should maybe interject to say this started off with a discussion that i had with him.
so um we were trying to think of ways that his interests could interact with ours.
uhhuh.
and um uh i thought that if we were going to project into the future when we had a lot of data uh and um such things might be useful for that in before we invested too much uh effort into that he should uh with jane's help look into some of the data that we're already have.
uhhuh.
and see is there anything to this at all.
is there any point which you think that you know you could gain some advantage and some potential use for it.
because it could be that you'd look through it and you say well this is just the wrong task for for him to pursue his
wrong.
yeah.
and and uh i got the impression from your mail that in fact there was enough things like this just in the little sample that that you looked at that that it's plausible at least.
it's possible.
uh he was he he you know we met and he was going to go and uh you know look through them more systematically.
yeah.
and then uh meet again.
yeah.
so it's you know not a matter of a
yeah.
but yeah i think i think it was optimistic.
so anyway that's that's a quite different thing from anything we've talked about that you know might might might come out from some of this.
but he can use text basically.
i mean he's talking about just using text.
that's his major
i mentioned several that had to do with implications drawn from intonational contours.
pretty much?
or
and that wasn't as directly relevant to what he's doing.
okay.
he's interested in these these knowledge structures.
yeah interesting.
inferences that you draw from.
i mean he certainly could use text.
but we were in fact looking to see if there is there is there something in common between our interest in meetings and his interest in in in this stuff.
so
and i imagine that transcripts of speech i mean text that is speech probably has more of those than sort of prepared writing.
i i don't know whether it would or not but it seems like it would.
i don't know.
probably probably depends on what the prepared writing was.
but
yeah i don't think i would make that leap.
because in narratives you know i mean if you spell out everything in a narrative it can be really tedious.
uhhuh.
so
yeah i'm just thinking you know when you're when you're face to face you have a lot of backchannel.
and and
oh!
that aspect.
yeah.
and so i think it's just easier to do that sort of broad inference jumping if it's face to face.
i mean so if i just read that dan was saying we're ahead of the game in that in that context.
well
yeah.
i might not realize that he was talking about disk space as opposed to anything else.
i you know i i had several that had to do with backchannels and this wasn't one of them.
uhhuh.
this this one really does um make you leap from
so he said you know we're ahead of the game we have built in down sampling.
uhhuh.
and the inference if you had it written down would be
i guess it would be the same.
uhhuh.
but there are others that have backchanneling.
it's just he was less interested in those.
can i
sorry to interrupt.
um i i've a minute uh several minutes ago i like briefly was was not listening.
and so who is he in this context?
yeah there's a lot of pronoun
okay.
so i was just realizing we've you guys have been talking about he um for at least uh i don't know three three four minutes without ever mentioning the person's name again.
i believe it.
yeah actually to make it worse uh morgan uses you and you.
so this is this is this is going to be a big big problem if you want to later do uh you know indexing or speech understanding of any sort.
it's in my notes.
with gaze and no identification.
or
i just wrote this down.
you just wrote this?
yeah actually because morgan will say well you had some ideas.
yeah.
and he never said he looked
well i think he's doing that intentionally.
right.
aren't you?
so it's great.
right.
so this is really great.
yeah.
because the thing is because he's looking at the even for addressees in the conversation.
uhhuh.
i bet you could pick that up in the acoustics.
just because your gaze is also correlated with the directionality of your voice.
uhhuh.
could be.
yeah that would be
oh that would be interesting.
can
yeah.
yeah so that i mean to even know um when
yeah if you have the p z m's you should be able to pick up what a person is looking at from their voice.
well especially with morgan with the way we have the microphones arranged i'm sort of right on axis.
and it would be very hard to tell.
right.
uh
oh but you'd have the
put morgan always like this.
and
you'd have fainter
well these
wouldn't you get fainter reception out here?
sure but i think if i'm talking like this
right now i'm looking at jane and talking.
now i'm looking at chuck and talking.
i don't think the microphones would pick up that difference.
but you don't have this this problem.
i see.
morgan is the one who does this most.
so if i'm talking at you or i'm talking at you.
i probably been no i i think i've been affected by too many conversations where we were talking about lawyers and talking about and concerns about oh gee is somebody going to say something bad and so on.
lawyers.
and so i so i'm i'm tending to stay away from people's names even though uh
i am too.
even though you could pick up later on just from the acoustics who you were who you were looking at.
i am too.
and we did mention who he was.
yeah.
early in the conversation.
yeah.
right but i missed it.
but it was uh
do can i say
yeah.
yeah yeah.
or or is that just too sensitive?
yeah.
yeah no no there's
no no it isn't sensitive at all.
well
i was just i was just i was overreacting just because we've been talking about it.
and in fact it is it is it is sensitive.
it's okay to
no but that
it's interesting.
i i came up with something from the human subjects people that i wanted to mention.
i mean it fits into the area of the mundane.
but they did say
you know i asked her very specifically about this clause of how um you know it says no individuals will be identified.
uh in any publication using the data.
okay well individuals being identified.
let's say you have a a snippet that says joe uh thinks such and such about about this field but i think he's wrongheaded.
now i mean we're we're going to be careful not to have the wrongheaded part in there.
but but you know let's say we say you know joe used to think so-and-so about this area in his publication he says that but i think he's changed his mind or whatever.
but i
then the issue of of being able to trace joe because we know he's well known in this field and all this and and tie it to the speaker whose name was just mentioned a moment ago can be sensitive.
so i think it's really really kind of adaptive and wise to not mention names any more than we have to.
because if there's a slanderous aspect to it then how much to we want to be able to have to remove.
yeah well there's that.
but i i mean i think also to some extent it's just educating the human subjects people in a way.
because there's if uh you know there's court transcripts there's there's transcripts of radio shows.
i mean people say people's names all the time.
so i think it it can't be bad to say people's names.
it's just that i mean you're right that there's more if we never say anybody's name then there's no chance of of of slandering anybody.
but then it won't i mean if we if we
but
it's not a meeting.
yeah i mean we should do whatever's natural in a meeting if if we weren't being recorded.
yeah.
right so i so my behavior is probably not natural.
so
if person x
well my feeling on it was that it wasn't really important who said it.
you know?
yeah.
well if you since you have to um go over the transcripts later anyway you could make it one of the jobs of the people who do that to mark
well we we we talked about this during the anonymization.
if we want to go through and extract from the audio and the written every time someone says a name.
right.
and i thought that our conclusion was that we didn't want to do that.
yeah we really can't.
but actually i'm sorry i really would like to push finish this off.
i understand.
no i just i just was suggesting that it's not a bad policy potentially.
so it's
so we need to talk about this later.
yeah i i didn't intend it an a policy though.
it was it was just it was just unconscious well semi conscious behavior.
uhhuh.
i sort of knew i was doing it but it was
well i still don't know who he is.
i i i don't remember who he is.
no you have to say you still don't know who he is with that prosody.
uh.
uh we were talking about dan at one point and we were talking about lokendra at another point.
oh.
yeah depends on which one you mean.
and i don't i don't remember which which part.
it's ambiguous so it's okay.
uh i think
well the inference structures was lokendra.
but no the inference stuff was was was lokendra.
okay.
yeah yeah yeah.
that makes sense yeah.
um
and the down sampling must have been dan.
yeah.
good.
yeah.
it's an inference.
yeah you could do all these inferences.
yeah.
yeah.
yeah.
um i i would like to move it into into uh what jose uh has been doing.
because he's actually been doing something.
yeah.
uhhuh.
so right.
okay.
as opposed to the rest of us.
okay.
i i remind that me my first objective uh in the project is to to study difference parameters.
to to find a a good solution to detect uh the overlapping zone in uh speech recorded.
but uh tsk uh in that way i i i begin to to study and to analyze the ehn the recorded speech uh the different session.
to to find and to locate and to mark uh the the different overlapping zone.
and uh so uh i was uh i am transcribing the the first session.
and i i have found uh uh one thousand acoustic events.
uh besides the overlapping zones uh i i i mean the uh breaths uh aspiration uh uh talk uh uh clap.
uh i don't know what is the different names uh you use to to name the the speech.
non speech sounds.
yeah.
oh i don't think we've been doing it at that level of detail.
so
yeah.
uh i i i i don't need to to to huh to to label the the different acoustic.
but i prefer because uh i would like to to study if uh i i will find uh uh a good uh parameters uh to detect overlapping.
i would like to to to test these parameters uh with the another uh uh acoustic events.
to nnn to uh to find what is the um the false uh the false uh hypothesis uh nnn which uh are produced when we use the the um this uh parameter uh i mean pitch uh uh difference uh feature.
uhhuh.
you know i think some of these um that are the non speech overlapping events may be difficult even for humans to tell that there's two there.
so it was
yeah.
i mean if it's a tapping sound you wouldn't necessarily or you know something like that it'd be it might be hard to know that it was two separate events.
yeah.
yeah.
yeah.
yeah.
well you weren't talking about just overlaps.
were you?
you were just talking about acoustic events.
i i i i i i talk uh about uh acoustic events in general.
someone starts someone stops.
yeah.
but uh my my objective uh will be uh to study uh overlapping zone.
oh.
uhhuh.
uh uh in twelve minutes i found uh uh one thousand acoustic events.
how many overlaps were there uh in it?
no no how many of them were the overlaps of speech though?
how many?
uh almost uh three hundred uh in one session.
oh god!
in five uh in forty five minutes.
ugh.
three hundred overlapping speech.
three hundred overlapping zone.
overlapping speech.
with the overlapping zone overlapping speech speech what uh different duration.
uhhuh.
sure.
does this
so if you had an overlap involving three people how many times was that counted?
yeah three people two people.
uh um i would like to consider uh one people with difference noise uh in the background.
no no but i think what she's asking is if at some particular for some particular stretch you had three people talking instead of two did you call that one event?
oh.
oh.
yeah.
i consider one event uh for for that uh for all the zone.
this i i i i consider i consider uh an acoustic event the overlapping zone the period where three speaker or uh are talking together.
well so let's
so let's say me and jane are talking at the same time.
and then liz starts talking also over all of us.
how many events would that be?
i don't understand.
so two people are talking and then a third person starts talking.
yeah?
is there an event right here?
uh no no no.
for me is the overlapping zone.
because because you you have you have more one uh more one voice uh uh produced in a in in a moment.
so if two or more people are talking
i see.
okay.
yeah so i think yeah we just wanted to understand how you're defining it.
yeah.
so then in the region between since there is some continuous region in between regions where there is only one person speaking.
uhhuh.
and one contiguous region like that you're calling an event.
is it are you calling the beginning or the end of it the event?
uhhuh.
yeah.
or are you calling the entire length of it the event?
i consider the the nnn the nnn nnn uh the entirety.
uh uh all all the time there were the voice has overlapped.
okay.
this is the idea.
but uh i i don't distinguish between the the numbers of uh speaker.
uh i'm not considering uh the the um uh the fact of uh uh for example what did you say.
uh at first uh uh two talkers are uh uh speaking.
and uh uh a third person uh join to to that.
for me it's uh it's uh all overlap zone with uh several numbers of speakers is uh uh the same acoustic event.
but uh without any mark between the zone of the overlapping zone with two speakers uh speaking together.
and the zone with the three speakers.
that would just be one.
it one one.
okay.
uh with uh a beginning mark and the ending mark.
got it.
because uh for me is the is the zone with uh some kind of uh distortion the spectral.
i don't mind by the moment by the moment.
well but but you could imagine that three people talking has a different spectral characteristic than two.
i i don't
yeah but uh but uh i have to study what will happen in a general way.
could.
so you had to start somewhere.
yeah we just
i i don't know what uh will will happen with the
so there's a lot of overlap.
yep.
so
that's a lot of overlap.
yeah?
so again that's that's three three hundred in forty five minutes that are that are speakers just speakers.
for forty five minutes.
yeah.
yeah.
yeah.
uhhuh okay yeah.
but a a a
so that's about eight per minute.
but a thousand events in twelve minutes that's
but
yeah.
but that can include taps.
uh yeah.
but
well but a thousand taps in eight minutes is a in twelve minutes is a lot.
general
actually
i i i consider i consider acoustic events uh the silent too.
silent.
silence starting or silence ending
yeah silent ground.
to to detect uh because i consider acoustic event all the things are not uh speech.
oh okay.
uhhuh.
oh.
in in in in a general point of view.
oh.
okay so how many of those thousand were silence?
all right.
not speech not speech or too much speech.
in the
too much speech.
right.
so how many of those thousand were silence silent sections?
yeah.
uh silent.
i i i i don't i i haven't the
uh i i would like to to do a stylistic study.
yeah.
and give you uh with the report uh from uh the the study from the the the session one session.
yeah.
yeah.
and i i found that uh another thing.
when uh uh i i i was uh look at uh nnn the difference speech file.
um for example uh if uh we use the um the mixed file to to transcribe the the events and the words i i saw that uh the uh speech signal collected by the uh this kind of mike uh of this kind of mike uh are different from the uh mixed signal uh we uh collected by headphone.
yep.
right.
and it's right.
yeah.
but the problem is the following.
the the the
i i i knew that uh the signal uh uh would be different.
but uh the the problem is uh uh we uh detected uh difference events in the speech file uh collected by by that mike uh compared with the mixed file.
and so if when you transcribe uh only uh using the nnn the mixed file it's possible uh if you use the transcription to evaluate a different system.
it's possible you uh in the uh
and you use the uh speech file collected by the uh fet mike to uh to nnn to do the experiments with the the system.
uhhuh.
right.
its possible to evaluate uh uh or to consider uh acoustic events that which you marked uh in the mixed file but uh they don't appear in the uh speech signal uh collected by the by the mike.
right.
the the reason that i generated the mixed file was for i b m to do word level transcription not speech event transcription.
yeah.
yeah oh it's a good idea.
it's a good idea i think.
so i agree that if someone wants to do speech event transcription that the mixed signals here.
yeah.
i mean if i'm tapping on the table you it's not going to show up on any of the mikes but it's going to show up rather loudly in the p z m.
yeah.
yeah.
yeah.
so and i i i say uh that uh uh or this uh only because uh i i i in my opinion it's necessary to uh to uh to put the transcription on the speech file collected by the objective signal.
so
i mean the the the signal collected by the uh the real mike in the future in the prototype to to uh correct the initial uh segmentation uh with the uh real speech.
uhhuh.
the the the far field.
yeah.
and you have to to analyze.
you have to to process.
because i i found a difference.
yeah well just i mean just in that that one ten second or whatever it was example that adam had that that we we passed on to others a few months ago.
there was that business where i i guess it was adam and jane were talking at the same time.
and and uh in the close talking mikes you couldn't hear the overlap and in the distant mike you could.
so yeah it's clear that if you want to study if you want to find all the places where there were overlap it's probably better to use a distant mike.
that's good.
on the other hand there's other phenomena that are going on at the same time for which it might be useful to look at the close talking mikes.
yeah.
so it's
but why can't you use the combination of the close talking mikes time aligned?
if you use the combination of the close talking mikes you would hear jane interrupting me.
but you wouldn't hear the paper rustling.
and so if you're interested in
i i mean if you're interested in speakers overlapping other speakers and not the other kinds of non speech that's not a problem.
some of it's masking masked.
yeah.
were you interrupting him or was he interrupting you?
right.
right.
right.
although the other issue is that the mixed close talking mikes
yeah.
i mean i'm doing weird normalizations and things like that.
but it's known.
yeah.
yep.
i mean the normalization you do is over the whole conversation.
right.
isn't it?
over the whole meeting.
yep.
so if you wanted to study people overlapping people that's not a problem.
i i i think uh
i saw the nnn the uh
but uh i uh i have uh any results.
i i i saw the the speech file collected by uh the fet mike.
and uh uh signal uh to uh to noise uh relation is uh low.
uhhuh.
it's low.
it's very low you would if we compare it with uh the headphone.
yep.
and
i i found that nnn that uh um probably
did did you
i'm not sure uh by the moment.
but it's it's probably that uh a lot of uh uh
for example in the overlapping zone on uh in in several uh parts of the files where you you can find uh uh uh smooth uh uh speech uh from uh one uh uh talker in the in the meeting.
uhhuh.
uhhuh.
it's probably in in that uh in in those files you you can not find you can not process.
because uh it's confused with with noise.
uhhuh.
there are a lot of
i think.
but i have to study with more detail.
but uh my idea is to to process only nnn this uh nnn this kind of of uh speech.
because i think it's more realistic.
i'm not sure it's a good idea but uh
well it's more realistic but it'll it'll be a lot harder.
no
well it'd be hard but on the other hand as you point out if your if if if your concern is to get uh the overlapping people people's speech you will you will get that somewhat better.
yeah.
yeah.
um are you making any use
uh you were you were working with the data that had already been transcribed.
does it uh
with by jane.
yes.
yeah.
now um did you make any use of that?
see i was wondering because we we have these ten hours of other stuff that is not yet transcribed.
yeah.
yeah.
do you
the the transcription by jane uh uh i i i want to use to to nnn uh to put
it's a reference for me.
but uh the transcription uh for example i i don't i i'm not interested in the in the in the words transcription words uh transcribed uh uh in uh follow in the in the in the speech file but uh uh jane uh for example uh put a mark uh at the beginning uh of each uh talker.
in the in the meeting.
um uh she she nnn includes information about the zone where uh there are uh there is an overlapping zone.
uhhuh.
but uh there isn't any any mark time temporal mark to to uh to huh e-heh to label the beginning and the end of the of the
okay.
right so she is
i'm i i i think uh we need this information to
right.
so the twelve you you it took you twelve hours of course this included maybe some some time where you were learning about what what you wanted to do.
but but uh it took you something like twelve hours to mark the forty five minutes your
twelve minutes.
twelve minutes.
twelve minutes!
twelve minutes.
twelve.
i thought you did forty five minutes of
no forty five minutes is the is the session.
all the session.
oh you haven't done the whole session.
oh!
yeah.
this is just twelve minutes.
all is the the session.
oh!
twelve hours of work to to segment uh and label uh twelve minutes from a session of part of
so let me back up again.
so the when you said there were three hundred speaker overlaps
yeah.
that's in twelve minutes?
no no no.
i i consider all the all the session because uh i i count the nnn the nnn the overlappings marked by by jane.
oh okay.
okay.
oh i see.
in in in in the in in the forty five minutes.
so it's three hundred in forty five minutes.
but you have you have time uh uh marked twelve minute the the the um overlaps in twelve minutes of it.
yeah.
got it.
well not just the overlaps everything.
so can i ask can i ask whether you found uh you know how accurate uh jane's uh uh labels were as far as
you know did she miss some overlaps or did she
but by by the moment i i don't compare my my temporal mark with uh jane but uh i i want to do it.
because uh uh perhaps i have uh errors in the in the marks.
i and if i i compare with uh jane it's probably i i i can correct and and and to get uh uh a more accurately uh uh transcription in the file.
yeah.
well also jane jane was doing word level.
yeah.
so we weren't concerned with exactly when an overlap started and stopped.
yeah.
right right.
i'm i'm not expecting
well
well not only a word level but actually
i mean you didn't need to show the exact point of interruption.
no it's
you just were showing at the level of the phrase or the level of the speech spurt or
right.
uhhuh.
yep.
yeah.
well
yeah.
well yeah yeah i would say time bin.
so my my goal is to get words with reference to a time bin beginning and end point.
yeah.
yeah.
yeah.
right.
yeah.
and and sometimes you know it was like you could have an overlap where someone said something in the middle.
but yeah it just wasn't important for our purposes to have it that disrupt that unit in order to have you know the words in the order in which they were spoken.
yeah.
it would have it would have been hard with the interface that we have.
right.
now my adam's working on a of course on a revised overlapping interface.
uhhuh.
i i i think it's it's a good uh work.
but
but uh i think we need uh uh more information.
no of course.
i expect you to find more overlaps than than jane.
yeah.
always need more for
no no i i have to go to
yeah.
because you're looking at it at a much more detailed level.
i want uh i wanted to uh compare the the transcription.
but if it takes sixty to one
i have
well i but i have a suggestion about that.
um obviously this is very very time consuming and you're finding lots of things.
which i'm sure are going to be very interesting.
but in the interests of making progress uh might i
how how would it affect your time if you only marked speaker overlaps?
only.
yes.
do not mark any other events.
yeah.
but only mark speaker.
uhhuh.
do you think that would speed it up quite a bit?
okay .
okay i i i i i i wanted to
do do you think that would speed it up?
uh speed up your your your marking?
nnn i don't understand very
it took you a long time to mark twelve minutes.
yeah.
oh yeah yeah.
now my suggestion was for the other thirty three
only to mark only to mark overlapping zone but
yeah and my question is if you did that if you followed my suggestion would it take much less time?
oh.
yeah.
yeah okay.
sure.
yeah sure.
then i think it's a good idea.
then i think it's a good idea because
sure sure.
sure.
because i i need a lot of time to to put the label or to do that.
yeah.
yeah i mean we we know that there's noise.
uhhuh.
there's there's uh continual noise uh from fans and so forth.
and there is uh more impulsive noise from uh taps and so forth.
yeah.
and and something in between with paper rustling.
we know that all that's there.
and it's a worthwhile thing to study.
but obviously it takes a lot of time to mark all of these things.
yeah.
whereas i would think that uh you we can study more or less as a distinct phenomenon the overlapping of people talking.
uhhuh.
okay.
so then you can get the because you need
okay.
if it's three hundred uh it sounds like you probably only have fifty or sixty or seventy events right now that are really
yeah.
and and you need to have a lot more than that to have any kind of uh even visual sense of of what's going on much less any kind of reasonable statistics.
right.
now why do you need to mark speaker overlap by hand if you can infer it from the relative energy in the
well that's
that's what i was going to bring up.
i mean you shouldn't need to do this completely by hand.
um okay yeah so let's back up because you weren't here for an earlier conversation.
right?
i'm sorry.
so the idea was that what he was going to be doing was experimenting with different measures.
such as the increase in energy such as the energy in the l p c residuals such as
i mean there's a bunch of things i increased energy is sort of an obvious one.
uhhuh.
in the far field mike.
yeah.
oh okay.
um and uh it's not obvious.
i mean you could you could do the dumbest thing and get get it ninety percent of the time.
but when you start going past that and trying to do better it's not obvious what combination of features is going to give you the you know the right detector.
so the idea is to have some ground truth first.
and so the the idea of the manual marking was to say okay this you know it's it's really here.
but i think liz is saying why not get it out of the transcripts.
what i mean is get it from the close talking mikes.
uh yeah.
or get a first pass from those.
we we we we talked about that.
and then go through sort of
it'd be a lot faster probably to
and you can
yeah that's his uh
we we we talked about that.
but so it's a bootstrapping thing and the thing is
yeah i just
the idea was we we thought it would be useful for him to look at the data anyway.
and and then whatever he could mark would be helpful.
right.
and we could uh
it's a question of what you bootstrap from.
you know do you bootstrap from a simple measurement which is right most of the time and then you do better?
or do you bootstrap from some human being looking at it and then then do your simple measurements uh from the close talking mike?
i mean even with the close talking mike you're not going to get it right all the time.
well that's what i wonder because um
or how bad it is.
well
i'm working on a program to do that and
um because that would be interesting.
especially because the bottleneck is the transcription.
right?
i mean we've got a lot more data than we have transcriptions for.
we have the audio data.
yeah.
we have the close talking mike.
so i mean it seems like one kind of project
that's not perfect but
um that you can get the training data for pretty quickly is you know
if you infer form the close talking mikes where the on off points are of speech.
right we discussed that.
you know how can we detect that from a far field?
and
oh.
i've i've written a program to do that.
okay.
and it uh
i'm sorry i missed the
it's okay.
and so but it's it's doing something very very simple.
it just takes a threshold based on on the volume.
uhhuh.
or you can set the threshold low and then weed out the false alarms by hand.
right.
by hand yeah.
yeah.
um and then it does a median filter and then it looks for runs.
and it seems to work.
i've i'm sort of fiddling with the parameters to get it to actually generate something.
and i haven't
i don't what i'm working on was working on was getting it to a form where we can import it into the user interface that we have into transcriber.
and so i told i said it would take about a day.
i've worked on it for about half a day.
i have to go.
so give me another half day and we'll have something we can play with.
okay.
see this is where we really need the meeting recorder query stuff to be working.
because we've had these meetings and we've had this discussion about this and i'm sort of remembering a little bit about what we decided.
right.
i'm sorry!
i just
but i couldn't remember all of it.
it
so i think it was partly that you know give somebody a chance to actually look at the data and see what these are like partly that we have some ground truth to compare against you know when when he he gets his thing going.
but
uh and
well it's definitely good to have somebody look at it.
i was just thinking as a way to speed up you know the amount of
but that was that was exactly the notion that that that we discussed.
uhhuh.
okay.
thanks.
another thing we discussed was um that
so
it looks good.
i'll be in touch.
thanks.
see ya.
yeah.
was that um there there was this already a script i believe uh that dan had written that uh handle bleedthrough.
i mean because you have this this close you have contamination from other people who speak loudly.
yeah.
and i haven't tried using that.
it would probably help the program that i'm doing to first feed it through that.
it's a cross correlation filter.
so i i haven't tried that but that if it it might be something it might be a good way of cleaning it up a little.
so some thought of maybe having
having that be a preprocessor and then run it through yours.
exactly yep.
but but that's a refinement.
that's what we were discussing.
and i think we want to see try the simple thing first.
because you add this complex thing up uh afterwards that does something good you sort of want to see what the simple thing does first.
yep.
but uh having having somebody have some experience again with with uh with marking it from a human standpoint.
we're i mean i don't expect jose to to do it for uh fifty hours of of speech.
but i mean we if uh if he could speed up what he was doing by just getting the speaker overlaps so that we had it say for forty five minutes then at least we'd have three hundred examples of it.
sure.
sure.
and when when uh adam was doing his automatic thing he could then compare to that and see what it was different.
oh yeah definitely.
you know i did i did uh something almost identical to this at one of my previous jobs and it works pretty well.
i mean almost exactly what you described an energy detector with a median filter you look for runs.
and uh
you know you can
it seemed like the right thing to do.
yeah i mean you you can get i mean you get them pretty close.
that was with zero literature search.
and so i think doing that to generate these possibilities and then going through and saying yes or no on them would be a quick way to to do it.
that's good validation.
yeah.
is this proprietary?
yeah do you have a patent on it?
uh no no.
it was when i was working for the government.
oh then everybody owns it.
it's the people.
well i mean is this something that we could just co opt?
or is it
no.
nah.
okay.
well i he's pretty close anyway i think i think it's
yeah he's it it doesn't take a long time.
right.
i just thought if it was tried and true then and he's gone through additional levels of of development.
just output.
although if you if you have some parameters like what's a good window size for the median filter?
oh i have to remember.
i'll think about it.
and try to remember.
and it might be different for government people.
that's all right.
yeah good enough for government work as they say.
they they
different different bandwidth.
they
i was doing pretty short you know tenth of a second sorts of numbers.
okay.
uh i don't know it if if we want to uh
so uh maybe we should move on to other other things in limited time.
can i ask one question about his statistics?
so so in the twelve minutes um if we took three hundred and divided it by four which is about the length of twelve minutes um i'd expect like there should be seventy five overlaps.
yeah.
did you find uh more than seventy five overlaps in that period or
more than?
more than
how many overlaps in your twelve minutes?
how many?
uh not
only i i transcribe uh only twelve minutes from the
yeah.
but uh i i don't uh i don't count uh the the overlap.
the overlaps.
okay.
i consider i i the the nnn the the three hundred is uh considered only you your transcription.
i have to to finish transcribing.
so
i i bet they're more because the beginning of the meeting had a lot more overlaps than than sort of the middle.
yeah.
middle or end.
yeah.
because we're we're dealing with the uh in the early meetings.
i'm not sure.
we're recording while we're saying who's talking on what microphone and things like that.
yeah.
and that seems to be a lot of overlap.
yeah.
i think it's an empirical question.
i think we could find that out.
yeah.
yep.
i'm i'm not sure that the beginning had more.
so so i was going to ask i guess about any any other things that that that either of you wanted to talk about.
especially since andreas is leaving in five minutes.
that that you want to go with.
can i just ask about the data?
like very straightforward question is where we are on the amount of data and the amount of transcribed data?
just because i'm
i wanted to get a feel for that.
to sort of be able to know what what can be done first.
and like how many meetings are we recording?
right so there's this this there's this forty five minute piece that jane transcribed.
and
that piece was then uh sent to i b m so they could transcribe so we have some comparison point.
then there's a larger piece that's been recorded and uh put on c d rom and sent uh to i b m.
right?
and then we don't know.
how many meetings is that?
what's that?
like how many
that was about ten hours and there was about
ten it's like ten meetings or something?
yeah something like that.
uhhuh.
and then then we
ten meetings that have been sent to i b m?
and
well i haven't sent them yet because i was having this problem with the missing files.
yeah.
oh.
oh that's right that had those have not been sent.
how many total have we recorded now altogether?
we're saying about twelve hours.
about twelve by now twelve or thirteen.
and we're recording only this meeting like continuously.
we're only recording this one now or
no.
nope.
no so the the that's the that's the biggest one uh chunk so far.
okay.
it was the morning one.
but there's at least one meeting recorded of uh the uh uh natural language guys.
jerry.
do they meet every week?
and then there
uh they do.
or every
and we talked to them about recording some more.
and we're going to
uh we've started having a morning meeting today uh starting a a week or two ago on the uh front end issues.
and we're recording those.
uh there's a network services and applications group here who's agreed to have their meetings recorded.
great.
and we're going to start recording them.
they're they meet on tuesdays we're going to start recording them next week.
so actually we're going to start having a a pretty significant chunk.
and
so you know adam's sort of struggling with trying to get things to be less buggy and come up quicker when they do crash and stuff things like that.
now that uh the things are starting to happen.
so right now yeah i i'd say the data is predominantly meeting meetings.
but there are scattered other meetings in it and that that amount is going to grow.
uh so that the meeting meetings will probably ultimately
if we're if we collect fifty or sixty hours the meeting meetings it will probably be you know twenty or thirty percent of it.
not not not eighty or ninety.
but
so there's probably there's three to four a week.
that's what we're aiming for.
that we're aiming for.
yeah.
and they're each about an hour or something.
yeah yeah.
although
yeah.
we'll find out tomorrow whether we can really do this or not.
so
yeah and the the other thing is i'm not
okay.
i'm sort of thinking as we've been through this a few times that i really don't know
maybe you want to do it once for the novelty.
but i don't know if in general we want to have meetings that we record from outside this group do the digits.
right.
because it's just an added bunch of weird stuff.
yeah.
and you know we we we're highly motivated.
uh in fact the morning group is really motivated.
because they're working on connected digits.
so it's
actually that's something i wanted to ask.
is i have a bunch of scripts to help with the transcription of the digits.
yeah.
we don't have to hand transcribe the digits because we're reading them and i have those.
yeah.
and so i have some scripts that let you very quickly extract the sections of each utterance.
right.
but i haven't been i haven't been doing that.
um if i did that is someone going to be working on it?
uh yeah i think.
i mean is it something of interest?
definitely absolutely.
yeah whoever we have working on the acoustics for the meeting recorder are going to start with that.
okay.
i mean i'm i'm interested in it.
i just don't have time to do it now.
i was these meetings i'm sure someone thought of this but these this uh reading of the numbers would be extremely helpful to do um adaptation.
so
yep yep.
um
actually i have
i i would really like someone to do adaptation.
uhhuh.
so if we got someone interested in that i think it would be great for meeting recorder.
well
i mean one of the things i wanted to do uh that i i talked to to don about is one of the possible things he could do or also we could have someone else do it is to do block echo cancellation.
since it's the same people over and over.
uhhuh.
to try to get rid of some of the effects of the the the far field effects.
uhhuh.
um i mean we have the party line has been that echo cancellation is not the right way to handle the situation.
because people move around.
and uh if if it's if it's uh not a simple echo like a cross talk kind of echo but it's actually room acoustics it's it's it's you can't really do inversion.
uhhuh.
and even echo cancellation is going to uh be something it may you someone may be moving enough that you are not able to adapt quickly.
and so the tack that we've taken is more lets come up with feature approaches and multi stream approaches and so forth that will be robust to it for the recognizer and not try to create a clean signal.
uhhuh.
uh that's the party line.
but it occurred to me a few months ago that uh party lines are always you know sort of dangerous.
it's good good to sort of test them actually.
and so we haven't had anybody try to do a good serious job on echo cancellation and we should know how well that can do.
so that's something i'd like somebody to do at some point just take these digits take the far field mike signal and the close uh mike signal and apply really good echo cancellation.
um there was a have been some nice talks recently by by lucent on on their
huh.
the block echo cancellation particularly appealed to me.
uh you know trying and change it sample by sample but you have some reasonable sized blocks and um you know
what is the um the artifact you try to you're trying to get rid of when you do that?
ciao.
uh so it's it you have a a direct uh
what's the difference in if you were trying to construct a linear filter that would
um
i'm signing off.
yeah.
that would subtract off the um uh parts of the signal that were the aspects of the signal that were different between the close talk and the distant.
you know so so uh um i guess in most echo cancellation.
yeah so you given that um
yeah so you're trying to so you'd there's a a distance between the close and the distant mikes so there's a time delay there.
and after the time delay there's these various reflections.
and if you figure out well what's the
there's a a least squares algorithm that adjusts itself adjusts the weight so that you try to subtract essentially to subtract off uh different uh different reflections.
right so let's take the simple case where you just had you had some uh some delay in a satellite connection or something.
and then there's a there's an echo.
it comes back and you want to adjust this filter so that it will maximally reduce the effect of this echo.
so that would mean like if you were listening to the data that was recorded on one of those.
uh just the raw data.
you would you might hear kind of an echo?
and and then this noise cancellation would
well i'm i'm i'm saying that's a simplified version of what's really happening what's really happening is
well when i'm talking to you right now you're getting the direct sound from my speech.
but you're also getting uh the indirect sound that's bounced around the room a number of times.
okay?
so now if you um try to you to completely remove the effect of that is sort of impractical for a number of technical reasons.
but i but
not to try to completely remove it that is invert the the room response.
but just to try to uh uh eliminate some of the the effect of some of the echos.
um a number of people have done this.
so that say if you're talking to a speakerphone uh it makes it more like it would be if you were talking right up to it.
so this is sort of the the straight forward approach.
you say i i i want to use this uh this item but i want to subtract off various kinds of echos.
so you construct a filter and you have this this filtered version uh of the speech um gets uh uh gets subtracted off from the original speech.
then you try to you try to minimize the energy in some sense.
and so um uh with some constraints.
kind of a clean up thing that
it's a clean up thing right.
okay.
so echo cancelling is is you know commonly done in telephony.
and and and it's sort of the obvious thing to do in this situation if you if you know you're going to be talking some distance from a mike.
when uh i would have meetings with the folks in cambridge when i was at b b n over the phone they had a um some kind of a special speaker phone.
and when they would first connect me it would come on and we'd hear all this noise.
yeah.
and then it was uh and then it would come on and it was very clear.
you know?
right so it's taking samples it's doing adaptation it's adjusting weights.
and then it's getting the sum.
so um uh anyway that's that's kind of a reasonable thing that i'd like to have somebody try.
somebody look and and the digits would be a reasonable thing to do that with.
i think that'd be enough data plenty of data to do that with.
and for that sort of task you wouldn't care whether it was uh large vocabulary speech or anything.
uh um
is brian kingsbury's work related to that?
or is it a different type of reverberation?
brian's kingsbury's work is an example of what we did from the opposite dogma.
right?
which is what i was calling the party line which is that uh doing that sort of thing is not really what we want.
we want something more flexible uh where people might change their position.
and there might be you know
there's also um
oh yeah!
noise.
so the echo cancellation does not really allow for noise.
it's if you have a clean situation but you just have some delays.
then we'll figure out the right the right set of weights for your taps for your filter in order to produce the effect of those those echos.
but um if there's noise then the very signal that it's looking at is corrupted.
so that it's decision about what the right you know right right uh delays are is uh is right delayed signal is is is uh is incorrect.
and so in a noisy situation um also in a in a situation that's very reverberant with long reverberation times and really long delays it's it's sort of typically impractical.
so for those kind of reasons
and also a a a complete inversion
if you actually i mentioned that it's kind of hard to really do the inversion of the room acoustics.
um that's difficult because um often times the the um the system transfer function is such that when it's inverted you get something that's unstable.
and so if you you do your estimate of what the system is and then you try to invert it you get a filter that actually uh you know rings and and uh goes to infinity.
so it's so there's there's there's that sort of technical reason and the fact that things move and there's air currents.
i mean there's all sorts of all sorts of reasons why it's not really practical.
so for all those kinds of reasons uh we we we sort of um concluded we didn't want to do inversion.
and we're even pretty skeptical of echo cancellation which isn't really inversion.
and um we decided to do this approach of taking uh just picking uh features which were uh will give you more something that was more stable in the presence of or absence of room reverberation.
and that's what brian was trying to do.
so um let me just say a couple things that i was i was going to bring up.
uh let's see i guess you you actually already said this thing about the uh about the consent forms.
which was that we now don't have to
so this was the human subjects folks who said this or that that
the apparently i mean we're going to do a revised form of course.
um
but once a person has signed it once then that's valid for a certain number of meetings.
she wanted me to actually estimate how many meetings and put that on the consent form.
i told her that would be a little bit difficult to say.
so i think from a practical standpoint maybe we could have them do it once every ten meetings or something.
it won't be that many people who do it that often.
but um just you know so long as they don't forget that they've done it i guess.
okay.
um back on the data thing.
so there's this sort of one hour ten hour a hundred hour sort of thing that that we have.
we have we have an hour uh that that is transcribed.
we have we have twelve hours that's recorded but not transcribed.
and at the rate we're going uh by the end of the semester we'll have i don't know forty or fifty or something if we if this really.
uh
well do we have that much?
not really.
let's see.
we have
it's three to four per week.
so that's what you know that
uh eight weeks uh is
so that's not a lot of hours.
eight weeks times three hours is twenty four so that's yeah so like thirty thirty hours.
three three hours.
yeah.
i mean is there
i know this sounds tough but we've got the room set up.
um i was starting to think of some projects where you would use
well similar to what we talked about with uh energy detection on the close talking mikes.
there are a number of interesting questions that you can ask about how interactions happen in a meeting that don't require any transcription.
so what are the patterns the energy patterns over the meeting?
and i'm really interested in this but we don't have a whole lot of data.
so i was thinking you know we've got the room set up.
and you can always think of also for political reasons if icsi collected you know two hundred hours that looks different than forty hours even if we don't transcribe it ourselves.
but i don't think we're going to stop at the end of this semester.
so
right?
so i i think that if we are able to keep that up for a few months we are going to have more like a hundred hours.
especially meetings that have some kind of conflict in them or some kind of
i mean that are less
well i don't
uh that have some more emotional aspects to them?
or strong
we had some good ones earlier.
there's laughter.
um i'm talking more about strong differences of opinion meetings.
maybe with manager types or
i think it's hard to record those.
to be allowed to record them?
yeah people will get
okay.
it's also likely that people will cancel out afterwards.
but i but i wanted to raise the k p f a idea.
okay.
well if there is any way.
yeah i was going to mention that.
oh that's a good idea.
that's that would be a good match.
yeah so yeah so i i uh i i'd mentioned to adam and that was another thing i was going to talk uh mention to them before that uh there's uh it it it occurred to me that we might be able to get some additional data by talking to uh acquaintances in local broadcast media.
because you know we had talked before about the problem about using found data that that uh it's just set up however they have it set up.
and we don't have any say about it.
and it's typically one microphone.
in a uh uh or and and so it doesn't really give us the the the uh characteristics we want.
um and so i do think we're going to continue recording here and record what we can.
but um it did occur to me that we could go to friends in broadcast media and say hey you have this panel show or this you know this discussion show and um can you record multi channel.
and uh they may be willing to record it uh with
lapel mikes or something?
well they probably already use lapel.
but they might be able to have it it wouldn't be that weird for them to have another mike that was somewhat distant.
right.
it wouldn't be exactly this setup but it would be that sort of thing.
and what we were going to get from u w you know assuming they they they start recording isn't also is not going to be this exact setup.
no i think that'd be great.
so i i i i was thinking of looking into that.
if we can get more data.
the other thing that occurred to me after we had that discussion in fact is that it's even possible since of course many radio shows are not live uh that we could invite them to have like some of their record some of their shows here.
wow!
well or the thing is they're not as averse to wearing one of these head mount
right as we are.
i mean they're on the radio.
right?
so um i think that'd be fantastic.
right.
because those kinds of panels and those have interesting
yeah.
that's an a side of style a style that we're not collecting here.
so
and and the i mean the other side to it was the what which is where we were coming from i'll i'll talk to you more about it later is that is that there's there's uh
it'd be great.
the radio stations and television stations already have stuff worked out presumably uh related to you know legal issues and and permissions and all that.
i mean they already do what they do do whatever they do.
so it's uh it's so it's so it's another source.
so i think it's something we should look into.
you know we'll collect what we collect here.
hopefully they will collect more at u w also.
and um and maybe we have this other source.
but yeah i think that it's not unreasonable to aim at getting you know significantly in excess of a hundred hours.
i mean that was sort of our goal.
the thing was i was hoping that we could in the under this controlled situation we could at least collect you know thirty to fifty hours.
and at the rate we're going we'll get pretty close to that i think this semester.
and if we continue to collect some next semester i think we should uh
right.
yeah i was mostly trying to think okay if you start a project within say a month you know how much data do you have to work with.
and you you want to you want to sort of freeze your your data for awhile.
so um right now
and we don't have the transcripts back yet from i b m.
right?
well we don't even have it for this you know forty five minutes that was
do oh do we now
so um not complaining i was just trying to think you know what kinds of projects can you do now versus uh six months from now.
yeah.
and they're pretty different because
right.
yeah so i was thinking right now it's sort of this exploratory stuff where you you look at the data you use some primitive measures and get a feeling for what the scatter plots look like.
um
right.
and and and uh and meanwhile we collect and it's more like yeah three months from now or six months from now you can you can do a lot of other things.
right right.
because i'm not actually sure just logistically that i can spend
you know i don't want to charge the time that i have on the project too early before there's enough data to make good use of the time.
and that's and especially with the student
right.
uh for instance this guy who seems.
yeah.
uh
anyway i shouldn't say too much.
but um if someone came that was great and wanted to do some real work and they have to end by the end of this school year in the spring how much data will i have to work with with that person?
and so it's
yeah so i would think exploratory things now uh three months from now
um i mean the transcriptions i think are a bit of an unknown because we haven't gotten those back yet as far as the timing.
but i think as far as the collection it doesn't seem to me like uh unreasonable to say that uh in january you know roughly uh which is roughly three months from now we should have at least something like you know twenty five thirty hours.
and we just don't know about the transcription part of that.
so that's
yeah we need to i think that there's a possibility that the transcript will need to be adjusted afterwards.
so
i mean it
and uh especially since these people won't be uh used to dealing with multi channel uh transcriptions.
right.
yeah.
so i think that we'll need to adjust some.
and also if we want to add things like um well more refined coding of overlaps then definitely i think we should count on having an extra pass through.
i wanted to ask another aspect of the data collection.
there'd be no reason why a person couldn't get together several uh you know friends.
and come and argue about a topic if they wanted to.
right?
if they really have something they want to talk about as opposed to something uh
i mean what we're trying to stay away from was artificial constructions.
but i think if it's a real
why not?
yeah.
i mean i'm thinking politically.
stage some political debates.
you could do this.
well yeah.
you know you could.
or just if you're if you if there are meetings here that happen that we can record even if we don't um have them do the digits or maybe have them do a shorter digit thing like if it was you know uh one string of digits or something they'd probably be willing to do.
we don't have to do the digits at all if we don't want to.
then having the data is very valuable.
because i think it's um politically better for us to say we have this many hours of audio data especially with the i t r if we put in a proposal on it.
it'll just look like icsi's collected a lot more audio data.
um whether it's transcribed or not um is another issue.
but there's there are research questions you can answer without the transcriptions or at least that you can start to answer.
yep.
it seems like you could hold some meetings.
so
you know you and maybe adam.
you you could you could maybe hold some additional meetings if you wanted.
would it help at all
i mean we're already talking about sort of two levels of detail in meetings.
one is uh um without doing the digits or i guess the full blown one is where you do the digits and everything and then talk about doing it without digits.
what if we had another level just to collect data which is without the headsets and we just did the table mounted stuff?
need the close talking mikes.
you do okay.
i mean absolutely.
yeah yeah.
yeah i'm really scared
it seems like it's a big part of this corpus is to have the close talking mikes.
um or at least like me personally i would i couldn't use that data.
i see okay.
yeah.
i agree.
um
and mari also.
we had this came up when she was here.
that's important.
yeah.
so it's a great idea.
i i by the by the way i don't think the transcriptions are actually in the long run such a big bottleneck.
and if it were true than i would just do that.
but it's not that bad.
like the room is not the bottleneck.
and we have enough time in the room.
it's getting the people to come in and put on the and get the setup going.
i think the issue is just that we're we're blazing that path.
right?
and and um do you have any idea when when uh the you'll be able to send uh the ten hours to them?
well i've been burning two c d's a day which is about all i can do with the time i have.
yeah.
yeah.
so it'll be early next week.
yeah okay.
so early next week we send it to them.
and then then we check with them to see if they've got it.
and we we start you know asking about the timing for it.
yep.
so i think once they get it sorted out about how they're going to do it which i think they're pretty well along on because they were able to read the files and so on.
yep.
right?
yeah but
well
yeah who knows where they are?
have they ever responded to you?
nope.
yeah but you know so they they they have you know they're volunteering their time and they have a lot of other things to do.
what if
yeah you we can't complain.
right?
but they but at any rate they'll i i think once they get that sorted out they're they're making cassettes there then they're handing it to someone who they who's who is doing it.
and uh i think it's not going to be i don't think it's going to be that much more of a deal for them to do thirty hours then to do one hour.
i think it's not going to be thirty
yep i think that's probably true.
really?
so it's the amount of
it's it's just getting it going.
it's pipeline pipeline issues.
right.
what about these lunch meetings?
once the pipeline fills
i mean i don't know if there's any way without too much more overhead even if we don't ship it right away to i b m even if we just collect it here for awhile to record you know two or three more meeting a week.
just to have the data even if they're um not doing the digits but they do wear the headphones.
but the lunch meetings are pretty much one person getting up and
no i meant
um sorry the meetings where people eat their lunch downstairs.
maybe they don't want to be recorded but
oh and we're just chatting?
just the the chatting.
yeah we have a lot of those.
i actually i actually think that's useful data um the chatting.
yeah the problem with that is i would i think i would feel a little constrained to you know uh some of the meetings.
but
okay.
you don't want to do it because
okay.
you know our soccer ball meeting?
i guess none of you were there for our soccer ball meeting.
all right.
all right so i'll just throw it out there.
that was hilarious.
if anyone knows of one more or two more meetings per week that happen at icsi um that we could record i think it would be worth it.
yeah well we should also check with mari again because they because they were really intending you know maybe just didn't happen but they were really intending to be duplicating this in some level.
so then that would double what we had.
and there's a lot of different meetings at u w.
uh i mean really a lot more than we have here right because we're not right on campus.
right.
so
is the uh notion of recording any of chuck's meetings dead in the water?
or is that still a possibility?
uh they seem to have some problems with it we can we can talk about that later.
um but again jerry is jerry's open.
so i mean we have two speech meetings one uh network meeting.
uh jerry was open to it but i
one of the things that i think is a little a little bit of a limitation there is a think when the people are not involved uh in our work we probably can't do it every week.
you know i i i i think that that people are going to feel uh are going to feel a little bit constrained.
now it might get a little better if we don't have them do the digits all the time.
yep.
and the then so then they can just really sort of try to put the mikes on and then just charge in and
what if we give people you know we cater a lunch in exchange for them having their meeting here or something?
well you know i i do think eating while you're doing a meeting is going to be increasing the noise.
okay.
all right all right all right.
but i had another question which is um you know in principle um i know that you don't want artificial topics.
but um it does seem to me that we might be able to get subjects from campus to come down and do something that wouldn't be too artificial.
i mean we could political discussions or or something or other.
no definitely.
and
you know people who are
because you know there's also this constraint we
it's like you know the the uh goldibears goldi goldilocks.
it's like you don't want meetings that are too large but you don't want meetings that are too small.
and um and it just seems like maybe we could exploit the human subject pool in the positive sense of the word.
well even i mean coming down from campus is sort of a big thing but what about
we could pay subjects.
or what about people in the in the building?
yeah i was thinking there's all these other
i mean there's the state of california downstairs and
yeah.
i mean
i just really doubt that uh any of the state of california meetings would be recordable and then releasable to the general public.
yeah.
oh.
uhhuh.
so i i mean i talked with some people at the haas business school who are who are interested in speech recognition.
all right well
and they sort of hummed and hawed and said well maybe we could have meetings down here.
but then i got email from them that said no we decided we're not really interested and we don't want to come down and hold meetings.
so i think it's going to be a problem to get people regularly.
what about joachim maybe he can
but but we but i think you know we get some scattered things from this and that.
and i i i do think that maybe we can get somewhere with the with the radio.
uh i have better contacts in radio than in television but
uhhuh.
you could get a lot of lively discussions from those radio ones.
yep.
well and they're already they're these things are already recorded.
yeah.
we don't have to ask them to
even and i'm not sure how they record it but they must record from individual
well
no i'm not talking about ones that are already recorded.
i'm talking about new ones.
because because because we would be asking them to do something different.
why why not?
well we can find out.
i know for instance mark liberman was interested uh in in l d c.
getting data.
right that's the found data idea.
uh and
but what i'm saying is uh if i talk to people that i know who do these who produce these things we could ask them if they could record an extra channel let's say of a distant mike.
yeah.
uhhuh.
and i think routinely they would not do this.
so since i'm interested in the distant mike stuff i want to make sure that there is at least that somewhere.
right.
great okay.
great.
okay.
and uh
but if we ask them to do that they might be intrigued enough by the idea that they uh might be willing to the i might be able to talk them into it.
uhhuh.
we're getting towards the end of our disk space.
so we should think about trying to wrap up here.
okay well i don't why don't we why why don't we uh uh turn them turn
that's a good way to end a meeting.
okay leave leave them on for a moment until i turn this off because that's when it crashed last time.
oh!
that's good to know.
turning off the microphone made it crash.
well
that's good to know.
okay.
all right.
so
so are you
are we going?
it is uh must be february fifteenth.
yeah.
i think the date's written in there yep.
and actually if everyone could cross out the r nine next to session and write m r eleven.
yeah.
yeah.
we didn't have a front end meeting today.
and let's remember also to make sure that one's gets marked as unread unused.
okay.
m r eleven?
m r eleven.
that sounds like a spy code.
huh.
okay.
so
there's lots of clicking i'm sure as i'm trying to get this to work correctly.
agenda.
any agenda items today?
i want to talk a little bit about getting how we're going to to get people to edit bleeps parts of the meeting that they don't want to include.
what i've done so far and i want to get some opinions on how to how to finish it up.
okay.
i want to ask about um some audio monitoring on some of the um well some of the equipment.
in particular the
well uh that's just what i want to ask.
okay audio monitoring jane.
based on some of the uh
in listening to some of these meetings that have already been recorded there are sometimes big spikes on particular things.
and in pact in fact this one i'm talking on is one of of the ones that showed up in one of the meetings.
so i
oh really?
uhhuh.
yeah.
spikes you mean like uh instantaneous click type spikes or
spikes?
clicks.
yeah.
huh.
yeah.
huh.
and i don't know what the electronics is but
yeah.
yeah.
well i think it's
touching.
uh it it could be a number of things.
it could be touching and fiddling.
yeah.
and the other thing is that it could
the fact that it's on a wired mike is suspicious.
it might be a connector.
oh okay.
well maybe
then we don't really have to talk about that as an
i i take that off the agenda.
you could try an experiment and say okay i'm about to test for spikes.
and then wiggle the thing there.
and then go and when they go to transcribe it it could ask them to come and get you.
yeah.
right.
oh that
come get me when you transcribe this and see if there's spikes.
um
well okay.
no i'm just
i mean were this a professional audio recording what we would do what you would do is in testing it is you would actually do all this wiggling and make sure that that that things are not giving that kind of performance.
and if they are then they can't be used.
right.
so
um let's see.
i guess i would like to have a discussion about you know where we are on uh recording transcription.
you know basically you know where we are on the corpus.
good.
and then um the other thing which i would like to talk about which is a real meta quest i think deal is uh agendas.
so maybe i'll i'll start with that actually.
uh um andreas brought up the fact that he would kind of like to know if possible what we were going to be talking about.
because he's sort of peripherally involved to this point.
and if there's going to be a topic about discussion about something that he uh strongly cares about then he would come.
and and i think part of part of his motivation with this is that he's trying to help us out.
in the because of uh the fact that the meetings are are tending to become reasonably large now on days when everybody shows up.
and so he figures he could help that out by not showing.
and and i'm sure help out his own time.
huh.
by not showing up if it's a meeting that he's he's
so uh in order
i'd i think that this is a wish on his part.
uh it's actually going to be hard.
because it seems like a lot of times uh things come up that are unanticipated.
and and
but um we could try anyway.
right.
uh do another try at coming up with the agenda uh at some point before the meeting.
uh say the day before.
well maybe it would be a good idea for one of us to like on wednesday or tuesday send out a reminder for people to send in agenda items.
yeah.
okay.
you you want to volunteer to do that?
sure.
okay.
all right so we'll send out agenda request.
uh
let me
that'll be
i think that'll help
i'll put that on my spare brain or it will not get done.
that'll help a lot actually.
yeah i have to tell you for the uh for the admin meeting that we have lila does that um every time before an admin meeting.
and uh she ends up getting the agenda requests uh uh ten minutes before the meeting.
but but but
uh but we can try.
maybe it'll work.
yeah.
huh.
maybe.
yeah.
weirder things have happened.
i'm wondering if he were to just uh specify particular topics.
i mean
maybe we'd be able to meet that request of his a little more.
i would i would also guess that as we get more into processing the data and things like that there'll be more things of interest to him.
well then
yeah.
actually it
this this maybe brings up another topic which is um
so we're done with that topic.
the other topic i was thinking of was the status on microphones and channels and all that.
yeah actually i i was going to say we need to talk about that too.
yeah.
why why don't we do that?
okay.
um the new microphones the two new ones are in.
um and they are being assembled as we speak i hope.
and i didn't bring my car today so i'm going to pick them up tomorrow.
um and then the other question i was thinking about is well a couple things.
first of all if the other headsets are a lot more comfortable we should probably just go ahead and get them.
so we'll have to evaluate that when they come in.
and get people's opinions on on what they think of them.
uhhuh.
um then the other question i had is maybe we should get another wireless.
another wireless setup.
i mean it's expensive but it does seem to be better than the wired.
so how many channels do you get to have in a wireless setup?
um well i'm pretty sure that you can daisy chain them together.
so what we would do is replace the wired mikes with wireless.
so we currently have one base station with six wireless mike.
possibility of six wireless receivers.
and apparently you can chain those together.
and so we could replace our wired mikes with wireless if we bought another base station and more wireless mikes.
so um
and
so let's see we
so you know it's still it's fifteen minus six.
right?
so we could have up to nine.
and right now we can have up to six.
right.
and we have five we're getting one more.
yeah.
and it's um about nine hundred dollars for the base station.
and then eight hundred per channel.
oh.
so yeah so the only
beyond the mike the cost of the mikes the only thing is the base station that's nine hundred dollars.
right.
oh we should do it.
okay.
yeah.
okay so i'll look into how you daisy chain them and and then just go ahead and order them.
yeah.
i don't quite understand how that how that works.
if
so we're not increasing the number of channels.
no we're just replacing the wired the two wired that are still working.
okay.
okay.
i see.
along with a couple of the wired that aren't working one of the wired that's not working with a wireless.
yeah.
basically we found
three wireds work.
right?
i i guess three wireds work yeah.
yeah.
yeah.
yeah.
but we've had more problems with that.
yep.
and that sort of bypasses the whole the whole jimbox thing and all that.
right.
and so um we we seem to have uh a reliable way of getting the data in which is through the sony radio mikes.
as long as we're conscious about the batteries.
that seems to be the key issue.
right.
everyone's battery okay?
i checked them this morning.
they should be.
okay.
yeah.
um because that's the only thing with them.
but the quality seems really good.
and
um i heard from u w that they're they're uh very close to getting their uh setup purchased.
they're they're they're buying something that you can just sort of buy off the shelf.
well we should talk to them about it because i know that s r i is also in the process of looking at stuff.
and so you know what we should try to keep everyone on the same page with that.
yeah.
s r i?
really?
yeah.
oh.
they got
well maybe this needs to be bleeped out?
i have no clue.
uh i don't know.
i don't know how much of it's public.
probably we shouldn't probably we shouldn't talk about funding stuff.
right.
yeah.
but anyway there's there's there's uh uh other activities that are going on there and and uh and nist and u w.
so
um but but yeah i i think that at least the message we can tell other people is that our experience is is quite positive with the sony.
uh radio mikes.
right.
now the one thing that you have said that actually concerns me a little is you're talking about changing the headsets.
meaning changing the connector which means some hand soldering or something.
right?
uh no.
no?
we're having the them do it.
so it's hand soldering it but i'm not doing it.
oh.
okay.
so they they charge.
nothing against you and your hand soldering.
right?
but
you've never seen my hand soldering.
but uh as i said they're coming in.
uh okay so that's being done professionally
and
yeah.
i i mean
yeah.
i mean
yeah.
as professionally as i guess you can get it done.
well it could
if they do a lot of it it's
i mean it's just their repair shop.
right?
their maintenance people.
well we'll see what it it's like.
yep.
that that can be quite good.
this
yeah okay.
good.
yeah.
so let's go with that.
uh
and i mean we'll see tomorrow you know what it looks like.
yeah.
so um uh dave isn't here but he was going to start working on some things with the digits.
uh so he'll be interested in what's going on with that.
i guess
was the decision last time was that the the uh transcribers were going to be doing stuff with the digits as well?
uhhuh.
has that started or is that
yeah.
uh it would be to use his interface and i was going to meet with him today about that.
right so the decision was that jane did not want the transcribers to be doing any of the paperwork.
so i did the all that last week.
so all the all the forms are now on the computer.
and uh then i have a bunch of scripts that we'll read those and let the uh transcribers use different tools.
and i just want to talk to jane about how we transition to using those.
uhhuh.
so he has a nice set up that they
it it will be efficient for them to do that.
okay.
so anyway
i i don't think it'll take too long.
so you know just uh a matter of a few days i suspect.
so anyway i think we we have at least one uh user for the digits once they get done which will be dave.
right.
i've already done five or six sets.
okay.
so if he wanted to you know just have a few to start with he could.
yeah he might he might be asking
you know and i also have a bunch of scripts that will like generate p files and run recognition on them also.
right.
okay.
uh is dave
i don't know if dave is on the list.
if he's invited to these meetings
uh
i don't tend to get an invitation myself for them even.
if he knows
no no.
uh we don't have a active one.
but i'll make sure he's on the list.
yeah.
should we call him?
i mean is he is he definitely not available today?
i don't know.
should i call his office and see?
uh well it's uh
he was in.
i mean he's still taking classes so uh he may well have conflicts.
yeah.
yeah.
yeah he was in
he wasn't there at
yeah so this might be a conflict for him.
yeah.
okay.
yeah.
okay.
uh so
yeah didn't he say his signal processing class was like tuesdays and thursdays?
i think he has a class.
yeah.
yeah.
he might have.
oh okay.
you talking about david gelbart?
oh well whatever.
yeah.
yeah.
yeah.
yes.
yeah.
yeah i think he's taking two twenty five a which is now.
yeah.
okay.
so
yeah.
okay.
so that's why we're not seeing him.
okay.
uh transcriptions uh beyond the digits where we are and so on.
okay.
and the and the recordings also.
um
just where we are.
yeah.
well so um should we we don't want to do the recording status first or
well we have about thirty two hours.
uh as of i guess a week and a half ago.
so we probably now have about thirty five hours.
and and that's that's uh
how much of that is digits?
it's uh
that's including digits.
that's including digits.
right?
so
i haven't separated it out so i have no clue how much of that is digits.
yeah.
so anyway there's at least probably thirty hours.
or something of
there's got to be more than thirty hour
it couldn't
of of non digits?
of of non digits.
huh.
yeah absolutely.
yeah yeah.
i mean the digits don't take up that much time.
okay.
okay and the transcribers
i uh don't have the exact numbers.
but i think it would come to about eleven hours that are finished uh transcribing from them right now.
the next step is to that i'm working on is to insure that the data are clean first and then channelized.
what i mean by clean is that they're spell checked.
that the mark up is consistent all the way throughout.
and also that we now incorporate these additional conventions that uh liz requested in terms of um um in terms of having a a systematic handling of numbers and acronyms which i hadn't been specific about.
um for example they'll say uh ninety two.
and you know so how you could
nine two.
exactly.
right.
so if you just say nine two the there are many ways that could have been expressed.
and i just had them
i i mean a certain number of them did put the words down.
but now we have a convention which also involves having it followed by um a gloss and things.
you know jane?
uhhuh.
um one suggestion
and you may already be doing this.
but i've noticed in the past that when i've gone through transcriptions and you know in in order to build lexicons and things
if you um just take all the transcriptions and separate them into words and then alphabetize them a lot of times just scanning down that list you'll find a lot of inconsistencies and
misspelled.
yeah.
you're talking about the type token frequency listings and i use those too.
you mean just uh on each on each line there's a one word.
right?
it's one token from the from the corpus.
uhhuh.
uhhuh.
yeah those are extremely efficient.
and and i i agree that's a very good use of it.
oh so you already have that.
okay.
well that's that's a way that's
you know the spell check basically does that.
but but in addition
yes that's that's exactly the strategy i want to do in terms of locating these things which are you know colloquial spoken forms which aren't in the lexicon.
uhhuh.
exactly.
because a lot of times they'll appear next to each other and uh
and then you then you can do a
yeah.
in alphabetized lists they'll appear next to each other and and so it makes it easier.
absolutely.
i agree.
that's a very good that's a very good uh suggestion.
and that was that's my strategy for handling a lot of these things in terms of things that need to be glossed.
i didn't get to that point.
but
so there are numbers then there are acronyms.
and then um there's a she wants the uh actually a an explicit marker of what type of comment this is.
so curly inside the curly brackets i'm going to put either voc for vocalized.
like cough or like laugh or whatever.
nonvoc for door slam.
and gloss for things that have to do with if they said a a spoken form with this this pronunciation error.
right.
i already had that convention.
but i i haven't been asking these people to do it systematically.
oh that's great.
because i think it most most efficiently handled by uh by a a filter.
that was what i was always planing on.
so that you know you get a whole long list
exactly what you're saying.
you get a whole list of things that say curly bracket laugh curly bracket.
uhhuh.
then you know it's it's
you you risk less error if you handle it by a filter than if you have this transcriber laboriously typing in sort of a voc space
yeah.
so so many ways that error prone.
right.
right.
so um um i'm i'm going to convert that via a filter into these tagged uh subcategorized comments.
and same thing with you know we see you get a subset when you do what you're saying.
you end up with a with uh you're collapsing across a frequency you just have the tokens.
uhhuh.
and you can um have a filter which more efficiently makes those changes.
uhhuh.
but the numbers and acronyms have to be handled by hand.
because you know i mean
you don't know what they could be.
yeah now timit's clear um and p l p is clear.
yeah.
but uh there are things that are not so well known in
or or have variant uses like the numbers you can say nine two or you can say ninety two.
and uh
so how are you doing the
i'd handle the numbers individually.
how are you doing the uh acronyms so if i say p z m what would it appear on the transcript?
it would be separate.
the letters would be separated in space.
okay.
and potentially they'll have a curly bracket thing afterwards.
but i'm not sure if that's necessary clarifying what it is.
uhhuh.
so gloss of whatever.
right.
i don't know if that's really necessary to do that.
maybe it's a nice thing to do because of it then indicating this is uh a step away from indicating that it really is intentional that those spaces are there.
and indicating why they're there to indicate that it's uh the you know uh enumerated or
uhhuh.
it's not a good way of saying.
but it's it's the specific uh way of stating these these letters.
right.
so it sounds good.
and so anyway the clean those are those things.
and then channelized is to then um get it into this multichannel format.
and at that point then it's ready for use by liz and don.
but that's been my top priority beyond getting it channelized.
the next step is to work on tightening up the boundaries of the time bins.
yeah.
right.
and uh thilo had a a breakthrough with this this last week in terms of getting the channel based um uh speech-nonspeech segmentation um up and running.
and i haven't i haven't been able to use that yet.
because i'm working
this is my top priority get the data clean and channelized.
i actually gave
have you also been doing spot checks jane?
oh yes.
okay good.
well you see that's part of the cleaning process.
i spent um
actually um i have a segment of ten minutes that was transcribed by two of our transcribers.
oh good.
good.
and i went through it last night it's it's almost spooky how similar these are word for word.
and there are some differences in commas because commas i i left them discretion at commas.
right.
uh and so
because it's not part of our of our needed conventions.
uhhuh.
and um
and so they'll be a difference in commas.
but it's word by word the same in in huge patches of the data.
and i have ten minute stretch where i can where i can show that.
and and sometimes it turns out that one of these transcribers has a better ear for technical jargon.
and the other one has a better ear for colloquial speech.
so um the one the colloquial speech person picked up gobbledy gook.
huh.
and the other one didn't.
and on this side this one's picking up things like neural nets and the one that's good on the on the vocabulary on the uh colloquial didn't.
right.
when
for the person who missed gobbledy gook what did they put?
it was an interesting approximation put in parentheses.
because i have this convention that if they're not sure what it was they put it in parentheses.
oh.
so they tried to approximate it but it was
oh good.
it was spelled g a b b l
sort of how it sounds?
yes.
more of an attempt to
yeah.
i mean apparently it was very clear to her that these this this was a sound these are the sounds.
it was a technical term that she didn't recognize.
yeah.
but
yeah.
but she knew that she didn't know it.
maybe it was a technical
exactly.
but she even though her technical perception is just really uh you
know i've i'm tempted to ask her if she's taken any courses in this area or if she's taken cognitive science courses.
right.
then because neural nets and oh she has some things that are oh down sampled she got that right.
huh.
and some of these are rather uh unexpected.
obscure.
yeah.
but ten solid uh chunk of ten solid minutes where they both coded the same data.
and um
and and again the main track that you're working with is eleven hours?
is that right?
yes exactly.
yeah okay.
and that's part of this
eleven hours.
is that is that that including digits?
yes it is.
yeah.
so let's say roughly ten hours or so of
uhhuh.
i mean it's probably more than that but but with of of non digits.
it'd be more than that.
yeah.
because i my recollection is the minutes that digits don't take more than half a minute per person.
oh okay.
but um the the total set that i gave them is twelve hours of tape.
oh i see.
but they haven't gotten to the end of that yet.
so they're still working some of them are two of them are still working on completing that.
oh i see.
yeah.
boy they're moving right along.
yeah they are.
yeah.
uhhuh they're very efficient.
there're some who have more hours that they devote to it than others.
uhhuh.
uhhuh.
yeah.
so what what what's the deal with with your
the channel thing?
yeah.
oh it's just uh i ran the recognizer uh the speech-nonspeech detector on different channels.
and it's just in uh in this new multi channel format and output.
and i just gave one one meeting to to liz who wanted to to try it for for the recognizer.
oh i see.
as uh apparently the recognizer had problems with those long chunks of speech which took too much memory or whatever.
right.
yeah.
and so she she will try that i think.
and
i'm i'm working on it.
so i hope
is this anything different than the h m m system you were using before?
yeah.
no.
uh i huh use some some different features but not not
uhhuh.
the basic thing is this h m m base.
so there's still no no knowledge using different channels at the same time.
you know what i mean?
there is some uh as the energy is normalized across channels.
across all of them.
yeah.
okay.
so
but basically that's one of the main changes.
uhhuh.
uhhuh.
what are some of the other features besides the energy?
you said you're trying some different features or something.
oh i just uh huh i just use um our loudness based things.
now as they
before there were they were some in in the log domain and i i changed this to the to the
cube root.
yeah.
to
no i changed this to the to the to the loudness thingy with the with the
huh.
uh.
how do you call it?
i'm not sure.
with the uh
fletcher munson?
no.
i'm not sure about the term.
oh okay.
uh i'll look it up.
yeah all right.
and say it to you.
uh okay and
yeah.
that's that's basically the the the thing.
okay.
yeah and i and i tried to normalize uh uh the features there's loudness and modified loudness um within one channel.
because they're yeah to to be able to distinguish between foreground and background speech.
and it works quite well.
but not always.
uhhuh.
uhhuh.
so
okay.
good.
um let's see.
i think the uh
were were you basically done with uh transcriptions or
so i guess the next thing is this uh bleep editing.
right.
so the the idea is that we need to have
we need to provide the transcripts to every participant of every meeting to give them an opportunity to bleep out sections they don't want.
so i've written a bunch of tools that will generate web pages uh with the transcription in it so that they can click on them and piece pieces and they can scroll through and read them.
and then they can check on each one if they want it excluded.
and then it's a form h t m l form so they can submit it and it will end up sending me e mail with the times that they want excluded.
and so uh some of the questions on this is what do we do about the privacy issue.
yeah.
and so i thought about this a little bit and i think the best way to do it is every participant will have a password.
a single password.
each person will have a single password user name and password.
and then each meeting we'll only allow the participants who were at that meeting to look at it.
and that way each person only has to remember one password.
i i can't help but wonder if this is maybe a little more elaborate than is needed.
i mean if people have
uh
i mean for me i would actually want to have some pieces of paper that had the transcription and i would sort of flip through it.
and then um if i thought it was okay i'd say it's okay.
uhhuh.
and i
uh i mean it depends how this really ends up working out.
but i guess my thought was that the occasion of somebody wondering whether something was okay or not and needing to listen to it was going to be extremely rare.
right i mean so the fact that you could listen to it over the web is a minor thing that i had already done for other reasons.
okay.
and so that that's a minor part of it.
i just wanted some web interface so that people you didn't actually have to send everyone the text.
so what my intention to do is that as the transcripts become ready um i would take them and generate the web pages.
and send email to every participant or contact them using the contact method they wanted.
and just uh tell them here's the web page.
um you need a password.
so question number one is how do we distribute the passwords?
and question number two is how else do we want to provide this information if they want it?
that's
i think what i was sort of saying is that if you just say here is a here is
i mean this maybe it sounds paleolithic but but i just thought if you handed them some sheets of paper that said uh here's what was said in this transcription is it okay with you.
and if it is here's this other sheet of paper that you sign that says that it's okay.
i think that um there are a subset of people who will want printouts that we can certainly provide.
and then they'd hand it back to you.
but certainly i wouldn't want a printout.
these are big.
and i would much rather be be able to just sit and leaf through it.
you find it easier to go through a large
i mean how do you read books?
well i certainly read books by hand.
but for something like this i think it's easier to do it on the web.
really?
i mean it
because you're going to get you know if i i'm i'm in a bunch of meetings and i don't want to get a stack of these.
i want to just be able to go to go to the web site and visit it as i want.
going to a web site is easy.
but flipping through a hundred pounds a hundred pages of stuff is not easy on the web.
well i don't think it's that much harder than paper.
i have one question.
really?
so
so are you thinking that um the person would have a transcript and go strictly from the transcript?
because i i do think that there's a benefit to being able to hear the tone of voice and the
so here's the way i was imagining it and maybe i'm wrong.
yeah.
but the way i imagined it was that um the largest set of people is going to go oh yeah i didn't say anything funny in that meeting.
just go ahead.
where's the where's the release?
and then there'll be a subset of people.
right?
okay there's
i mean think of who it is we've been recording mostly.
okay there'll be a subset of people who um will say uh well yeah i really would like to see that.
yeah.
and for them the easiest way to flip through if it's a really large document
i mean unless you're searching.
searching of course should be electronic.
but if you're not
so if you provide some search mechanism you go to every place they said something or something like that.
yeah.
but see then we're getting more elaborate with this thing.
um if if uh you don't have search mechanisms you just sort of have this really really long document.
i mean whenever i've had a really really long document that it was sitting on the web i've always ended up printing it out.
i mean so it's it's
i mean you you're you're not necessarily going to be sitting at the desk all the time.
you want to figure you have a train ride and there's all these situations where where i
i mean this is how i was imagining it anyway.
and then i figured that out of that group there would be a subset who would go huh you know i'm really not sure about this section here.
and then that group would need it
it seems like
if i'm right in that it seems like you're setting it up for the most infrequent case rather than for the most frequent case.
so that uh now we have to worry about privacy.
well no for the most
we have to worry about all these passwords for different people.
for the most frequent case they just say it's okay.
and then they're done.
and i think almost everyone would rather do that by email than any other method.
uhhuh.
the other thing too is it seems like
um yeah that's true.
go ahead.
i mean because you don't have to visit the web page if you don't want to.
yeah.
i guess
yeah i guess we don't need their signature.
i guess an email okay is all right.
oh that was another thing.
i i had assumed that we didn't need their signature that it that an email approval was sufficient.
but i don't actually know.
are are people going to be allowed to bleep out sections of a meeting where they weren't speaking?
yes.
i also
if someone feels strongly enough about it then i i i think they should be allowed to do that.
uhhuh.
so that means other people are editing what you say?
uh i don't know about that.
yeah.
i don't know if i like that.
well the only other choice is that the person would say no don't distribute this meeting at all.
and i would rather they were able to edit out other people then just say don't distribute it at all.
but what they signed in the consent form was something that said you can use my voice.
well but if if someone is having a conversation and you only bleep out one side of it that's not sufficient.
right?
yeah.
yeah but that's our decision then.
right?
um i don't think so.
i think it is.
i mean because if i object to the conversation
if i say we were having a conversation and i consider that conversation private and i consider that your side of it is enough for other people to infer i want to be able to bleep out your side.
the
i agree that the consent forms were uh i
i agree with what adam's saying that um the consent form did leave open this possibility that they could edit things which they found offensive whether they said them or didn't say them.
i see.
okay well if that's what it said.
and the other thing is from the standpoint of the of the
i'm not a lawyer.
but it strikes me that uh we wouldn't want someone to say oh yes i was a little concerned about it.
but it was too hard to access.
so i think it's kind of nice to have this facility to listen to it.
now in terms of like editing it by hand i mean i think it's some people would find that easier to specify the bleep part by having a document they edited.
but but it seems to me that sometimes um you know if a person had a bad day and they had a tone in their voice that they didn't really like you know it's nice it's nice to be able to listen to it and be sure that that was okay.
i mean i can certainly provide a printable version if people want it.
um i mean it's also a mixture of people.
um
i mean some people are do their work primarily by sitting at the computer flipping around the web.
and others do not.
yep.
others would consider it
this uh a a set of skills that they would have to gain.
you know?
well i think most of the people in the meetings are the former.
it depends on what meetings.
that's true.
so far.
so
in the meetings so far yeah.
yep.
but we're trying to expand this.
right?
right.
so i i i actually think that paper is the more universal thing.
and that
uhhuh.
well but if they want to print it out that's all right.
yeah.
i think everyone in the meeting can access the web.
no i think we have to be able to print it out.
it's not just if they want to print it out.
okay so does that mean that i can't use email?
i i think
or what?
because you could send it through email you're thinking.
i i
well we
well i don't think i
there was this
well i don't think we can send the text through email because of the privacy issues.
good.
no.
for security?
yeah okay good.
yeah.
right.
good point.
um so giving them you think a web site to say if you want to print it out here it is is not sufficient?
yeah.
i
certainly for everybody who's been in the meetings so far it would be sufficient.
i'm just wondering about
yeah i'm just thinking for people that that's not sufficient for what the only sufficient thing would be for me to walk up to them and hand it to them.
you could mail it to them.
get a mailing address.
yeah.
equivalent.
but i think it's easier to drop in the box.
but
just put the button on on the web page which say please send me the the scripts.
oh that's interesting.
that's right.
yeah.
what um
when you display it on the web page what are what are you showing them?
utterances?
uhhuh.
or
and so can they bleep within an utterance?
no.
whole utterances only.
whole utterances.
and that was just convenience for my sake that it's uh uh
it would end up being fairly difficult to edit the transcripts if we would do it at the sub utterance level.
because this way i can just delete an entire line out of a transcript file rather than have to do it by hand.
there's another aspect to this which maybe is part of why this is bothering me.
um i think you're really trying very hard to make this as convenient as possible for people to do this.
i mean that's why i did the web form because for me that would be my most convenient.
huh.
i i i understand.
i think that's the bad idea.
i know where you're going.
oh.
see because you're you're uh
really!
you're going to end up with all these little patchy things whereas really what we want to do is have the the the bias towards letting it go.
because you
know it
there was a
one or once or twice in the in the meetings we've heard where somebody said something that they might be embarrassed by.
but overall people are talking about technical topics.
nobody's going to get hurt.
nobody's being libeled.
you know this is this we're we're covering
we're playing the lawyer's game and we're playing we're we're we're looking for the extreme case.
if we really orient it towards that extreme case make it really easy we're going to end up encouraging a headache.
that i think that's
i'm sort of psyching myself out here.
i i'm trying to uh
but i i think that's
i guess i don't see having a few phrases here and there in a meeting being that much of a headache bleeped out.
well it's
so
but
i think what morgan's saying is the easier it is the more is going to be bleeped.
and and it really depends on what kind of research you're doing.
i think some researchers who are going to be working with this corpus years from now are really going to be cursing the fact that there's a bunch of stuff in there that's missing from the dialogue.
uhhuh.
you know it depends on the kind of research they're doing.
yeah.
but it might be uh it might be really a a pain.
and you know where it's really going to hurt somebody in some way.
the one who said it or someone who is being spoken about we definitely want to allow the option of it being bleeped out.
but i really think we want to make it the rare incidence.
and and uh i am just a little worried about making it so easy for people to do.
and so much fun that they're going to go through and bleep out stuff.
so much fun.
and they can bleep out stuff they don't like too.
right?
from somebody else as you say.
you know so well i didn't like what he said.
well i don't see any way of avoiding that.
i mean we have to we have promised that we would provide them the transcript and that they can remove parts that they don't like.
yeah.
so that the
no no i i i don't
the only question is
you've talked me into that.
but i i just think that we should make it harder to do.
the problem is if it's harder for them it's also harder for me.
whereas this web interface i just get email.
it's all formatted.
it's all ready to go and i can just insert it.
so maybe you don't give them access to the web interface unless they really need it.
well i guess
so so so
yeah.
i'm sorry.
so so so maybe this is a a way out of it.
huh.
you've provided something that's useful for you to do handle.
and useful for someone else if they need it.
but i think the issue of privacy and ease and so forth should be that uh they get access to this if they really need it.
well
so you're saying the the sequence would be more like first adam goes to the contact lists contacts them via whatever their preferred method is to see if they want to review the meeting.
right.
and then if they don't you're done.
if they do then he provides them access to the the web site.
well to some extent i have to do that anyway because as i said we have to distribute passwords.
or a printed out form.
there's there
so
but you don't necessarily have to distribute passwords is what i'm saying.
so
well but
only if they want it.
what i'm saying is that i can't just email them the password because that's not secure.
no no no.
but you aren't necessarily giving them
so they have to call me and ask.
right.
but we don't even necessarily need to end up distributing passwords at all.
huh
well we do because of privacy.
we can't just make it openly available on the web.
no no.
you're missing the point.
uhhuh.
we're we're trying we're trying to make it less of an obvious just uh fall off a log to do this.
not everyone gets a password unless they ask for it.
right?
so so what i would see is that first you contact them and ask them if they would like to review it for to check for the
yeah.
not just for fun.
okay?
but to to check this for uh things that they're worried about having said or if they're willing to just send an approval of it at from their memory.
um and uh and we should think carefully actually we should review go through how that's worded.
okay?
then if someone uh wants to review it uh
and i know you don't like this but i'm offering this as a suggestion is that is that we then give them a print out.
and then if they say that i have a potential problem with these things then you you say okay well you might want to hear this in context to think if you need that you issue them a password in the
but the the problem with what you're suggesting is it's not just inconvenient for them it's inconvenient for me.
because that means multiple contacts every time for every single meeting every time anyone wants anything.
i would much prefer to have all be automatic they visit the web site if they want to.
obviously they don't have to.
i know you'd prefer it but the
we have
yeah.
there's a problem with it.
so i think you're thinking people are going to arbitrarily start bleeping and i just don't think that's going to happen.
i'm also concerned about the spirit of the of the informed consent thing.
because i think if they feel that uh it's i i
you know if it turns out that something gets published in this corpus that someone really should have eliminated and didn't detect then it could have been because of their own negligence that they didn't pursue that next level and get the password and do that.
um but but they might be able to argue oh well it was cumbersome and i was busy and it was going to take me too much time to trace it down.
so it could that the burden would come back onto us.
so i'm a little bit worried about uh making it harder for them from the legal standpoint.
well you can go too far in that direction and you need to find somewhere between i think.
yeah.
because
it seems to me that sending them email saying if you have an o okay reply to this email and say okay.
uhhuh.
if you have a problem with it contact me and i'll give you a password.
seems like is a perfectly reasonable compromise.
and if they want a printout they can print it out themselves.
or we could print it up for them.
i mean we could offer that.
yeah.
but but there's uh another aspect to that and that is that in the informed consent form um my impression is that they that we offered them at the very least that they definitely would have access to the transcript.
and and i
i don't know that there's a chance of really skipping that stage.
yeah.
i mean i i thought that you were
maybe i misinterpreted what you said but it's
having access to it doesn't necessarily mean that having it.
having it.
giving it to them.
right?
okay.
it just means they have the right to have it.
well the in
all right.
the consent form is right in there if anyone wants to look at it.
fine.
okay.
fair enough.
so
yeah.
you want me to grab one?
well i could
i'm closer.
i could
yeah but you're wired.
aren't you?
yeah.
that is true.
um yeah i mean i don't want to fool them.
i don't know
i just meant that every any time you say anything to anyone there is in fact a a bias that is presented.
oh yeah yeah.
oh i know.
right?
yeah that's true.
of and
if you agree to participate you'll have the opportunity to have anything anything excised which you would prefer not to have included in the data set.
yeah.
yeah.
once a transcript is available we will ask your permission to include the data in the corpus for the larger research community.
there again you will be allowed to indicate any sections that you'd prefer to have excised from the data base.
and they will be removed both from the transcript and the recording.
huh.
well that's more open than i realized.
well i mean it
the one question is definitely clear with anything as opposed to just what you said.
i
yeah.
yeah uh no that it
that's true.
that's more severe but the next one says the transcript will be around.
that's right.
and it doesn't really say we'll send it to you.
or it'll be available for you on the web or anything.
i think it probably leaves it open how we get it to them.
i
at least it more often.
yeah.
it means also we don't have to to give it to them.
i mean like like morgan was saying they they
they just have to make sure that it is available to them.
it's available to them if they ask for it.
yeah okay.
so um i think i have an idea that may be may satisfy both you and me in this which is um
it's a it we just go over carefully how these notes to people are worded.
so i i just want it to be worded in such a way where it gives the strong it gives very i mean nothing hidden very strongly the bias that we would really like to use all of these data.
right.
that that we really would rather it wasn't a patchwork of things tossed out.
good.
that it would be better for um our uh field if that is the case.
but if you really think something is going to
and i don't think there's anything in the legal aspects that that is hurt by our expressing that bias.
great.
great great.
and then then my concern about which
yeah.
i agree.
you know you might be right it may be it was just paranoia on my part uh but people just
see i'm worried about this interface so much fun that people start bleeping stuff out just as just because they can.
it's just a check box next to the text it's not any fun at all.
yeah.
well i don't know.
i kind of had fun when you played me something that was bleeped out.
you know?
i
well but they won't get that feedback.
all
oh they won't?
no because it doesn't automatically bleep it at the time.
oh good.
it just sends me
so you haven't made it so much fun.
right.
oh good.
okay.
it just sends me the time intervals.
and then at some point i'll incorporate them all and put bleeps.
i mean i don't want to have do that yet until we actually release the data.
yeah.
because um then we have to have two copies of every meeting.
and we're already short on disk space.
yeah.
so i i want to i just keep the times until we actually want to release the data and.
then we bleep it.
okay.
all right so i think
yeah so if we have if
again let's you know sort of circulate the the wording on each of these things and get it right.
but but
well since you seem to feel uh strongest about it would you like to do the first pass?
okay.
uh fair enough.
turn about is fair play.
also there is this other question the legal question that that adam's raised uh about whether we need a concrete signature or email suffices or whatever.
sorry.
yeah.
and i don't know how that works.
there's something down there about if you agree to
i'm i'm i'm i thought i i thought about it with one of my background processes.
i don't think so.
and i uh it's uh it's uh it's fine to do the email.
uh.
fine.
good.
okay.
yeah because they're signing here that they're agreeing to the paragraph which says you'll be given an opportunity.
okay.
yeah.
and
and so i don't think they need another signature.
well and furthermore i
it's now fairly routine in a lot of arrangements that i do with people on contracts and so forth.
that that uh if it's if it's that sort of thing where you're you're saying uh okay i agree we want eighty hours of this person at such and such amount and i agree that's okay.
uh if it's a follow up to some other agreement where there was a signature it's often done in email now.
right.
so it's it's okay.
great.
um
so i guess i probably should at the minimum think about how to present it in a printed form.
i'm not really sure what's best with that.
the problem is a lot of them are really short.
well
and so i don't necessarily want to do one per line.
well i
but i don't know how else to do it.
i also have this
i i think it's nice you have it uh hearable on the on the web for those who might wonder about um the non nonverbal side.
i mean i i agree that our bias should be as as expressed here.
and but i i think it's nice that a person could check.
because sometimes you know you the words on a on the page come out sounding different in terms of the social dynamics if they hear it.
huh.
uhhuh.
and i realize we shouldn't emphasize that people you know shouldn't borrow trouble.
uhhuh.
what it comes down to but
yeah i think actually
my opinion probably is that the only time someone will need to listen to it is if the transcript is uh not good.
you know if if there are lots of mumbles and parentheses and things like that.
oh you know or what if there was an error in the transcript that didn't get detected and there was a whole uh segment against some personal
right.
that was all mumbled?
i think microsoft is
yeah.
yeah exactly.
oh.
sorry transcribers.
or or even or even there was a a line you know about how huh uhhuh bill gates duh duh duh duh
yeah.
but but it was all the words were all visible but they didn't end up there was a slip in the transcript.
oh god.
they're going to hate this meeting.
yeah.
yeah that's true.
actually liz will like it.
know but
liz will like it.
we had a pretty strong disagreement going there.
yep yep that's right.
yeah.
yeah.
so i don't know.
i mean i i guess we're assuming that the transcript is a close enough approximation and that that my double checking will be so close to absolutely perfect that it that nothing will slip by.
uhhuh.
but it the
something might sometime and they uh
if if it's something that they said they might
i mean you might be very accurate in putting down what they actually said.
uhhuh.
but when they hear it themselves they may hear something different because they know what they meant.
i don't know how to notate that.
sarcasm.
yeah that's right.
how do you how do you indicate sarcasm?
yeah that's right.
no i'm serious.
so the so the
so we might we might get some feedback from people that such and such was you know not not really what i said.
yeah.
well that would be good to get definitely.
yeah but
yeah sure.
just for corrections.
yeah.
so um in terms of password distribution i think phone is really the only way to do it.
phone and in person.
yeah.
or mail physical mail.
or if
leave it on their voice mail.
any sub word level thing.
any sub
yeah okay.
i mean you could do it with p g p or things like that but it's too complex.
you know i just realized something which is of this question about the uh the possible mismatch of
i mean
well and actually also the lawyer saying that um we shouldn't really have them have the people believing that they will be cleared by our checks.
you know?
i mean so it's like in a way it's it's nice to have the responsibility still on them to listen to the tape and and hear the transcript.
uhhuh.
to have that be the
well yeah but you can't
i mean most people will not want to take the time to do that though.
yeah okay fair enough.
and they're they're absorbing the responsibility themselves.
and they they have to
so it's not it's not um
yeah good.
but i mean if you were at a meeting and and you you don't think at least that you said anything funny.
and the meeting was about you know some some funny thing about semantics or something or uh
you probably won't listen to it.
yeah.
it is true that that the content is technical.
i and so and we're not having these discussions which
yeah.
i i mean when i listen to these things i don't find things that are questionable in other people's speech or in my own.
you would think it would be rare.
just
it should be very rare.
i mean we're not talking about the energy crisis or something people have
yeah.
yeah okay.
how about them energy crises?
yeah.
i think we're uh
done?
kind of done.
actually i was going to
did you have anything that's going on or
not really.
no.
um my project is going along.
but um i'm really just here to um fill the project uh the overall progress.
yeah.
i don't really have anything specific to to talk about.
that's fine.
i just didn't want to go by you if you had something.
oh okay.
you don't have anything to say.
nah.
no.
oh.
transcribers he was rattling the marbles in his brain back and forth just then this this
shall we do digits?
oh yeah.
um oh by the way i did find a bunch
it um
uh we should count out
how many more digits to forms do we have back there?
there were quite a few.
that's what i thought.
uh
i i was going through them all and i found actually a lot filed in with them that were blanks that no one had actually read.
huh.
and so we still have more than i thought we did.
oh good.
so we have a few more digits before we're done.
okay.
so let's see.
which one of these buttons will do this for me?
aha!
okay.
should you go back to the first one?
do i want to go back to the first one?
well
okay.
i'm sorry i
well i mean just to
okay.
introduce.
okay.
yeah um well the search for the middle layer.
it's basically uh talks about uh it just refers to the fact that uh one of main things we had to do was to decide what the intermediate sort of nodes were.
i can read!
i'm kidding.
uhhuh.
you know because
but if you really want to find out what it's about you have to click on the little light bulb.
although i've i've never i don't know what the light bulb is for.
i didn't install that into my powerpoint presentation.
it opens the assistant that tells you that the font type is too small.
uh.
do you want to try?
ach
i'd prefer not to.
okay.
continue.
it's a needless good idea.
is that the idea?
why are you doing this in this mode and not in the presentation mode?
okay.
because i'm going to switch to the javabayes program.
oh okay.
of course.
uhhuh.
and then if i do that it'll mess everything up.
i was wondering.
is that okay?
yeah it's okay.
sure.
can you maximize the window?
proceed.
you want me to
wait what do you want me to do?
can you maximize the window so all that stuff on the side isn't doesn't appear?
no it's okay.
it's it'll work.
well i can do that.
but then i have to end the presentation in the middle so i can go back to open up javabayes.
okay fine.
here let's see if i can
all right.
very nice.
is that better?
yeah.
okay.
uh i'll also get rid of this click to add notes.
okay.
perfect.
so then the features we decided or we decided we were talked about.
right?
uh the the prosody the discourse verb choice.
you know we had a list of things like to go and to visit and what not.
the landmark-iness of uh
i knew you'd like that.
nice coinage.
thank you.
uh of a of a building.
whether the
and this we actually have a separate feature.
but i decided to put it on the same line for space.
nice walls which we can look up.
because i mean if you're going to get real close to a building in the tango mode
right?
there's got to be a reason for it.
and it's either because you're in route to something else or you want to look at the walls.
the context which in this case we've limited to business person tourist or unknown.
the time of day.
and open to suggestions isn't actually a feature.
it's we are open to suggestions.
right.
can i just ask the nice walls part of it is that uh in this particular domain
you said be it could be on two different lines.
but are you saying that in this particular domain it happens the that landmark-iness is correlated with
no.
oh
we have a separate
they're separate things.
their being nice
feature.
okay.
yeah.
i either could put nice walls on its own line or open to suggestions off the slide.
and and
like you could have a
by nice you mean
you like you could have a post office with uh you know nice murals or something.
right.
okay.
or one time i was at this
so nice walls is a stand in for like architecturally it uh significant.
architecturally appealing from the outside.
or something like that.
but see the thing is if it's
okay.
yeah but if it's architecturally significant you might be able to see it from
like you might be able to vista it.
right?
uhhuh.
and be able to
uhhuh.
appreciate it.
yeah.
versus like
i was at this place in europe where they had little carvings of like dead people on the walls or something.
uhhuh.
i don't remember
uhhuh.
it was a long time ago.
there's a lot of those.
but if you looked at it real close you could see the the intricacy of the of the walls.
okay.
so that count as counts as a nice wall.
right.
the
okay.
uhhuh.
right.
something you want to inspect at close range because it's interesting.
the
exactly.
okay.
huh.
robert?
well there there is a term that's often used.
that's saliency or the salience of an object.
and i was just wondering whether that's the same as what you describe as landmark-iness.
but it's really not.
i mean an object can be very salient.
huh.
but not a landmark at all.
not a landmark at all.
there's landmark for um touristic reasons and landmark for
i don't know.
navigational reasons or something.
right.
yep.
yeah.
we meant uh touristic reasons.
yeah.
okay.
huh.
right.
okay.
but you can imagine maybe wanting the both kinds of things there for different um goals.
huh.
yeah.
right?
right.
but
yeah.
touristy landmarks also happen to be
wouldn't couldn't they also be
they're not exclusive groups.
are they?
like non tourist y landmarks and
or it can be
they're not mutually exclusive?
direct navigational
yeah.
right.
okay.
right.
definitely.
okay so our initial idea was not very satisfying because uh our initial idea was basically all the features pointing to the output node.
uh
so a big flat structure.
right?
right.
yep.
and uh so we
reasons being you know it'd be a pain to set up all the probabilities for that.
if we moved onto the next step and did learning of some sort uh according bhaskara we'd be handicapped.
i don't know belief nets very well.
well usually i mean you know n
if you have n features then it's two to the n or exponential in n.
and they wouldn't look pretty.
so
yeah.
they'd all be like pointing to the one node.
uhhuh.
uh
so then our next idea was to add a middle layer.
right?
so the thinking behind that was we have the features that we've drawn from the communication of some like the someone
the person at the screen is trying to communicate some abstract idea.
like i'm the the abstract idea being i am a tourist.
i want to go to this place.
right?
so we're going to set up features along the lines of where they want to go.
and what they've said previously and whatnot.
and then we have the means that they should use.
right?
but the middle thing we were thinking along the lines of maybe trying to figure out like the concept of whether they're a tourist or whether they're running an errand or something like that.
along those lines.
or
yes we things we couldn't extract the from the data the hidden variables.
yes.
good.
so then the hidden variables hair variables we came up with were whether someone was on a tour running an errand or whether they were in a hurry.
because we were thinking uh if they were in a hurry there'd be less likely to like or
want to do vista.
right?
because if you want to view things you wouldn't be in a hurry.
right.
or they might be more likely to be using the place that they want to go to as a like a navigational point to go to another place.
uhhuh.
whether the destination was their final destination.
whether the destination was closed.
those are all
and then let's look at the belief net okay.
so that means that i should switch to the other program.
um right now it's still kind of in a toy version of it.
because we didn't know the probabilities of or
well i'll talk about it when i get the picture up.
no one knows it.
okay.
so this right what we
let's see.
what happens if i maximize this?
there we go.
but uh
so the mode basically has three different outputs.
the probability whether the probability of a vista tango or enter
um the context we simplified.
basically it's just the businessman the tourist unknown.
verb used is actually personally amusing.
mainly because it's it's just whether the verb is a tango verb an enter verb or a vista verb.
yeah.
that one needs a lot of
and are those mutually exclusive sets?
not at all.
that's that that needs a lot of work.
no.
right.
got it.
but uh that would've made the probably significantly be more complicated to enter.
uhhuh.
yeah.
so we decided that for the purposes of this it'd be simpler to just have three verbs.
simple.
yeah.
stab at it.
yep.
right.
um
why don't you mention things about this bhaskara?
that i am not that are not coming to my mind right now.
okay so yeah so note the four nodes down there the sort of the things that are not directly extracted.
actually the five things.
the closed is also not directly extracted i guess.
from the uh
well it's
from the utterance?
huh.
actually no.
it's it sort of is.
wait.
it is.
because it's because have the the time of day.
okay closed sort of is.
and the close
it just had the uh and what time it closed.
right.
so
right.
but the other ones the final destination the whether they're doing business whether they're in a hurry and whether they're tourists.
that kind of thing is all uh sort of you know probabilistically depends on the other things.
inferred from the other ones?
yeah.
okay.
and the mode you know depends on all those things only.
yeah the the actual parse is somewhere up around in here.
yeah.
so we haven't uh managed
like we don't have nodes for discourse and parse.
although like in some sense they are parts of this belief net.
uhhuh.
but uh the idea is that we just extract those features from them so we don't actually have a node for the entire parse.
uhhuh.
right.
because we'd never do inference on it anyway.
so
so some of the the top row of things
what's what's disc admission fee?
whether they discuss the admission fees.
so we looked at the data.
oh.
and in a lot of data people were saying things like can i get to this place.
what is the admission fee.
so that's like a huge uh clue that they're trying to enter the place rather than uh to tango or vista.
uhhuh.
right.
okay.
so
i see.
there were there'd be other things besides just the admission fee.
but you know we didn't have
uhhuh.
that was like our example.
that was the initial one that we found.
uhhuh.
okay.
so there are certain cues that are very strong either lexical or topic based um concept cues.
from the discourse that
yeah.
for one of those.
and then in that second row or whatever that row of time of day through that so all of those some of them come from the utterance.
and some of them are sort of either world knowledge or situational things.
right?
so that you have no distinction between those and
right.
okay.
one uh uh um
anything else you want to say bhaskara?
um
unmark time of day?
yeah.
i i mean
one thing uh
yeah.
they're they're are a couple of more things.
i mean uh i would actually suggest we go through this one more time so we we all uh agree on what what the meaning of these things is at the moment.
and maybe what changes we
yeah
okay.
so one thing i i'm you know unsure about is how we have the uh the admission fee thing set up.
so one thing that we were thinking was by doing the layers like this
uh we kept um things from directly affecting the mode beyond the concept.
but you could see perhaps the admission fee going directly to the mode pointing at enter.
uhhuh.
right?
versus pointing just at tourist.
uhhuh.
uhhuh.
okay?
but we just decided to keep all the things we extracted to point at the middle and then down.
uhhuh.
why is the landmark
okay.
the landmark is facing to the tourists.
that's because we're talking about landmarks as touristic landmarks.
right.
not as possible um
yeah.
navigational landmarks.
navigational cue.
yeah.
navigational landmarks.
so
uhhuh.
yeah.
then
that would be whatever building they referred to.
prosody.
right.
so let's see.
the variables.
admission fee is a binary thing.
uhhuh.
time of day is like morning afternoon night.
is that the deal?
that's how we have it currently set up.
yeah.
but it could be you know based upon hour.
yep.
yeah.
yeah.
or we could discrete it descret-ize it.
whatever granularity.
yeah.
uhhuh.
uhhuh.
yeah.
yeah.
normally context will include a huge amount of information.
but um we are just using the particular part of the context which consists of the switch that they flick to indicate whether they're a tourist or not i guess.
yep.
okay.
so that's given in their input.
right.
so
right?
right.
so it's not really all of context.
similarly prosody is not all of prosody.
but simply for our purposes whether or not they appear tense or relaxed.
uhhuh.
that's very nice.
huh?
okay.
and
the the so the context is a switch between tourist or non tourist?
or also unknown?
or unknown.
yeah.
yeah.
okay.
unknown.
so final
right?
so it seems like that would really help you for doing business versus tourist.
which is which one?
but
okay.
so the the context being um
i don't know if that question's sort of in general.
are you
i mean the
are
do they allow business people to be doing non business things at the moment?
yeah it does.
okay.
so then you just have some probabilities over
everything is probablistic.
and there's always
okay.
over which which of those it is.
yeah.
um right.
so then landmark is
oh!
sorry.
verb used is like
right now we only have three values.
but in general they would be a probability distribution over all verbs.
uhhuh.
rather
let me rephrase that.
it it can take values in the set of all verbs that they could possibly use.
uhhuh.
um nice walls is binary.
closed is binary.
final destination again
yeah all those are binary i guess.
and mode is one of three things.
so the the middle layer is also binary?
no
yeah.
anything with a question mark after it in that picture is a binary node.
uh it
yeah.
but all those things without question marks are also binary.
right?
which things?
uhhuh.
nice walls.
oh.
nice walls is uh something that we extract from our world knowledge.
uhhuh.
yeah oh.
yeah sorry it is binary.
it is binary.
that's true.
but it doesn't have question mark because it's extracted.
yeah.
okay i see your point.
yeah.
yeah.
okay.
uhhuh.
i i gotcha.
yeah similarly closed i guess.
so we can either be in a hurry or not.
but we cannot be in a medium hurry at the moment?
well
to do that we would add another uh value for that.
uhhuh.
okay.
and that would require updating the probability distribution for mode as well.
uhhuh.
because it would now have to like uh take that possibility into account.
uhhuh.
take a
uhhuh.
so um of course this will happen when we think more about the kinds of verbs that are used in each cases.
yeah yeah.
but you can imagine that it's verb plus various other things that are also not in the bottom layer that would that would help you.
yeah.
like it's a conjunction of
i don't know.
you know the verb used and some other stuff that that would determine
right.
other syntactic information you mean?
yeah.
exactly.
yeah.
um
well the the sort of the landmark is is sort of the object.
right?
the argument in a sense?
usually.
i i don't know if that's always the case.
i i guess haven't looked at the data as much as you guys have.
so um
that's always warping on something some entity.
uhhuh.
uhhuh.
and um uh maybe at this stage we will we do want to uh sort of get uh modifiers in there.
huh yeah.
because they may also tell us whether the person is in a hurry or not.
yeah.
i want to get to the church quickly.
uhhuh.
and uh
yeah.
right.
that would be a cue.
what's the fastest way
yeah.
uhhuh.
correct.
um okay.
right.
excellent.
do we have anything else to say about this?
we can do a little demo.
oh the
yeah we could.
but the demo doesn't work very well.
no.
then it wouldn't be a demo.
i mean we can do a demo in the sense that we can um just observe the fact that this will in fact do inference.
i was just going to
observe nodes.
so we can you know set some of the uh nodes.
yeah.
go ahead.
and then try to find the probability of other nodes.
okay.
dat-dat-dah.
what should i observe?
just set a few of them.
you don't have to do the whole thing that we did last time.
just like uh maybe the fact that they use a certain verb.
okay.
actually forget the verb.
okay.
just uh
i don't know.
say they discussed the admission fee.
okay.
and uh the place has nice walls.
i love nice walls.
okay.
i'm a big fan.
it's starting to grow on me.
and it's night.
and the time of day is night?
yeah.
no.
wait.
that that doesn't uh
it's not really consistent.
they don't discuss the admission fee.
make that false.
all right.
and it's night.
oh they
okay.
oh whoops.
that didn't work.
i forgot to uh
ach!
i'd like to do that again.
one thing that bugs me about javabayes is you have to click that and do this.
yeah.
that seems kind of redundant.
but
okay.
that all you want?
yes.
okay.
so let's see.
i want to query.
go and right query.
right?
the mode.
okay and then on here
so let's see.
so that is the probability that they're entering vista ing or tango ing.
uhhuh.
yeah.
and uh
so slightly biased toward tango ing?
yeah.
okay.
if it's night time they have not discussed admission fee and the walls are nice.
so yeah.
i guess that sort of makes sense.
the reason i say the demo doesn't work very well is yesterday we uh observed everything in favor of taking a tour.
and it came up as tango.
right?
over and over again.
so
we couldn't we couldn't figure out how to turn it off of tango.
uhhuh.
huh!
it loves the tango.
um
well that's obviously just to do with our probabilities.
yeah yeah.
like we totally hand tuned the probabilities.
yeah.
right.
we were like huh well if the person does this and this and this let's say forty percent for this.
okay.
fifty like you know
so obviously that's going to happen.
right.
yeah.
yeah.
but it it
maybe the bias toward tango ing was yours then.
yeah.
yeah.
it's so we have to like fit the probabilities.
that's that's at
spent my youth practicing the tango de la muerte.
so the real case.
however you know it the purpose was not really at this stage to come up with meaningful probabilities.
but to get thinking about that hidden middle layer.
uhhuh.
and so
and
we would actually
i guess once we look at the data more we'll get more hidden nodes.
uhhuh.
yeah.
but i'd like to see more.
not because it would expedite the probabilities because it wouldn't.
it would actually slow that down tremendously.
um well yeah.
i guess.
but
not that much though.
only a little early.
no.
i think we should have uh exponentially more middle nodes than features we've extracted.
okay.
i'm i'm just
so are doing business versus tourist
they refer to your current task.
like like current thing you want to do at this moment.
um yeah.
well that's that's an interesting point.
whether you're it's whether it's not
and are
i think it's more like are you are tourist.
are you in like heidelberg for a
oh.
so i thought that was directly given by the context switch.
that's a different thing.
what if the context which is not set but still they say things like i want to go uh see the uh the the castle and uh et cetera?
is it
well the i kind of thought of doing business as more of running an errand type thing.
yeah.
business on the other hand is uh definitely what you're doing.
so if you run out of cash as a tourist and and and you need to go to the a
so
okay.
oh i see.
you may have a task.
you have to go get money and so you are doing business at that stage.
huh.
yeah.
right.
how do i get to the bank.
i see.
huh.
and that'll affect whether you want to enter or you if you
kind of thing.
okay.
so the tourists node should be um very consistent with the context node.
right?
if you say that's more their in general what their background is.
yeah.
i think this context node is a bit of a
i don't know.
like in
uh do we want to have
like it's
are you assuming that or not?
like is that to be i mean if that's accurate then that would determine tourist node.
if the context were to set one way or another that like strongly uh um says something about whether whether or not they're tourists.
uhhuh.
uhhuh.
so what's interesting is when it's not when it's set to unknown.
uhhuh.
what set the they set the context to unknown?
okay.
okay.
right now we haven't observed it.
so i guess it's sort of averaging over all those three possibilities.
uhhuh.
uhhuh.
right.
but yes.
you can set it to unknown.
and if we now do leave everything else as is the results should be the same.
oops!
right?
no.
well no.
because we the way we set the probabilities might not have
yeah.
it's it's an it's an issue.
right?
like
pretty much the same?
yeah.
it is.
so the issue is that um in belief nets it's not common to do what we did of like having you know a bunch of values and then unknown as an actual value.
yeah.
what's common is you just like don't observe the variable.
yeah.
right?
and then just marginalizes.
yep.
but uh we didn't do this because we felt that there'd
i guess we were thinking in terms of a switch that actually.
we were
yeah.
uhhuh.
we were
but uh i don't know what the right thing is to do for that.
i'm not i don't know if i totally am happy with the way it is.
why don't we
can we
um how long would it take to to add another node on the observatory and um play around with it?
another node on what?
uh well it depends on how many things it's linked to.
let's just say make it really simple.
if we create something that for example would be um
so some things can be landmarks in your sense.
but they can never be entered.
so for example a statue.
good point.
yeah?
right.
uhhuh.
so maybe we want to have landmark meaning now enterable landmark versus um something that's simply just a vista point for example.
yeah that's true.
yeah?
uh a statue or um
so basically it's addressing a variable that's enterable or not.
so like an enterable question mark.
also you know didn't we have a size as one?
what?
the size of the landmark.
um not when we were doing this.
because if it's
yeah.
but i guess at some point we did.
for some reason i had that okay that was a thought that i had at one point.
but then went away.
so you want to have a a node for like whether or not it can be entered?
well for example if we include that.
yeah.
yeah?
um accessibility or something.
yeah?
huh.
is it can it be entered?
then of course this is sort of binary as well.
yeah.
and then um there's also the question whether it may be entered.
in the sense that you know if it's tom the house of tom cruise you know it's enterable but you may not enter it.
you know?
you're not allowed to.
yeah.
unless you are whatever his his divorce lawyer or something.
yeah.
yeah?
and um and these are very observable sort of from the from the ontology sort of things.
does it actually help to distinguish between those two cases though?
whether it's practically speaking enterable or actually physically enterable or not?
it seems like it would for uh uh determining whether they want to go into it or not.
if if you're running an errand you maybe more likely to be able to enter places that are usually not you're not usually not allowed to uh
because they
well i can see why
let's get this uh clearer.
so it's matrix between if it's not enterable.
whether it's a whether it's a public building and whether it's actually has a door.
period.
yeah.
exactly.
okay.
this is sort of uh
uhhuh.
so tom cruise's house is not a public building.
but it has a door.
right.
uhhuh.
but the thing is
okay
explain to me why it's necessary to distinguish between whether something has a door and is not public.
or if something
it seems like it's equivalent to say that it doesn't have a door and it
uhhuh.
or not public and not a door are equivalent things.
it seems like in practice.
yeah.
right.
yeah.
so we would have
what does it mean then that we have to
we have an object type statue.
that really is an object type.
right.
so there is there's going to be a bunch of statues.
and then we have for example an object type huh that's a hotel.
how about hotels?
okay.
so the most famous building in heidelberg is actually a hotel.
it's the hotel zum ritter.
which is the only renaissance building in heidelberg that was left after the big destruction and for the thirty years war blah blah blah.
huh.
does it have nice walls?
it has wonderful walls.
excellent.
um and lots of detail and carvings engravings and so forth.
so
but um it's still an unlikely candidate for the tango mode i must say.
but um so so if you are a
well it's very tricky.
so i guess your question is so far i have no really no real argument why to differentiate between statues as statues and houses of celebrities.
from that point of view.
huh.
okay.
let let's do a
can we add just so i can see how it's done uh a has door property?
or
okay.
what would it uh connect to?
like what would uh it affect?
um i think um it might affect
oh actually it's it it wouldn't affect any of our nodes.
right?
what i was thinking was if you had a like
oh it's it affects the doing business is certainly not.
you could affect theoretically you could affect doing business with has door.
yeah.
huh.
okay.
it should um inhibit that.
right.
let's see.
right?
yeah.
i don't know if javabayes is nice about that.
it might be that if you add a new thing pointing to a variable you just like it just overwrites everything.
but you can check.
well we have it saved.
so we can open it up again.
okay.
it's true.
the safety net.
i think you could just add it.
i mean i have before.
okay.
whew!
well that's fine.
but we have to see the function now.
has it become all point fives or not?
oh right.
let's see.
so this is has door.
uh true false.
that's acceptable.
and i want to edit the function going to that.
right?
no.
this is fine.
oh no.
this business.
right.
it was fine.
added this one.
yep.
what would be nice if it is if it just like kept the old function for either value.
this
but
nope.
didn't do it.
oh.
oh wait.
it might be
did we
yes.
that's not good.
that's kind of annoying.
okay.
so just dismiss everything.
close it and and load up the old state so it doesn't screw screw that up.
let's see.
oops!
huh.
maybe you can read in.
so have you used javabayes a lot?
yes.
really i i've i haven't used it a lot.
and i haven't used it in the last you know many months.
so
okay.
um uh we can ask someone.
it might be worth uh asking around.
um
like we looked at sort of uh a page that had like a bunch of
yeah.
srini
okay.
yeah.
srini's the one to ask i would say.
i guess he'd be the person.
um he might know.
yeah.
because
and
yeah.
i mean in a way this is a lot of good features in java.
it's has a gui and it's uh
uhhuh.
i guess those are the main two things.
it does learning.
it has
uhhuh.
no it doesn't actually.
yeah.
what?
i didn't think it did learning.
okay.
maybe it did a little bit of learning.
oh right.
i don't remember.
maybe you're right.
okay.
right.
but uh it's free.
which is quite positive.
yeah.
but uh
yeah.
maybe another thing that uh
but i mean its interface is not the greatest.
so
uhhuh.
but actually it had an interface.
a lot of them were like you know
yep.
command line.
huh.
what is the code?
can can we see that?
how do you write the code?
the
or do you actually never have to write any code there?
yeah.
there is actually a text file that you can edit.
but it's
you don't have to do that.
there's like an x m l format for bayes-nets.
is it x m l?
there is one.
i don't know if this uses it.
oh i see.
no this doesn't use it.
but it
i didn't think it did.
yeah the the
you can look at the text file.
yeah.
but do you have it here?
well maybe you don't.
uh yes i do actually.
let me see.
oh yes.
of course.
like there's the
oh man.
i didn't is there an ampersand in dos?
nope.
just start up a new d o s.
that's all right.
i can probably double click on it.
or
yeah right.
uh
let's see.
yep.
let's see.
come on.
it'll ask you what you what it wants what you want to open it with and see what bat i guess.
one of these days it should open this.
theoretically.
go right mouse.
open with.
that's
oh!
oh there we go.
maybe it was just
oh!
oh.
uh it was dead to the world.
god!
okay.
through the old notepad.
that's my favorite editor.
i like i like word pad because it has the uh the returns.
wordpad?
i
the carriage returns on some of them.
uhhuh.
okay.
you know how they get auto fills i guess.
uhhuh.
or whatever you call it.
anyway there it is.
so this is sort of lisp y?
no.
uh yeah.
it just basically looks like it just specifies a bunch of structs.
uhhuh.
yeah.
that's how actual probability tables are specified.
as like lists of numbers.
yeah.
uhhuh.
uhhuh.
so theoretically you could edit that.
uhhuh.
it just that it's
but they're not very friendly.
yeah the ordering isn't very clear on
right.
so you'd have to like figure out like you have to go and
the layout of the table.
yeah.
yeah.
well i
actually we could write a program that could generate this.
yeah.
i think so.
you could.
yeah you could.
it's not
we were doing it
yeah.
we can maybe write an interface for uh entering probability distributions easily.
something like like a little script.
that might be worth it.
and that might do.
yeah.
i actually seem to recall srini complaining about something to do with entering probability.
so this is probably
the other thing is it is in java.
yeah it's
yeah.
so
we could manipulate the source itself?
yeah.
or
do you have the true source files?
i don't know if he actually
or just the class?
yeah.
uh yeah we do.
does he
i i saw directory called source.
oh.
uhhuh.
i didn't
or
yeah.
go up one.
up one.
yeah.
uh yes good.
source.
that's that's quite nice.
i don't know if it actually manipulate the source though.
that might be a bit complicated.
i think it might it might be simpler to just have a script that you know
uhhuh.
it's like friendly.
the the data tables.
it allows you enter things well.
yeah.
right.
but if if there is an x m l file that or format that it can also read
i mean it just reads this.
right?
when it starts.
uhhuh.
yeah i know there is an i was looking on the web page.
and he's updated it for an x m l version of i guess bayes-nets.
there's a bayes-net spec for in x m l.
uhhuh.
he's like this guy has?
the javabayes guy?
yeah.
so but he doesn't use it.
so in what sense has he updated it?
well you can either you or you can read both.
oh i see.
to my understanding.
okay.
oh.
that would be awesome.
because uh
well at least the uh
i could have misread the web page.
i have a habit of doing that.
but
okay wonderful.
okay.
so you got more slides?
do i have more slides.
um yes.
one more.
future work.
i think every presentation have a should have a future work slide.
but uh it's basically
we already talked about all this stuff.
so
um the additional thing is i guess learning the probabilities also.
that's maybe
i don't know if
uh that's future future work.
does
that's
yeah.
right.
very future.
uhhuh.
and of course if you have a presentation that doesn't have something that doesn't work at all then you have what i learned as a slide.
can't you have both?
you could.
my first approach failed.
right.
what i learned.
okay so i think that uh our presentation's finished.
good.
i know what i like about these meetings is one person will nod and then the next person will nod and then it just goes all the way around the room.
so the uh
i missed my turn.
no i earlier i went and bhaskara went and you did it.
you did it.
it's like yawning.
it's like yawning.
and this announcement was in stereo.
ha!
okay.
so this means um
should i pull up the net again?
yeah.
could you put the the um net up again?
yes.
thanks.
there we go.
and actually i was
because i got a wireless mike on.
so a more general thing than discussed admission fee um could be
i i'm just wondering whether the context the background context of the discourse might be
i don't know.
if there's a way to define it or maybe you know generalize it some way.
um there might be other cues that say um in the last few utterances there has been something that has strongly associated with say one of the particular modes.
uh i don't know if that might be
uhhuh.
uh and and into that node would be various various things that that could have specifically come up.
i think we
i think a a sort of general strategy here
you know this is this is excellent because um it gets you thinking along these terms
is that maybe we we could observe a couple of um discourse phenomena such as the admission fee.
and something else and something else that happened in the discourse before.
uhhuh.
right.
and um let's make those four.
and maybe there are two um
so maybe this could be sort of a separate region of the net which has two has it's own middle layer.
maybe this you know has some kind of um funky thing that
if this
and this may influence these hidden nodes of the discourse.
which is maybe something that is uh a more general version of the actual phenomenon that you can observe.
so things that point towards
so instead of single node for like if they said the word admission fee
exactly.
yeah.
admission fee or maybe you know how much to enter.
or you know something.
opening hours or something like that.
other cues.
exactly.
that would all funnel into one node that would constitute entrance requirements or something like that.
so pay a visit
uhhuh.
uh uh
sure.
yeah.
yeah?
i mean it sort of get into plan recognition kinds of things in the discourse.
i mean that's like the bigger um version of it.
exactly.
yeah.
and then maybe there are some discourse acts if they happened before.
um it's more for um a cue that the person actually wants to get somewhere else.
and that you are in a in a in a route.
um sort of proceeding past these things.
so this would be just something that where you want to pass it.
huh.
is that it?
however these are of course then
the the nodes the observed nodes for your middle layer.
so this again points to final destination doing business tourist hurry and so forth.
uhhuh.
okay.
yeah?
and so then we can say okay.
we have a whole region in a
that's a whole set of discourse related cues to your middle layer.
yeah exactly.
right?
and this is just then just one.
so because at the end the more we um add you know the more spider-web-ish it's going to become in the middle and the more of hand editing.
it's going to get very ugly.
but with this way we could say okay these are the discourse phenomena.
they may have there own hidden layer that points to some of the the real hidden layer um or the general hidden layer.
sure.
and the same we will be able to do for syntactic information.
the verbs used the object types used modifiers.
and maybe there's a hidden layer for that.
yep.
and so forth and so forth.
then we have context.
yeah.
so essentially a lot of those nodes can be expanded into little bayes-nets of their own.
yep.
uhhuh.
precisely.
so
one thing that's kind of been bugging me when i more i look at this is that the
i guess the fact that the there's a complete separation between the observed features and in the output.
yeah.
i mean it makes it cleaner but then uh i mean
that's true.
for instance if the discourse does
what do you mean by that?
well for instance the discourse admission fee node seems like it should point directly to the
uhhuh.
or increase the probability of enter directly versus going there via tourist.
yeah.
or we could like add more uh sort of middle nodes.
like we could add a node like do they want to enter it.
which is affected by admission fee and by whether it's closed and by whether it has a door.
uhhuh.
so it's like there are those are the two options.
right.
either like make an arrow directly or put a new node.
huh.
yeah.
that makes sense.
yeah.
and if it if you do it if you could connect it too hard you may get such phenomenon that like so how much has it cost to enter.
and the answer is two hundred fifty dollars.
and then the persons says um yeah i want to see it.
yeah?
meaning it's way out of my budget.
um
there are places in germany where it costs two hundred fifty dollars to enter?
um nothing comes to mind.
without thinking too hard.
um maybe
yeah of course.
um opera premiers.
really?
so you know
huh.
or or any good old pink floyd concert.
i see.
if you want to see the magic flute or something.
yeah.
or maybe um a famous restaurant.
or i don't know.
there are various things that you might not want to eat a meal there but your own table.
the spagos of heidelberg.
i think that the i mean nothing beats the the admission charge prices in japan.
so there two hundred dollars is is moderate for getting into a discotheque.
you know.
really?
then again everything else is free then once you're in there.
food and drink and so forth.
so i mean
but you know we can
something somebody can have discussed the admission fee and the answer is if we
um you know um still based on that result is never going to enter that building.
huh.
you know.
because it's just too expensive.
oh yeah.
i think i see.
so the discourse refers to admission fee but it just turns out that they change their mind in the middle of the discourse.
yeah.
you have to have some notion of not just
i mean there's a there's change across several turns of discourse.
right.
so i don't know how if any of this was discussed but how if it all this is going to interact with whatever general uh other other discourse processing that might be happen.
uhhuh.
uhhuh.
i mean
yeah yeah.
what sort of discourse processing is uh are the
how much is built into smartkom and
it works like this.
the uh um i mean
the first thing we get is that already the intention is sort of
they tried to figure out the intention.
right?
simply by parsing it.
and this um won't differentiate between all modes.
yeah?
but at least it'll tell us okay here we have something that
somebody that wants to go someplace.
now it's up for us to figure out what kind of going there is is is happening.
and um if the discourse takes a couple of turns before everything all the information is needed
what happens is you know the parser parses it.
and then it's handed on to the discourse history which is um one of the most elaborate elaborate modules.
it's it's actually the the whole memory of the entire system that knows what who said what which was what was presented.
it helps anaphora resolution and it and it fills in all the structures that are omitted.
so um
because you say okay how can i get to the castle.
oh how how much is it.
and um yeah i would like uh um to let's do it and so forth.
so even without an anaphora somebody has to make sure that information we had earlier on is still here.
uhhuh.
because not every module keeps a memory of everything that happened.
so whenever the uh um person is not actually rejecting what happened before.
so as in no i really don't want to see that movie.
i'd rather stay home and watch t v.
um what movie was selected in what cinema in what town is is going to be sort of added into the into the representations every at each dialogue step by the discourse model discourse model.
yeah that's what it's called.
and um it does some help in the anaphora resolution.
and it also helps in coordinating the gesture screen issues.
so a person pointing to something on the screen you know
huh.
the discourse model actually stores what was presented at what location on the on the screen.
so it's a it's a rather huge huge thing.
but um um we can sort of
it has a very clear interface.
we can query it whether admission fees were discussed in the last turn.
and and the turn before that or you know
how deep we want to search
okay.
um which is a question.
how deep do we want to sear?
you know?
um but we should try to keep in mind that you know we're doing this sort of for research.
so we we should find a limit that's reasonable and not go you know all the way back to adam and eve.
you know did that person ever discuss admissions fee fees in his entire life?
and the dialogues are pretty pretty you know concise.
and
anyway
so one thing that might be helpful which is implicit in the use of admission fee discussion as a cue for entry is thinking about the plans that various people might have.
like all the different sort of general schemas that they might be following.
okay.
this person is um finding out information about this thing in order to go in as a tourist or finding out how to get to this place in order to do business.
um because then anything that's a cue for one of the steps would be slight evidence for that overall plan.
um i don't know.
they're in in in sort of more traditional a i kinds of plan recognition things you sort of have you know some idea at each turn of agent doing something.
okay what plans is this a consistent with?
and then get some more information and then you see here's a sequence that this sort of roughly fits into.
it it might be useful here too.
i i don't know how
uhhuh.
you know you'd have to figure out what what knowledge representation would work for that.
i mean the
huh.
it's in the these these these plan schemas.
i mean there are some some of them are extremely elaborate.
you know.
what do you need need to buy a ticket?
uhhuh.
you know.
uhhuh.
and it it's fifty steps.
uhhuh.
huh.
just for buying a ticket at a ticket counter.
you know and and maybe that's helpful to look at it to look at those.
it's amazing what human beings can do.
when we talked uh we had the example you know of you being uh a a person on a ticket counter working at railway station.
and somebody runs up to you with a suitcase in his hands says new york.
and you say track seven.
huh?
and it's because you know that that person actually is following you know you execute a whole plan of going through a hundred and fifty steps you know without any information other than new york.
huh.
inferring everything from the context.
so works
um even though there is probably no train from here to new york.
right?
huh.
not direct.
you'd uh probably have to transfer in chicago.
uhhuh.
but uh it's possible.
um no you probably have to transfer also somewhere else.
right?
is that san francisco chicago?
i think
is that possible?
one time i saw a report on trains.
and i think there is a
i don't know if
i thought there was a line that went from somewhere.
maybe it was sacramento to chicago.
but there was like a california to chicago line of some sort.
uhhuh.
huh.
i could be wrong though.
it was a while ago.
the transcontinental railroad.
doesn't that ring a bell?
i think it has to exist somewhere.
yeah.
but i don't know if it's still
they might have blown it up.
well it never went all the way.
right?
i mean you always had to change trains at omaha.
well most of the way.
uh uhhuh.
right?
one track ended there and the other one started at five meters away from that.
yeah.
and sort of
well you seem to know better than we do so.
yeah?
has anybody ever been on an amtrak?
i have.
but not transcontinentally.
i'm frightened by amtrak myself.
what?
why?
i just
they seem to have a lot of accidents on the amtrak.
really?
their reputation is very bad.
yeah yeah.
huh.
it's not maybe reality.
it's not like german trains.
like german trains are really great.
so
but you know i don't know whether it's which ones are safer you know statistically.
um but they're faster.
yeah.
much faster.
uhhuh.
and there's much more of them.
yeah they're yeah it's way better.
yeah i used um amtrak quite a bit on the east coast.
and i was surprised.
uhhuh.
it was actually okay.
you know on boston new york.
yeah.
new york rhode island.
yeah.
i've done that kind of thing.
whatever.
boston.
uhhuh.
yeah.
but
that's a different issue.
this is going to be an interesting transcript.
huh.
i i want to see what it does with uh landmark-iness.
that's
yeah.
let's all say it a few more times.
just kidding.
so
it'd help it figure it out?
right.
yeah.
so by the way that structure that robert drew on the board was like more um cue type based.
right?
here's like we're going to segment off a bit of stuff that comes from discourse.
and then some of the things we're talking about here are more
you know we mentioned maybe if they talk about um
i don't know.
entering or you know like they might be more task based.
huh.
so i i don't know if there there's obviously some more than one way of organizing the variables into something.
so
i think that um what you guys did is really nicely sketching out different tasks and maybe some of their conditions.
uhhuh.
one task is more likely you're in a hurry when you do that kind of doing business.
uhhuh.
and and less in a hurry when uh you're a tourist.
um tourists may have never have final destinations you know because they are eternally traveling around.
so maybe what what what happened what might happen is that we do get this sort of task based middle layer.
uhhuh.
uhhuh.
and then we'll get these sub middle layers that are more cue based.
that feed into those.
nah?
uhhuh.
might be might be a nice dichotomy of of the world.
so um i suggest to for to proceed with this in in the sense that maybe throughout this week the three of us will will talk some more about maybe segmenting off different regions.
and we make up some some toy observable nodes.
is that what
refined just refine the
okay.
what's the technical term?
for which?
for the uh nodes that are observable.
the outer layer.
just observable nodes.
evidence nodes.
the features.
i don't know.
whatever you
feature make up some features for those.
yeah.
identify four regions.
maybe make up some features for each region.
and uh and uh uh and uh
middle layer for those.
and then these should then connect somehow to the more plan based deep space.
yeah.
basically just refine some of the more general nodes.
yep they they will be ad hoc for for for some time to come.
yeah this is totally like
the probabilities and all are completely ad hoc.
we need to look at all of them.
i mean but they're even like i mean like close to the end we were like uh you know we were like uh really ad hoc.
it's a even distribution.
like
whatever.
right?
because if it's like uh if it's four things coming in.
right?
and say some of them have like three possibilities and all that.
so you're thinking like like a hundred and forty four or something possible things numbers to enter.
and
that's terrible.
right?
so
some of them are completely absurd too.
that's uh well
like they want to enter but it's closed.
it's night time.
you know
there are tourists and all this weird stuff happens at the line up and you're like
yeah the only like possible interpretation is that they are like come here just to rob the museum or something to that effect.
confused.
in which case you're supposed to alert the authorities and see appropriate action.
yeah.
yeah.
yeah another thing to do um is also to um i guess to ask around people about other bayes-net packages.
is srini going to be at the meeting tomorrow?
do you know?
maybe.
the day after tomorrow.
wait
quite possibly.
oh oh sorry.
wednesday.
day after tomorrow.
yeah.
sorry.
wednesday.
yeah.
maybe we can ask him about it.
who's talking on wednesday?
huh.
i haven't jerry never sent out a sent out an email.
did he ever?
no.
but he mentioned at the last meeting that someone was going to be talking.
i forget who.
uh
oh.
isn't ben?
ben?
i think it's ben actually.
ben then.
ben.
yeah.
uh!
um
giving his job talk i think.
um
sorry.
i was just reading the screen.
okay.
oh.
yeah.
so the uh
that will be one one thing we could do.
i actually uh have um
also we can uh start looking at the smartkom tables.
and i will
right.
i actually wanted to show that to you guys now.
but um
do you want to trade?
um no.
i i actually made a mistake because it it fell asleep.
and when linux falls asleep on my machine it's it doesn't wake up ever.
so i had to reboot.
oh no!
and if i reboot without a network i will not be able to start smartkom because i need to have a network.
uh.
so we'll do that maybe uh
but
okay.
but once you start start smartkom you can be on you don't have to be on a network anymore.
is that the deal?
yep.
uh interesting.
why does smartkom need a network?
um it looks up some stuff that you know is is that is in the written by the operating system only if it if you get a d h c p request.
so it you know my computer does not know its i p address.
you know?
uh.
you know so
unless it boots up with networking.
it's plugged in.
yeah.
and i don't have an i p address they can't look up
they don't know who localhost is and so forth and so forth.
huh.
always fun.
but it's a um simple solution.
we can just um go downstairs and and and look at this.
but maybe not today.
the other thing um i will
oh yeah.
okay i have to report um data collection.
we interviewed fey.
uhhuh.
she's willing to do it.
meaning be the wizard for the data collection.
also maybe transcribe a little bit if she has to.
but also recruiting subjects organizing them and so forth.
so that looks good.
jerry however suggested that we should uh have a trial run with her.
see whether she can actually do all the uh spontaneous eloquent and creativeness that we uh expect of the wizard.
and i talked to liz about this.
and it looks as if friday afternoon will be the time when we have a first trial run for the data.
so who would be the subject of this trial run?
pardon me?
who will there be a is one is you one of you going to be the subject?
like are you
um liz also volunteered to be the first subject which i think might be even better than us guys.
good.
one of us.
yeah.
if we do need her for the technical stuff then of course one of you has to sort of uh jump in.
i like how we've you guys have successfully narrowed it down.
is one of you going to be the subject.
is one of you
jump in.
reference.
i haven't done it yet.
well i just figured it has to be someone who's um familiar enough with the data to cause problems for the wizard so we can uh see if they're you know good.
oh plants someone who can plant difficult things.
yeah.
i mean that's what we want to check.
right?
um
well in this case it's a it's a sort of testing of the wizard rather than of the subject.
isn't that what it is?
it's uh
yes we we would like to test the wizard.
but you know if we take a subject that is completely unfamiliar with the task or any of the set up we get a more realistic
i guess that would be reasonable.
yeah.
yeah.
you know set up as
i know.
that's probably a good enough test of
uhhuh.
sort of having an actively antagonistic uh
yeah.
yeah.
that might be a little unfair.
um
yeah.
i'm sure if we uh
you think there's a chance we might need liz for whatever the technical side of things.
i'm sure we can get other people around who don't know anything um if we want another subject.
yeah yeah.
you know like i can drag ben into it or something.
although he might cause problems.
but
so is it a experimental setup for the um data collection totally ready determined?
i like that.
test the wizard.
i want that on a t shirt.
um i think it's it's it's i mean
experimental setup on the technical issue.
yes.
except we i think we still need uh a recording device for the wizard.
just a tape recorder that's running in a room.
uhhuh.
but um in terms of specifying the scenario um uh uh we've gotten a little further.
uhhuh.
but um we wanted to wait until we know who is the wizard.
and have the wizard partake in the ultimate sort of definition probe.
so so if if on friday it turns out that she really likes it and and we really like her then nothing should stop us from sitting down next week and getting all the details completely figured out.
uhhuh.
and um
okay.
so the ideal task um will have whatever
i don't know how much the structure of the evolving bayes-net will affect
like we want to we want to be able to collect as much of the variables that are needed for that.
huh yeah some
right?
in the course of the task?
well not all of them.
but you know
i'm even this this tango enter vista is sort of itself an ad hoc scenario.
uhhuh.
uhhuh.
the the basic um idea behind the uh data collection was the following.
the data we get from munich is very command line.
simple linguistic stuff.
uhhuh.
hardly anything complicated.
no metaphors whatsoever.
uhhuh.
not a rich language.
so we wanted just to collect data to get that that that elicits more uh that elicits richer language.
uhhuh.
and we actually did not want to constrain it too much.
uhhuh.
you know?
just see what people say.
and then maybe we'll discover the phenomenon the phenomena that we want to solve you know with whatever engine we we come up with.
um so this this this is a parallel track.
okay.
you know there they hopefully meet.
so in other words this data collection is more general.
but since
it could it could be used for not just this task.
it should tell us you know what kind of phenomenon could occur.
it should tell us also maybe something about the difference between people who think they speak to a computer versus people who think they speak to a human being.
uhhuh.
and the sort of differences there.
so it may get us some more information on the human machine pragmatics um that no one knows anything about as of yesterday.
and uh nothing has changed since then.
so
uh and secondly now that of course we have sort of started to lick blood with this.
and especially since um johno can't stop tango-ing we may actually include you know those those intentions.
so now i think we should maybe have at least one navigational task with with sort of explicit uh
uhhuh.
not it's implicit that the person wants to enter.
uhhuh.
and maybe some task where it's more or less explicit.
that the person wants to take a picture.
uhhuh.
or see it or something.
so that we can label it.
i mean that's how we get a corpus that we can label.
uhhuh.
exactly.
whereas you know if we'd just get data we'd never know what they actually wanted.
we'd get no cues.
yep.
alrighty.
okay.
that was that.
so is this the official end of the meeting now?
yep.
looks like it.
so what's economics the fallacy?
i just randomly label things.
so that has nothing to do with economics or anything.
oh really?
okay.
maybe we ought to switch off these things before we continue.
okay.
switching
and we're on.
so uh
time?
fourteen whatever.
so topic number one is we got the n s f a t r.
i t r.
excuse me.
which was originally pretty big but it's divided among many sites.
and they like cut the budget by a by two thirds.
so they they approved it for all the sites.
but only gave us like sixty two percent of the money.
or excuse me cut it by sixty two percent.
thirty two.
yeah thirty whatever.
so uh morgan is uh
and the other condition is that they have to get a budget out like immediately.
or or they'll give the money to someone else.
by next friday.
so morgan's working on that.
uhhuh.
so i mean it's good news.
it's not as good as it could have been.
but it was uh
it's more money.
the the i t rs are you know a a long shot and so it's it's really nice that we got that.
what does that stand for?
information technology research.
something like that.
something like that.
huh.
all right.
it's uh
doesn't say much.
yeah it's pretty vague.
it's it's um it's some money that we can use
it's government run i guess.
uh i mean we're in conjunction with uh s r i washington and um columbia.
uhhuh.
okay.
and to do research on the meetings.
cool.
yeah it was specifically on the meeting.
not great.
so was that the one that was the meeting maps?
yeah.
okay good.
i liked that one.
yeah.
what is meeting maps?
mapping meetings or
well the the sort of general idea was you can think about meetings at lots of different levels.
all the way from all the way down to acoustics to all the way up to dialogue discourse topic.
yeah.
yeah.
yeah.
and so this this was how do you map all that information onto that?
onto the meeting domain.
it's like creating a a map of a meeting in a sense.
so that the the analogy was you have maps of different things at different resolutions.
and so uh
uh okay.
so this had a lot of different stuff in it.
okay.
it actually doesn't have a whole lot of speech stuff to it.
yeah sure.
yeah that's
it's mostly higher level.
it's
yeah.
higher level yeah.
yeah.
yeah.
but nonetheless i think if we
it's a it's a cool idea.
and if we can actually get it get it going that would be neat.
so and uh this was the one also that was
uh
was it with susan ervin tripp?
i don't remember.
it might have been
yeah we had to get some
with who?
she's a linguist on campus.
okay.
anyway.
so other topics i wanted to talk about are um the darpa demo which i guess since morgan isn't here we can't really talk about.
i just wanted to make sure it was ready to go.
yeah.
uh but i guess
i sent
your stuff is all ready.
so it's just a question of did morgan get around to testing it?
yeah and i sent you an email
yeah i got it with the instructions.
with the instructions.
i don't know.
so morgan
i guess morgan's leaving on saturday?
yep.
so until then
i guess we might hear something.
yeah so i guess we we had better be on call for a little while.
but
yeah exactly.
did he ever figure out how to switch between applications like he was wanting to do?
i don't know.
i haven't spoken with him.
yeah.
he did?
yeah.
switch between applications?
how is he going to do it?
alt tab.
it's just alt tab.
like if you hold down alt and then tap tab you just the just the task manager window comes up.
yeah yeah he
yes that's
i i think
he
right.
i think the problem was that i was saying hit alt tab with the assumption that he knew what i meant.
yeah.
but he didn't.
so
no he said he tried that but he didn't like it because it
well
uh
it took it out of the um
but it doesn't.
presentation mode.
it doesn't.
well because if you just touch alt
if you just hit it once it'll go back to the previous application.
and the previous application was the powerpoint demo.
which was the display of like you know
oh!
and and all i said was oh just cycle through with alt tab.
not the slide show.
because that's the standard way of doing it in windows.
yeah he didn't
but of course he doesn't use windows.
yeah.
right.
so
he didn't know what i meant.
oh so what do you have to do?
huh.
you have alt tab and
hold down at hold down alt.
and then tap tab several times.
and you'll cycle through all the open applications.
yeah.
yeah.
uh okay and then it won't
and then you can come back to your full screen version of your
right.
right.
yeah.
oh!
and i told them um if he decides to use multiple examples for um the prosody demo to open up separate transcribers.
just so you don't have to load up files during the meeting.
that's a good idea.
yeah.
so
yeah.
that's not a bad idea at all.
so that uh
he's got the memory for it.
so
yeah and since the files are short it's just
yeah they're all really short.
i mean the tcl t k overhead is pretty high.
but it shouldn't matter.
how much memory does he have?
no i didn't have any problems.
i'm sorry?
how much memory does he have?
i don't know.
but it seemed it didn't seem to be a problem at all.
i would imagine you add sixty four to a hundred and twenty eight.
huh.
i think he had one twenty eight.
yeah either one will work i think.
uh
but
it was faster than my p c.
i mean it was
huh.
so
wow!
yeah it was fine i think
yeah i mean i i i we kind of walked through it a little bit on his on his laptop.
so are we the only ones who are giving demo or is uh washington going to have a demo too?
i could.
i assume but they're all doing communicator stuff.
or
oh okay.
so
huh.
and it's part of the talk itself.
so it's really not a big part.
yeah i see.
i mean the talk's only twenty minutes.
it's just going to be a couple of
so you know all this time we spent on it's going to be thirty seconds of screen time.
i know.
oh my gosh.
right uhhuh
oh!
but it's a good tool to have.
yeah
the the the meeting i r stuff is actually
actually both of them.
you know because we're going to want to be playing with the
uhhuh.
uh
help my brain.
with the uh
prosody?
prosody stuff anyway.
um so actually at some point we should talk to users of that technology who are actually going to be using it.
and see what they would like to see.
yeah have you guys seen the display?
have you guys seen either of the demos?
they're pretty cool.
no.
nope.
like the prosody demo we basically loaded up um word alignments.
so instead of having um utterances in the bottom it's like just word by word.
uhhuh.
so you can see exactly where the overlaps are.
oh.
and then um adam did something where um we converted these feature files into wave files.
so you can display them.
display the features.
instead of the wave file?
yeah instead of the wave file.
oh okay.
and then using multi wave you can add a file and play that sound.
uhhuh.
play the audio corresponding to that um um feature file.
okay.
and it's all like aligned and stuff.
uh great.
so you see like the f zero contour?
yeah so you see the pitch contours.
and you and when you hit play like there's a line that goes through it.
you know and it's all aligned with like the the audio and the feature file.
uhhuh.
oh that's great.
audio.
yeah.
and the stylized f naught features are pretty cool.
huh.
yeah.
so these does a piece wise linear.
and so normally if you look at an f naught track it jumps all over the place.
yeah.
yeah.
this is a nice smooth one.
oh neat.
and it really does a good job.
and there's something like a median filter in it or something?
and uh and it's it's a linear fit or whatever.
yeah there's a
it's a combination.
yeah.
there's a median filtering and then there's a piece wise linear fit based on some criteria.
yeah.
yeah.
i'm not sure.
okay.
uh
actually could you email me a reference to that paper if you have it?
yeah sure.
because i'm sort of curious how what criteria it has.
i should email morgan about it too.
because he was asking about that.
and yeah.
i mean so there's obviously a trade off between the number of knots you pick and the best fit.
i mean so if you're doing a piece wise linear you have to figure out how many pieces you want.
yeah.
yeah sure.
and so there's a trade off.
because the best the best mean squared fit would be with an infinite number of knots.
right well then it's not a linear fit.
yeah.
yeah yeah.
you're not fitting anything then.
well sure it is.
but
well
yeah.
so so you there's a trade off.
yeah it
it's kind of a useless one.
and i'm wondering what criteria they use.
yeah i actually don't know off the top of my head.
uhhuh.
was this code that you downloaded to do this?
or was it
this was um kemal sonmez at s r i.
who's a co worker with uh liz.
huh.
dot com.
at s r i dot com?
yeah did i just
no i you said at s r i.
no
so i had to add the dot com.
it sounded like a u r l the way you were saying it.
oh oh.
yeah.
sorry.
um yeah but so kamal has uh
he did a paper on this for some speech verification uh project.
and we basically extrapolated what he what he did for that work and applied it to our files.
and that's how we do that's how we get our feature files and everything for all the prosody.
so what i mean is like the implementation that you used was something that you coded up here?
or you grabbed code from somewhere else?
that
no we grabbed most of it.
like we grabbed the linear fitting stuff.
uh.
i see.
we did all the alignments.
and all the matching and stuff.
uhhuh.
and
but the actual coding of the
and
the math was done somewhere else.
these are our f zero candidates which i'm supposed to use for the synthesis thing?
i suppose?
uh so that's what you were working on.
right?
right that's the same software i used to get the data.
yeah okay.
okay.
i i gave you a couple examples.
yeah i didn't have time to to look at that.
and
huh.
so
right right.
yeah.
so other topic is uh the i b m transcription status stuff.
so we've sent a few to them.
we've gotten a few back.
um
i sort of wish jane was were here.
because i think she gave
didn't she give one of them to one of the transcribers?
yeah it's done now.
and how was it?
she said it was much faster.
great.
it really helped a lot to have that.
so um you know there were a
much faster than doing everything alone?
hand.
from scratch.
okay.
yeah so
from end to like from the beginning of the process to the end of the process?
no no so from the point where we take it it's faster.
well but that's what we wanted.
yeah.
right?
we knew it would be longer to have multiple groups.
ooo yeah.
so it's sort of like doing things in parallel.
yeah right.
right.
you know so if you know if they can be working on that while we're working on the other things then when we get them in they don't have to do that one from scratch basically.
well pipelined.
uhhuh.
right.
the idea is that we're taking less time of the linguists.
right.
of the transcribers.
so
yeah right now it is taking a bit of time to turn around the meetings from i b m.
there's there's several problems.
one is um we were giving them these large files.
which they then had to split up and put onto cassette tapes.
so either ninety minute or sixty minute tapes.
oh wow.
so they had to try to find an appropriate place to break this large file that we gave them.
so one of the things that we did is adam made a modification to his script that generates this so you can tell it that you want uh basically uh chunks that are no more than either thirty or forty five minutes.
uhhuh.
and then we can give those to i b m.
and it'll make it easier for them to put them on tape.
um then the other thing is that these pool of transcribers that they're using um sort of are not dedicated to to our project.
but are in use for all i b m projects.
and i b m has recently giving given them uh a whole bunch of stuff.
so it's going to sort of delay us.
we might want to re visit hiring our own external transcription company.
because i think that would be cheaper than having our people do it.
but do it with the same process with these beep files.
huh that's an interesting idea.
i mean because originally i had discounted that.
because it was just going to be too hard.
but now we have this procedure with the beep files worked out.
yeah so the only difference would be we would be putting the stuff on tape versus i b m.
then maybe it would be okay.
yeah.
yep.
and then we could send if if we had that process down of putting the audio onto tape
we can send i b m the tapes or we can send this transcription place the
and we might want to do that anyway.
because it seems like that's being a bottleneck and that's silly.
it shouldn't be a bottleneck.
yeah.
i mean it would once we got it set up it it
well
so i don't know where the bottleneck was.
was the bottleneck in breaking this large file up into the appropriate size chunks or was it actually putting it onto the tape?
it may well have been partially that you know.
because they tried it.
and then they would have to listen to the tape and find out where it bleeps.
exactly.
and go forward in the file et cetera et cetera.
yeah.
but uh
yeah.
it just seems like it's not hard for us to do.
and that would just make things go faster.
yeah so how do we do that?
how would we put
get a tape deck.
yeah.
yeah.
plug it in to the computer.
plug it into the sound card.
hit record.
on the tape deck.
yeah.
and just do it all all uh analog.
so plug so like the headphone output in the back of the computer?
well line out.
line out yeah.
line out into the line in on a tape player.
uhhuh.
huh.
yep.
and
oh well that sounds pretty easy.
so i mean that's the easy way.
they might have a more complex set up at i b m.
i've seen digital digital analog tapes where it's controlled by the computer.
yeah but the easy way is
huh.
but but we don't need to do that.
yeah i know.
no.
yeah.
the the disadvantage of doing it this way is it's real time.
so you would have to sit there for an hour while it's recording.
yeah but if it was at your desk or something
but
yeah but you
yeah yeah.
right it's not a big deal.
so what's the turn around period right now from i b m?
well they there was
should we be discussing this
okay.
so th they um they gave they've given us
since we started doing this new uh beep format with beep number beep
uhhuh.
they've given us uh i think two full uh transcripts.
they have a third one.
and brian just sent me a note saying oh that third one somehow slipped through the cracks of their of the uh transcriptionists' uh company.
huh.
and so there's going to be delay of a week before we get that third one back.
and i've already given him three more to work on.
okay.
but he said those new three won't get done until this big chunk of data has been processed by the company.
okay.
and the transcribers here are still uh are working though.
yeah yeah they're working away.
they're just doing their own thing like while they're waiting.
they're just doing
right right.
okay.
so what we do is uh when i go to um select meetings for i b m i go into the um
well into the the uh master sheet here with the statuses
master.
uhhuh.
and i pick ones that thilo has um pre segmented.
and i change the status to i p i b m.
so that you know when jane or whoever goes to select the next meeting they won't choose one of those.
and um
so i'm not picking huge chunks right now of meetings for them.
so
we should at least ask um brian if there are any particular requirements for putting things on tape.
uhhuh.
but
is there a particular type of tape that they want to use or anything else like that?
uhhuh that's a good idea.
and uh
and then we should see how much of a pain it is for one of us.
i mean the the other problem is you would want to do it on an unloaded machine.
right?
because you wouldn't want it to start stuttering.
yep.
yeah.
yeah.
yeah.
huh.
and that means that that's more of a problem.
and all
because then you have to be there.
yeah.
and all of the p cs are connected to the network too here?
what
or uh
right.
you're just talking about playing it and recording it like real time?
bad
yep.
oh.
so when that thing when it stutters like that is it because there's a delay getting the data off a disk or is it because there's a delay in the d to a?
uhhuh.
it could be either.
i mean there are lots of different places.
okay.
it can be delay on the disk.
or it can be just load gets high on the machine.
yeah.
um either i o load or processor load.
i see.
you know it's a problem with these multi user systems is that lots of processes are running.
and you don't really have control.
maybe we could do it from a a p c or something.
yeah.
but they are also on the networks.
yeah.
so
if you used a line out too you have to worry about if you click around and you hit a beep or something it'll beep onto onto the tape too.
maybe.
that's true.
it'll
that's right so you really do want to be using a machine that's not.
uhhuh.
uhhuh.
and you have to record every channel separately.
no because we're doing the beep files.
the beep files.
well you know i i think david might have a bunch of old p cs.
oh okay.
because he's replacing them with these new ones.
maybe we could get one.
and just dedicate it to doing this.
not even hook it
well i guess we need to hook it to the network.
we
so we can get the files to it.
but
yep.
uh
or you could just you could have it no active connection.
just s c p them.
yeah right.
and then just use that machine to hook up to the tape.
actually it wouldn't be bad to have that plus put on that machine a c d burner.
yep.
yeah.
we don't need a powerful machine to do these things.
right?
no.
so one of these old pentiums that we have
huh!
should talk to him about that.
or even a d v d burner.
a d v d burner yeah.
uhhuh.
i guess the real question is where would we put it and who would do it?
i guess whoever records the meeting.
i mean it's not a big deal.
but it's yet another thing you have to do.
yeah yeah.
that's why i wanted to avoid having to do that if we could.
but
it's just another
they couldn't use c ds?
no because the transcription company does what they do.
they have a um a device that you put the cassette in.
and it has a foot pedal that lets you go back and forth and stuff like that.
oh i see.
i see.
so
i guess they haven't got a
actually there are a few i saw a web based one at one point.
web based?
where you could send them audio files over the web.
and they send you back text files over the web.
really?
i should look that up again.
can you send them huge audio files?
you can.
but they never will be able to read them.
yeah.
i assume that you can send them anything that they can access with h t t p.
huh!
well they must be dealing with large amounts of data if they're transcribing anyway.
that's interesting.
yeah.
i mean people probably aren't sending them you know an utterance or two.
well that's true.
are how big are these files that we send to brian?
uh pfff!
half a gig.
really?
that big?
oh no wait.
i thought it was like a hundred megs.
they're the beep files.
yeah it's a hundred meg.
they're the beep files.
that's right.
so they're smaller.
yeah.
huh!
that's still pretty dang big.
yes.
send it via email.
yeah.
and we are sending them the compressed ones.
right?
i don't know.
whatever you generate
yeah.
yeah the last ones you did were compressed.
were shortened.
yeah.
yeah.
wonder if the ones before that were shortened.
yeah.
the beep files actually the shorten doesn't save a whole lot.
right?
because it's all
because the whole point is that it's extracting parts that are loud anyway.
it's all speech.
yeah yeah.
uhhuh.
huh!
so
huh.
i think it only saves like half.
only.
okay so i think we just got to let things slide for now.
and see see what's going to happen.
yeah.
nothing else really to do.
um
we have new equipment that i'm going to try to set up tomorrow.
new wireless.
what will that do?
and so uh
is that going to
give us a couple more wireless channels.
oh so we can get rid of the uh
yep.
oh that'll be nice.
yep.
but it involves rewiring.
and so if i could borrow someone tomorrow afternoon to help my brain
i should be around.
you know just have someone to bounce the instructions off of.
and make sure i don't do anything too stupid.
yeah sure.
did i have another topic on the on the list?
experiments?
isn't that next week?
experiments?
that's
yeah i guess that's probably next
yeah.
these are next week.
what did i have on the agenda i mailed out?
yeah.
i think it was
oh yeah i don't know.
the demo.
something about
the demo was one of them.
demo and i b m i think.
demo.
yeah that's it.
yeah.
yeah i think that's it.
yeah.
that was it.
yeah.
short meeting?
yeah.
short meeting.
does anyone have anything else?
oh i made a
oh but that's not specific to meeting recorder.
i was going to say i took some a suggestion of adam's.
and i um created a c g i script that lets you see the current status of the speech disks.
uhhuh.
so if you go to the speech local web page and then there's a disk status page.
you can click on that.
and it'll query abbott.
and give you a a list of all the disks that we have.
and then uh what's on each one.
and then it shows how big the disk is.
and what percent full it is.
cool.
so
and that's you know each time you go there it's updated.
so
so the new disks are installed now?
so what what
or
the new disks are installed.
if you need space just let me know.
i have to create the appropriate sub directory.
so we're kind of trying to keep even the scratch ones we're keeping a little organized where we put a a u doctor speech data.
uhhuh.
and then i'll either create a sub directory for like if it's meeting recorder project or hub five or whatever.
so we
i just have to create those sub directories.
okay.
do we have new d d disks also?
and
no.
darn.
no.
nnn.
so it's all just scratch.
it's all scratch.
okay.
yeah.
how are we doing on
uh d e which is where we're putting all the meeting stuff
no d e is full.
we must be doing it on d f.
no it's d e.
d e i think.
d e's not full.
yeah?
d d is full.
d d's full.
that was the first one.
or that was the last one that filled up.
and d e still had like
okay i guess i'm just one off.
last time i looked it was like seven gigs or something.
okay.
so
and each meeting is roughly half a gig.
half.
and so
so that means we're getting pretty close.
we're getting close.
yeah.
so
shoot!
are we just going to keep recording?
yeah.
like at some point
morgan said something about stopping at the end of the year.
okay.
like when we get around a hundred meetings.
okay.
um because there's going to be data coming from u w hopefully by then.
because
i just hate not to get data.
how
yeah it feels funny.
huh?
you're a data fiend.
if we have the capability.
yeah.
yeah it's like you build this room up for so long.
and then it's just like you stop.
now that we're used to recording it's like uh well why not keep going?
i the other thing we could do is stop doing the regular ones.
and try to convince other topics to come in and do some.
because i think that would be nice.
uhhuh.
we have right now like seventy five or seventy six hours of meetings.
yeah the other thing we were talking about also is once we once the disk has been backed up we don't actually need it online.
so another option is to copy it to tape manually.
so we have the back up copy and the tape copy.
the archive copy and the back up copy.
and then just take them off line.
but aren't we still using the uh compressed
once we have the expanded versions.
so if we have a ton of scratch disk maybe the thing to do is leave those up.
oh.
yeah.
and then use and then not necessarily have the original data online.
yeah we need to figure something out.
because the abbott is basically
full.
we can't add anymore disks to it.
we can increase the size of the disks that are on it.
but that will only take us so far.
so um
david had the suggestion about you know new servers and things like that.
and so i'm going to see if we can talk to morgan about getting some money for a new disk server.
and uh
well that would be pretty tight in the machine room.
yeah it would have to replace abbott basically.
eesh!
yeah.
well he david's planning to get new servers anyways for all of the main servers.
uhhuh.
i think he wants to sort of go with the same type of machine for all of these.
but um
yeah it's full in there.
have you seen it recently with all of the new machines that we got
uhhuh.
uh the sun blade one hundreds.
they're like stacked up on benches and things all over.
it's really crowded.
so
but they are not yet accessible?
the new machines.
oh yeah yeah yeah.
yeah they are?
they're in the yeah if you do a p make
oop!
what are they called?
what are they named?
yeah.
oh i've forgotten now.
um
i assume they're in the p make pool.
yeah they're in the p make pool.
so if you do a reginfo minus show attribute and
uh
a what?
a
p make stuff.
p make magic.
yeah
or customs magic.
oh god!
you can get a list of all the machines that are available to p make by using the command reginfo.
okay.
reginfo okay.
minus show a t t r.
maybe you can send me that.
yeah.
yeah.
and then um
if you do man customs it has almost all of that.
okay.
yeah.
and i i'm not sure what attributes we attach to the sun blade one hundreds.
but it could be s.b. one hundred.
and so if you query for all machines that have that attribute then you should see the names of the new machines.
okay.
okay.
but
do we have an attribute for non interactive machines?
because i was thinking that would be a good one to have.
non interactive
ones that aren't on people's desks.
well there's sort of.
there's this one called there's the attribute called what's that called uh uh no evict.
which is roughly that.
right.
it's although it's like my machine has it on my desk.
so i do get jobs running on my machine all the time.
um but uh
so so that's that's sort of what it is.
yep.
uhhuh.
anyway.
shall we do digits?
sure.
so we're on.
and somewhere is my agenda.
i think the most important thing is morgan wanted to talk about uh the arpa demo.
well so here's the thing.
um why don't we again start off with with
uh uh
yeah i'll get it.
i'll get the door.
um i think we want to start off with the agenda.
and then given that uh liz and andreas are going to be ten fifteen minutes late we can try to figure out what we can do most effectively without them here.
so so so one thing is yeah talk about demo.
okay.
so uh.
uh i b m transcription status.
i b m transcription.
uh what else?
smartkom.
smartkom.
smartkom.
what's smartkom?
uh we want to talk about if if we want to add the data to the meeting recorder corpus.
the data.
the data which we are collecting here.
what what what are we collecting here?
data.
so why don't we have that on the agenda and we'll we'll get to it and talk about it?
the smartkom data.
yeah right.
yeah.
uh right.
uh.
uh reorganization status.
reorganization status.
oh.
files and directories?
files and directories.
yep.
uhhuh.
absinthe which is the multiprocessor unix linux.
i think it was andreas wanted to talk about segmentation and recognition.
and update on s r i recognition experiments.
um
and then if if there's time i wanted to talk about digits.
but it looked like we were pretty full so i can wait till next week.
right.
okay.
well let's see.
i think the certainly the segmentation and recognition we want to maybe focus on when andreas is here since that was particularly his thing.
and also the smartkom thing should
smartkom also andreas.
absinthe i think also he has sort of been involved in a lot of those things.
at least
yeah.
yeah.
he'll he'll probably be interested.
but
yeah.
um
so i mean i think they'll be i'll be interested in all this.
but but uh probably if we had to pick something that we would talk on for ten minutes or so while they're coming here or i guess it would be you think reorganization status?
or
yeah.
i mean i think chuck was the one who added out the agenda item.
i don't really have anything to say other than that we still haven't done it.
so
well i mean i uh just basically that
maybe i said maybe we said this before just that we met and we talked about it.
and we sort of have a plan for getting things organized.
and
and i and i think a crucial part of that is the idea of of not wanting to do it until right before the next level zero back up so that there won't be huge number of of added.
right.
uh
right.
that that was basically it.
not not much.
although dave basically said that if we want to do it just tell him and he'll do a level zero then.
yeah.
uhhuh.
oh excellent.
so
oh so maybe we should just go ahead and get everything ready.
oh good.
and
yep.
so i think we do need to talk a little bit about
well we don't need to do it during this meeting.
we have a little more to discuss.
yeah.
but uh we're we're basically ready to do it.
and uh i have some web pages on more of the background.
so naming conventions and things like that that i've been trying to keep actually up to date.
so
and i've been sharing them with u u w folks also.
i'm sorry you've been what?
okay.
showing them?
sharing them with the u w folks.
sharing them.
okay.
okay.
okay.
well maybe uh since that that was a pretty short one maybe we should talk about the i b m transcription status.
someone can fill in liz and andreas later.
okay.
uh
so we uh we did another version of the beeps where we separated each beeps with a spoken digit.
chuck came up here and recorded some himself speaking some digits.
and so it just goes beep one beep and then the phrase.
and then beep two beep and then the phrase.
and that seems pretty good.
um i think they'll have a easier time keeping track of where they are in the file.
and we have done that on the automatic segmentations.
and we did it with the automatic segmentation.
and i don't think we we didn't look at it in detail.
we just sent it to i b m.
we we sort of spot checked it.
i listened to probably uh five or ten minutes of it from the beginning.
yeah.
oh really?
okay.
yeah.
and
i sort of spot checked here and there and it sounded pretty good.
so i think it'll work.
okay.
and uh we'll just have to see what we get back from them.
uh
and the main thing will be if we can align what they give us with what we sent them.
i mean that's the crucial part.
right.
and i think we'll be able to do that at with this new beep format.
yep.
well i think it's also they are much less likely to have errors.
uhhuh.
i mean so the problem last time is that there were errors in the transcripts where they put beeps where there weren't any or and they put in extraneous beeps.
right.
yeah.
and with the numbers there it's much less likely.
yeah one interesting note is uh or problem
i don't know if this was just because of how i play it back i say uh s n d play and then the file.
every once in a while uh like a beep sounds like it's cut into two beeps.
yeah.
into two pieces.
yeah.
yeah and i i don't know if that's an uh artifact of playback.
yep.
uh i don't think it's probably in the original file.
um but uh
i recognize that too.
yeah.
ha.
that's interesting.
i didn't hear that.
yeah.
but with this new format um that hopefully they're not hearing that and if they are it shouldn't throw them.
yep.
so
well maybe we better listen to it again.
make sure.
but i mean certainly the software shouldn't do that.
yeah.
that's what i thought.
so
it's probably just you know huh somehow the audio device gets hung for a second.
uhhuh.
yeah.
some latency or something.
hiccups.
yeah.
or
yeah.
as long as they have one number and they know that there's only one beep maximum that goes with that number.
yeah.
yeah.
the only the only part that might be confusing is when chuck is reading digits.
right.
yep.
right.
so
well you know actually are we having them
seven four eight beep seven beep eight three two.
yeah but are we having them do digits?
yes.
because uh we don't we didn't in order to cut them out we'd have to listen to it.
we we didn't cut those out.
yeah.
they are not transcribed yet.
so
yeah.
yeah.
and we wanted to avoid doing that.
okay.
so we they are transcribing the digits.
okay.
okay.
although we could tell them we could tell them if you hear someone reading a digits string just say bracket digit bracket.
we can we can ignore it when we get it back.
huh.
and don't bother actually computing the writing down the digits.
yeah.
that'd be great.
that'd be what i'm having the transcribers here do because it can be extracted later.
yep.
and then i wanted to talk about but as i said i we may not have time what we should do about digits.
we have a whole pile of digits that haven't been transcribed.
let's talk about it.
because that's that's something that i i know andreas is less interested in than liz is.
okay.
so you know.
do we have anything else to say about transcription?
it's good.
about i b m stuff?
uh brian i i sent bresset sent brian a message about the meeting.
and i haven't heard back yet.
so i hope he got it and hopefully he's
okay.
huh.
maybe he's gone.
i don't know.
he didn't even reply to my message.
so i should probably ping him just to make sure that he got it.
all right.
so we have a whole bunch of digits.
if we want to move on to digits.
actually maybe i one one more related thing in transcription.
so that's the i b m stuff.
we've got that sorted out.
um how're we doing on the on the rest of it?
we're doing well.
i i hire i've hired two extra people already expect to hire two more.
huh.
and um i've prepared um uh a set of five which i'm which i'm calling set two.
which are now being edited by my head transcriber in terms of spelling errors and all that.
she's also checking through and and and monitoring um the transcription of another transcriber.
you know i mean she's going through and doing these kinds of checks.
uhhuh.
and i've moved on now to what i'm calling set three.
i sort of thought if i do it in sets groups of five then i can have like sort of a a parallel processing through through the the current.
uhhuh.
and and you indicated to me that we have a a goal now for the for the um the uh darpa demo of twenty hours.
so i'm going to go up to twenty hours be sure that everything gets processed and released and and that's that's what my goal is.
package of twenty hours right now and then once that's done move on to the next.
yeah uh
so twenty hours.
but i guess the other thing is that um that that's kind of twenty hours a. sap because the longer before the demo we actually have the twenty hours the more time it'll be for people to actually do cool things with it.
uhhuh.
so okay.
good.
i'm i'm hiring people who uh really are
they would like to do it full time several of these people.
uhhuh.
and and i don't think it's possible really to do this full time but that what it shows is motivation to do as many hours as possible.
it'll keep your accuracy up.
yep.
yeah.
yeah.
and they're really excellent.
well that's good.
yeah.
yeah i mean i guess the so the difference if if um if the i b m stuff works out the difference in the job would be that they primarily would be checking through things that were already done by someone else?
got a good core group now.
again.
uhhuh.
is that most of what it
and correcting.
i mean correcting.
correcting.
we'll we'll expect that they'll have to move some time bins and do some corrections.
and i you know i've also uh discovered
so with the new transcriber i'm
um
so
uh let me say that my
uh
so um
at present um the people have been doing these transcriptions a channel at a time.
and that sort of um is useful.
and you know and then once in a while they'll have to refer to the other channels to clear something up.
okay.
well i realize that um we're using the pre segmented version.
and um the pre segmented version is extremely useful.
and wouldn't it be useful also to have the visual representation of those segments?
and so i've uh i uh uh i've trained the new one uh the the newest one to um use the visual from the channel that is going to be transcribed at any given time.
and that's just amazingly helpful.
because what happens then is you scan across the signal and once in a while you'll find a blip that didn't show up in the pre segmentation.
oh right.
i see what you mean.
and that'll be something like it's it's interesting.
a backchannel or
once in a while it's a backchannel.
yep.
sometimes it seems to be um similar to the ones that are being picked up.
uhhuh.
and they're rare events but you can really go through a meeting very quickly.
you just you just you know you you scroll from screen to screen looking for blips.
and i think that we're going to end up with uh better coverage of the backchannels.
yeah.
but at the same time we're benefitting tremendously from the pre segmentation.
because there are huge places where there is just absolutely no activity at all.
uhhuh.
and uh the audio quality is so good.
so they can they can um scroll through that pretty quick?
yeah.
uhhuh.
that's great.
yeah.
so i think that that's going to also uh you know speed the efficiency of this part of the process.
huh.
okay.
uh yeah.
so uh
yeah.
so let's talk about the digits since they're not here yet.
uh so we have a whole bunch of digits that we've read and we have the forms and so on.
um but only a small number of that
well not a small number.
only a subset of that has been transcribed.
and so we need to decide what we want to do.
and uh liz and andreas actually they're not here but they did say at one point that they thought they could do a pretty good job of just doing a forced alignment.
and again i don't think we'll be able to do with that alone because um sometimes people correct themselves and things like that.
but
so i was just wondering what people thought about how automated can we make the process of finding where the people read the digits doing a forced alignment and doing the timing.
well forced alignment would be one thing.
what about just actually doing recognition?
well we we know what they read because we have the forms.
no they make mistakes.
right.
but the point is that we want to get a set of clean digits.
right.
you're talking about as a pre processing step.
right morgan?
um
is that what you're
yeah i'm i'm not quite sure what i'm talking about.
i mean i
i mean uh we're talking about digits now.
and and so um there's a bunch of stuff that hasn't been marked yet.
uh.
and um there's the issue that that they we know what what was said but do we?
i mean so one option
because people make mistakes and stuff.
i was just asking just out of curiosity if if with uh uh the s r i recognizer getting one percent word error.
uh would we would we do better?
so if you do a forced alignment but the but the but the transcription you have is wrong because they actually made mistakes uh or false starts it's it's much less it's much less common than one percent?
but that's pretty uncommon.
um if we could really get one percent on
well i guess yeah i guess if we segmented it we could get one percent on digits.
we should be able to.
right?
yeah.
yeah.
so that's just my question.
but
i'm not saying it should be one way or the other but it's if
well there there're a couple different ways of doing it.
we could use the tools i've already developed and transcribe it.
hire some people or use the transcribers to do it.
we could let i b m transcribe it.
you know they're doing it anyway.
and unless we tell them different they're going to transcribe it.
um or we could try some automated methods.
and my my tendency right now is well if i b m comes back with this meeting and the transcript is good just let them do it.
well
yeah it's you raised a point kind of uh euphemistically.
but i mean maybe it is a serious problem.
what will they do when they go hear beep seven beep seven three five two?
i mean you think they'll we'll get
it's pretty distinct.
the beeps are pre recorded.
yeah.
it'll only be a problem for for mine.
yeah.
well it it well it'd be preceded by i'm reading transcript so-and-so?
yeah.
yes.
so i think if they're processing it at
i mean it'll be it will be in the midst of a digit string.
so
yeah.
i mean it
sure there there might be a place where it's beep seven beep eight beep eight beep.
but you know they they're they're going to macros for inserting the beep marks.
and so i i don't think it'll be a problem.
we'll have to see.
but i don't think it's going to be a problem.
okay.
well i i i don't know.
i i think that that's if they are in fact going to transcribe these things uh certainly any process that we'd have to correct them or whatever is needs to be much less elaborate for digits than for other stuff.
right.
so why not?
sure.
that was it?
that was it.
just what do we do with digits.
okay.
we have so many of them and it'd be nice to actually do something with them.
well we we we want to have them.
you mean there're more than ten?
yeah i
anything else?
your mike is a little low there.
in berkeley yeah.
so uh you you have to go a little early.
right?
well i can stay till about uh three forty.
at twenty
all right.
so let's make sure we do the ones that that uh saved you.
yeah.
uhhuh.
so there was some uh in in adam's agenda list.
he had something from you about segmentation this last recognition?
well yeah.
so this is just partly to inform everybody um and and of course to get um input.
oops!
um so uh we had a discussion don and liz and i had discussion last week about how to proceed with uh you know with don's work.
and and and uh one of the obvious things that occur to us was that we're since we now have thilo's segmenter and it works you know amazingly well um we should actually basically re evaluate the recognition um results using you know without cheating on the segmentations.
and that should be fairly
so
and how do we find the transcripts for those so that
yeah.
oh okay.
the references for for those segments?
so there's actually
it's not that
why do you ask?
i could
no actually um nist has um a fairly sophisticated scoring program that you can give a um a time.
well
hand ones.
okay.
uh you know you basically just give two time marked sequences of words and it computes the um the uh you know the the
it does all the work for you.
it does all the work for you.
yeah.
okay.
so it we just and we use that actually in hub five to do the scoring.
um so what we've been using so far was sort of a simplified version of the scoring.
and we can we can handle the the the type of problem we have here.
so we
so basically you give some time constraints for for the references and for for the hypothesis.
yeah.
right.
right.
and
yeah.
maybe the start of your speech and the end of it.
yeah okay.
so
or stuff like that.
okay.
right.
it does time constrained word alignment.
okay.
so
so that should be possible.
i mean that shouldn't be a problem.
uh so that was the one thing.
and the other was that
um
what was the other problem?
oh!
that thilo wanted to use the recognizer alignments to train up his um speech detector.
yeah.
um so that we could use uh you know there wouldn't be so much hand labelling needed to uh to generate training data for for the speech detector.
yeah.
i'm just in progress of of doing that.
so
and i think you're in the process of doing that.
so you can you can
yeah.
it'll give you a lot more data too.
won't it?
yeah.
so it's basically
i think eight meetings or something which which i'm using.
and it's before it was twenty minutes of one meeting.
uhhuh.
so should be a little bit better.
right.
that won't be perfect the alignments aren't perfect.
great.
yeah.
but um it's probably still better to have all this extra data than
but
yeah.
yeah.
yep.
yeah.
we'll see that.
yeah.
okay.
actually i had a question about that.
if you find that you can lower the false alarms that you get where there's no speech that would be useful for us to know.
so um
there were the false alarms.
yeah.
so right now you get you know false false uh speech regions when it's just like um breath or something like that.
okay.
yeah.
and i'd be interested to know the if you retrain
yep.
um
yeah.
do those actually go down or not?
because of
yeah.
i'll can make can like make a comparison of of the old system to the to the new one.
yeah just to see if by doing nothing in the modeling of just having that training data what happens.
and then
yeah.
yeah.
yep.
um another one that we had on adam's agenda that definitely involved you was something about smartkom?
right.
so rob porzel
uh porzel?
and the uh porzel and the uh smartkom group are collecting some dialogues.
porzel.
porzel.
basically they have one person sitting in here looking at a picture and a wizard sitting in another room somewhere.
and uh they're doing a travel task.
and uh it involves starting i believe starting with a
it's it's always the wizard.
but it starts where the wizard is pretending to be a computer and it goes through a uh speech generation system.
yeah.
actually it's changed to a synthesis for for the first part now.
synthesis system.
yeah.
um and then it goes to a real wizard and they're evaluating that.
and they wanted to use this equipment and so the question came up is
well here's some more data.
should this be part of the corpus or not?
and my attitude was yes because there might be people who are using this corpus for acoustics as opposed to just for language.
um or also for dialogue of various sorts.
um so it's not a meeting.
right?
because it's two people and they're not face to face.
wait a minute.
yeah.
so i just wanted to understand it because i i'm uh hadn't quite followed this process.
um so it's wizard in the usual sense that the person who is asking the questions doesn't know that it's uh a not a machine?
right.
actually actually the the we do this
at the beginning.
i don't know who came up with it but i think it's a really clever idea.
we simulate a computer breakdown halfway through the session and so then after that the person's told that they're now talking to a uh to a human.
yeah.
it's a human operator.
yeah.
yeah.
but of course they don't know that it's the same person both times.
so we we collect we collect both human computer and human human data essentially in the same session.
you might want to try collecting it the other way around sometime saying that the computer isn't up yet.
huh.
and then so then you can separate it out whether it's the beginning or end kind of effects.
that's an idea.
but
yep.
yeah.
yeah.
that's a good idea.
i have to go now.
you can talk to the computer.
it's a lot more believable too.
no!
if you tell them that they're the computer part is running on a windows machine.
and the whole breakdown thing kind of makes sense.
just just reboot it.
abort
abort retry fail?
so did they actually save the far field data?
because at first they weren't they weren't
well this was this was the question.
yes.
yeah.
so so they were saying they were not going to.
yeah.
okay.
and i said well that's silly if if we're going to try to do it for a corpus there might be people who are interested in acoustics.
yeah.
wow!
no.
or
projector we were not saying we are not doing it.
yeah.
no the the question is do we save one or two far field channels or all of them?
we we just wanted to do
right.
yeah.
yeah.
i i see no reason not to do all of them.
that that if we have someone who is doing acoustic studies uh it's nice to have the same for every recording.
um
nnn.
huh.
yeah.
so what is the purpose of this recording?
uhhuh.
this is to get acoustic and language model training data for smartkom.
it's to be to training data and development data for the smartkom system.
the english system?
yeah.
yeah.
right.
right.
okay.
where does this
maybe we can have him vary the microphones too.
well
or they're different speakers.
right.
so so so for their usage they don't need anything.
so why not
yeah.
but but i'm not sure about the legal aspect of of that.
right?
is is there some contract with smartkom or something about the data?
yeah.
what they
or is is that our data which we are collecting here?
we've never signed anything that said that we couldn't use anything that we did.
or
okay.
okay.
we weren't supposed to collect any data.
so
okay.
this was all
yeah.
so yeah that was the question.
if if
yeah.
yeah.
basically.
no that's not a problem.
i look it seems to me that if we're doing it anyway and we're doing it for these these purposes that we have and we have these distant mikes we definitely should should save it all as long as we've got disk space.
uhhuh.
and disk is pretty cheap.
okay.
so should we save it?
and then
now
yeah.
so we save it because it's it it's potentially useful.
right.
and now what do we do with it is is a separate question.
i mean anybody who's training something up could choose to put it uh to include this or not.
right.
i i would not say it was part of the meetings corpus.
it isn't.
but it's some other data we have.
and if somebody doing experiment wants to train up including that then they can.
uhhuh.
right?
so it's it it i guess it the begs the question of what is the meeting corpus.
so if at u w they start recording two person hallway conversations is that part of the meeting corpus?
i think it's i i think i think the idea of two or more people conversing with one another is key.
well this has two or more people conversing with each other.
nnn well
yeah.
they're just not face to face.
what if we just give it a a name like we give these meetings a name?
well this
no it doesn't.
right?
it has
i mean that was my intention.
and then later on some people will consider it a meeting and some people won't.
yeah.
that was my intention.
well this
and
so so so part of the reason that i wanted to bring this up is do we want to handle it as a special case or do we want to fold it in?
just give it a title.
oh.
i think it is a
give everyone who's involved as their own user i d give it session i ds let all the tools that handle meeting recorder handle it or do we want to special case it?
and if we were going to special case it who's going to do that?
well it it makes sense to handle it with the same infrastructure since we don't want to duplicate things unnecessarily.
so
it it it
i think
but as far as distributing it we shouldn't label it as part of this meeting corpus.
we should let it be its own
yeah.
i don't see why not.
well it's it well because
it's just a different topic.
i i have an extra point which is the naturalness issue.
because we have like meetings that have a reason.
that's one of the reasons that we were talking about this.
and and those and this sounds like it's more of an experimental setup.
yeah.
it's scenario based it's it's human computer interface it's really pretty different.
it's got a different purpose.
but i i have no problem with somebody folding it in for some experiment they're going to do.
yeah.
but i don't think it it doesn't match anything that we've described about meetings.
uhhuh.
whereas everything that we talked about them doing at at u w and so forth really does.
okay.
they're actually talking
so so what does that mean for how we are going to organize things?
huh.
yeah.
you can you can
again as as i think andreas was saying if you want to use the same tools and the same conventions there's no problem with that.
it's just that it's you know different directory it's called something different it's
you know
it is different.
you can't just fold it in as if it's
i mean digits are different too.
right?
it might also be potentially confusing.
yeah but those are folded in.
and it's just you just mark the transcripts differently.
so so one option is you fold it in.
right.
and just simply in the file you mark somewhere that this is this type of interaction rather than another type of interaction.
yeah i
well i i wouldn't call reading digits meetings.
right?
i mean we we we were doing
well but but i put it under the same directory tree.
you know it's in user doctor speech data m r.
well
can we just have a directory called like other stuff?
and
other.
well or i don't know.
i mean i don't care what directory tree you have it under.
and and just um store it there.
right?
i mean that's just a
okay.
my preference is to have a single procedure so that i don't have to think too much about things.
yes.
i mean
and just have a marking.
yeah.
if we do it any other way that means that we need a separate procedure and someone has to do that.
o you you can use whatever procedure you want that's convenient for you.
all i'm saying is that there's no way that we're going to tell people that reading digits is meetings.
right.
and similarly we're not going to tell them that someone talking to a computer to get travel information is meetings.
those aren't meetings.
but if it makes it easier for you to fold them in the same procedures and have them under the same directory tree knock yourself out.
you know?
there's a couple other questions that i have too.
and and one of them is what about uh consent issues?
and the other one is what about transcription?
transcription is done in munich.
are
okay.
so we don't have to worry about transcribing it?
yeah.
all right.
so we will have to worry about format.
that's a that's another argument to keep it separate.
because it's going to follow the smartkom transcription conventions and not the icsi meeting transcription conventions.
oh okay.
yeah.
uh.
okay.
well i didn't realize that.
good point.
that's that's a
good point.
but i'm sure no one would have a problem with our folding it in for some acoustic modeling or or some things.
um.
do we do we have uh um american born folk uh reading german german uh uh place names and so forth?
yeah.
is that
exactly.
yep.
yeah.
yeah great.
yeah.
they they even have a reading list.
i bet that sounds good.
it's pretty funny.
yeah.
huh?
yeah.
you can do that if you want.
yeah.
okay.
yeah.
i don't know if you want that.
right.
so
huh.
heidelberg.
exactly.
disk might eventually be an issue so we might we we might need to uh get some more disk pretty soon.
do you want to be a subject?
yeah i be pretty good.
we
yeah.
we're about we're about half halfway through our disk right now.
that was one of our concerns.
yeah.
are we only half?
i thought we were more than that.
we're probably a little more than that because we're using up some space that we shouldn't be on.
so once everything gets converted over to the disks we're supposed to be using we'll be probably uh seventy five percent.
well when i was looking for space for thilo i found one disk that had uh i think it was nine gigs and another one had seventeen.
yep.
and everything else was sort of committed.
uh.
were those backed up or non backed up?
those were non backed up.
non back up.
right.
so that's different.
oh you're talking about backed up.
i'm much more concerned about the backed up.
the non backed up.
i haven't looked to see how much of that we have.
yeah.
is cheap.
i mean if we need to we can buy a disk hang it off a uh workstation.
if it's not backed up the sysadmins don't care too much.
yeah.
so i mean pretty much anytime we need a disk we can get it at the rate that we're
you can
i shouldn't be saying this but you can just you know since the back ups are every night you can recycle the backed up diskspace.
yeah.
but that's that's that's risky.
yeah you really shouldn't be saying
huh.
i didn't say that.
huh.
yeah that's right.
i didn't say that.
beep that out.
we had allowed dave to listen to these these uh recordings.
right.
um yeah.
i and there's been this conversation going on about getting another file server.
and and we can do that.
uhhuh.
we'll take the opportunity and get another big raft of of disk i guess.
yeah.
well i think i think there's an argument for having you know you could use our old file server for for disks that have data that is very rarely accessed.
it's really the back up issue rather than the file server issue.
and then have a fast new file server for data that is um heavily accessed.
yeah.
my understanding is the issue isn't really the file server.
yeah.
we could always put more disks on.
yeah.
it's the it's the back up
it's the back up system.
yeah.
so
which is near saturation apparently.
so
soon.
i think i think the file server could become an issue as we get a whole bunch more new compute machines.
and we've got you know fifty machines trying to access data off of abbott at once.
well we're all right for now.
because the network's so slow.
i mean i think i think we've raised this before and someone said this is not a reliable way to do it.
but the
what about putting the stuff on like c c d rom or d v d or something?
yeah.
that was me.
i was the one who said it was not reliable.
okay.
they they wear out.
oh okay.
yeah.
the the
but they wear out just from sitting on the shelf?
yep.
absolutely.
or from being read and read?
no.
read and write don't hurt them too much unless you scratch them.
oh okay.
but the the write once and the read writes don't last.
so you don't you don't want to put reproduceable data on them.
uhhuh.
wear out after what amount of time?
year or two.
would it be
year or two?
yep.
wow!
huh.
but if that
then you would think you'd hear much more clamoring about data loss.
yeah.
and
i mean yeah all the l
i i don't know many people who do it on c d.
i mean they're the most
l d all the l d c distributions are on c d rom.
yeah.
they're on c d but they're not that's not the only source.
like
they have them on disk.
and they burn new ones every once in a while.
but you know we have
but if you go if you go
but we have like thirty you know from ten years ago?
no.
we have all sorts of c d roms from a long time ago.
yeah.
right.
ten years ago.
yeah.
well
okay.
ninety one.
and they're still all fine.
were they burned or were they pressed?
yeah.
uh both.
i've burned them and they're still okay.
yeah.
i mean usually they're
the the pressed ones last
well not forever.
they've been finding even those degrade.
oh i see.
but uh the burned ones
i mean when i say two or three years what i'm saying is that i have had disks which are gone in a year.
that's what i
on the average it'll probably be three or four years.
but uh i i you don't want to have your only copy on a media that fails.
huh.
and they do.
so how about
um if you have them professionally pressed you know they're good for decades.
so so how about putting them on that plus like on a on on dat or some other medium that isn't risky?
i think
um
we can already put them on tape.
okay.
and the tape is is very reliable.
uhhuh.
so the the only issue is then if we need access to them.
right.
so that's fine if we don't need access to them.
well if if if you if they last
say they actually last like five years huh in in the typical case.
and and occasionally you might need to re create one.
and then you get your tape out but otherwise you don't.
can't you just
you just put them on
so you just archive it on the tape and then put it on c d as well?
yeah.
right.
oh.
so you're just saying put them on c ds for normal access.
right.
yeah.
yeah.
i mean you can do that.
what you
but that's pretty annoying.
because the c ds are so slow.
see
yeah.
huh.
yeah.
what'd be nice is a system that re burned the c ds every year.
every time it was a going to going to die.
well i mean the c ds are are an
well
yeah.
it's like like dynamic d ram.
just before.
just before before it goes bad it burns them in.
yeah.
the the c d is an alternative to tape.
yeah.
icsi already has a perfectly good tape system and it's more reliable.
you know i would think
so for archiving we'll just use tape.
one one thing i don't understand is if you have the data if if if the meeting data is put on disk exactly once then it's backed up once and the back up system should never have to bother with it uh more than once.
well regardless
well first of all there was um a problem with the archive in that i was every once in a while doing a chmod on all the directories or recursive chmod and chown.
because they weren't getting set correctly every once in a while.
uhhuh.
and i was just doing a minus r star not realizing that that caused it to be re backed up.
uhhuh.
uh
but normally you're correct.
but even without that the back up system is becoming saturated.
but but this back up system is smart enough to figure out that something hasn't changed and doesn't need to be backed up again.
sure but we still have enough changed that the nightly back ups are starting to take too long.
the i think the at least the once that you put it on it would it would kill that.
okay.
so so then if so so then let's
it has nothing to do with the meeting.
so
it's just the general icsi back up system is becoming saturated.
right.
okay.
right.
so what if we buy uh uh what what do they call these um high density
well why don't you have this have a this conversation with dave johnson rather than with me?
no no.
because this is maybe something that we can do without involving dave and and putting more burden on him.
how about we buy uh uh uh one of these high density tape drives?
and we put the data actually on non backed up disks.
and we do our own back up once and for all all and then and we don't have to bother this up.
actually you know we could do that just with the tape with the current tape.
i don't know what these tapes.
uh
at some point these
i don't know.
what kind of tape drive is it?
is it is
i don't know but it's an automatic robot so it's very convenient.
right.
you just run a program to restore them.
the the one that we have?
yeah.
but it might interfere with their back up schedule.
the i mean
but
no we have we don't we have our own?
uh.
something that doesn't that isn't used by the back up gang?
don't we have something downstairs?
well they
what kind of tape drive?
just in
well
yeah.
but
no.
but andreas's point is a good one.
and we don't have to do anything ourselves to do that.
right.
they're already right now on tape.
right?
so your your point is and i think it's a good one that we could just get more disk and put it there.
huh.
on an x h uh x x whatever partition.
yeah.
that's not a bad idea.
yeah.
yeah that's basically what i was going to say is that a disk is is so cheap it's essentially you know close to free.
so once it's on tape
right.
and the only thing that costs is the back up issue uh to first order.
right.
and we can take care of that by putting it on non back up drives and just backing it up once onto this tape.
uhhuh.
i think that's a good idea.
right.
oh.
okay.
yeah.
good.
it's good.
so who's going to do these back ups?
the people that collect it?
uh
well i'll talk to dave and and see what how what the best way of doing that is.
it's probably going to
there's a little utility that will manually burn a tape for you.
and that's probably the right way to do it.
yeah and we should probably make that part of the procedure for recording the meetings.
well
yep.
yeah.
that's what i'm wondering.
if
well we're we're going to automate that.
okay.
my intention is to do a script that'll do everything.
i mean you don't have to physically put a tape in the drive?
no.
or
oh okay.
it's all tape robot.
so it's just
so you just sit down at your computer and you type a command.
oh okay.
yeah but then you're effectively using the resources of the back up system.
or is that a different tape robot?
but not at the same time.
yeah.
but but you would be anyway.
no no.
right?
no no no.
because
see
he's saying get a whole different drive.
yeah just give a
but there's no reason to do that.
well i'm saying is if you go to dave and and and ask him can i use your tape robot.
it we already have it there and it it's
he will say well that's going to screw up our back up operation.
no we won't.
he'll say if if that means that it's not going to be backed up standardly great.
i dave has has promoted this in the past.
so i don't think he's actually against it.
yeah.
it's it's definitely no problem.
oh okay.
all right.
all right.
yeah.
good.
okay.
what about if the times overlap with the normal back up time?
um it's it's just it's just a utility which queues up.
okay.
it just queues it up and and when it's available it will copy it.
yeah.
and then you can tell it to then remove it from the disk or you can you know do it a few days later or whatever you want to do.
after you confirm that it's really backed up.
okay.
n w
you saying n w archive?
n w archive.
that's what it is.
yep and if you did that during the day it would never make it to the nightly back ups.
okay.
right.
and then there wouldn't be this extra load.
well it if he you have to put the data on a on a non backed up disk to begin with.
so that so that otherwise you don't you
right.
well but you can have it n w archive to you can have uh a non backed up disk n w archived.
right.
and it'll never show up on the nightly back ups.
and then it never
right.
right.
right.
which i'm sure would make the sysadmins very happy.
right.
yeah.
so i think that's a good idea.
okay.
that's what we should do.
okay.
so that means we'll probably want to convert all all those files filesystems to non backed up media.
that sounds good.
yep.
yeah.
um another thing on the agenda said s r i recognition experiments?
what's that?
s r i recognition?
oh.
that wasn't me.
um well
uh.
who's that?
we have lots of them.
uh i don't know.
chuck do you have any any updates?
i'm successfully uh increasing the error rate.
uh
that's good.
oh.
huh.
lift the herve approach.
yeah.
so i mean i'm just playing with um the number of gaussians that we use in the the recognizer and
well you have to you have to tell people that you're you're doing you're trying the tandem features.
yes i'm using tandem features.
oh you are?
and i'm still tinkering with the p l p features.
cool.
and
yeah i got confused by the results.
it because uh the meeting before you said okay we got it down to where they're they're within a tenth of a percent.
that was on males.
right.
that was that was before i tried it on the females.
oh.
see women are are trouble.
it's the women are the problem.
okay.
right?
as we all know.
so
well let's just say that men are simple.
so so when so i i had i
that was a quick response.
so we had reached the point where
i'm well rehearsed.
yeah.
we had reached the point where um on the male portion of the development set the um or one of the development sets i should say the um the male error rate with uh icsi p l p features was pretty much identical with uh s r i features.
which are m f c c.
so um then i thought oh great.
i'll i'll just let's make sure everything works on the females.
and the error rate you know there was a three percent difference.
oh.
uhhuh.
so
is there less training data?
uh
i mean we
no actually there's more training data.
this is on just digits?
no no.
oh sorry.
no.
no.
it's uh
okay.
hub five.
this is on
this is hub five.
oh okay.
hub five.
yeah.
yeah.
um and the test data is callhome and switchboard.
so uh so then um
oh and plus the the vocal tract length normalization didn't actually made things worse.
so something's really seriously wrong.
so
um
aha.
so so
okay.
so but you see now between between the males and the females there's certainly a much bigger difference in the scaling range than there is say just within the males.
and what you were using before was scaling factors that were just from the the the s r i front end.
and that worked that worked fine.
that's true.
yeah.
uh but now you're looking over a larger range and it may not be so fine.
well um
so
i just
so the one thing that i then tried was to put in the low pass filter which we have in the
so most most hub five systems actually band limit the uh at about uh thirty seven hundred um hertz.
uhhuh.
although you know normally i mean the channel goes to four four thousand.
right?
so um
and that actually helped uh uh a little bit.
uhhuh.
um and it didn't hurt on the males either.
so um
and i'm now uh trying the
oh and suddenly also the the vocal tract length normalization only in the test on the test data.
so you can do vocal tract length normalization on the test data only or on both the training and the test.
yeah.
and you expect it to help a little bit if you do it only on the test and more if you do it on both training and test.
yeah.
and so the it now helps if you do it only on the test.
and i'm currently retraining another set of models where it's both in the training and the test.
and then we'll we'll have hopefully even better results.
so
but there's
it looks like there will still be some difference.
maybe between one and two percent um for the females.
huh.
and so um you know i'm open to suggestions.
and it is true that the uh that the you know we are using the
uhhuh.
but
it can't be just the v t l.
uhhuh.
because if you don't do v t l in both systems uh you know the the females are considerably worse in the with the p l p features.
no no.
i i remember that.
it's much worse.
yeah.
so there must be some something else going on.
well what's the standard?
yeah so i thought the performance was actually a little better on females than males.
that's what i thought too.
um that overall yes.
but on this particular development test set they're actually a little worse.
but that's beside the point.
we're looking at the discrepancy between the s r i system and the s r i system when trained with icsi features.
right.
i'm just wondering if that if if you have any indication of your standard features.
what's are the
you know if that's also different or in the same direction or not.
you're this is let me ask a more basic
because
uhhuh.
i mean is this uh uh iterative baum welch training?
or is it viterbi training?
or
it's baum welch training.
baum welch training.
and how do you determine when to to stop iterating?
um
well actually we we just basically do a a fixed number of iterations.
huh.
uh in this case four.
um which uh we used to do only three.
and then we found out we can squeeze
and it was basically we're we're keeping it on the safe side.
but you're
right.
it might be that one more iteration would would help but it's sort of
you know.
or maybe or maybe you're doing one too many.
i mean it's it's
no but with baum welch there shouldn't be an over fitting issue really.
uh well there can be.
sure.
um
well you can try each one on a cross validation set.
it if you if you remember some years ago bill byrne did a thing where he was he was looking at that.
can't you?
and he showed that you could get it.
yeah.
so
but but but um
well yeah.
we can
well that's that's the easy one to check.
because we save all the intermediate models.
yeah.
do you
and we can
and in each case
what
um i'm sorry.
in each case how do you determine you know the the usual fudge factors?
the uh the uh language uh scaling acoustic scaling.
uh uh
um i i'm actually re optimizing them.
although that hasn't shown to make a big difference.
okay.
and the the question he was asking at one point about pruning.
uh
pruning
remember that one?
pruning in the
well he was he's it looked like the at one point he was looking at the probabilities he was getting out at the likelihoods he was getting out of p l p versus mel cepstrum and they looked pretty different.
yeah the likelihoods were lower for the p l p.
as i recall.
oh.
and so uh there's the question.
you mean did you see this in the s r i system?
uhhuh.
um well the likelihoods are
was just looking through the log files.
and
you can't directly compare them because for every set of models you compute a new normalization.
and so these log probabilities they aren't directly comparable.
oh.
because you have a different normalization constants for each model you train.
so
huh.
but still it's a question.
if you have some threshold somewhere in terms of beam search or something?
well yeah.
or
that's what i was wondering.
yeah.
i mean
uh
i mean if you have one threshold that works well because the range of your likelihoods is in this area
we prune very conservatively.
i mean as we saw with the meeting data um we could probably tighten the pruning without really
so we basically we have a very open beam.
but you're only talking about a percent or two.
yeah.
right?
here we're we're saying that there gee there's this uh there's this difference here.
and it
see because there could be lots of things.
right?
right.
but but but but um let's suppose just for a second that uh we've sort of taken out a lot of the the major differences uh between the two.
course.
uhhuh.
right.
i mean we're already sort of using the mel scale and we're using the same style filter integration.
and and well we're making sure that low and high
actually there is the difference in that.
so for the p l p features we use the triangular filter shapes.
and for the in the s r i front end we use the trapezoidal one.
and what's the top frequency of each?
well now it's the same.
it's thirty thirty to seven hundred and sixty hertz.
yeah.
one's triangular one's trapezoidal.
so
no no.
but
before we with straight p l p it's trapezoidal also.
well
but
but then we had a slight difference in the in the scale.
uh so
since currently the feacalc program doesn't allow me to change the filter shape independently of the scale.
uhhuh.
and i did the experiment on the s r i front end where i tried the where the standard used to be to use trapezoidal filters.
you can actually continuously vary it between the two.
and so i i i tried the uh triangular ones.
and it did slightly worse but it's really a small difference.
huh.
so
couple tenths of a percent or something.
okay.
yeah exactly.
so it's not just losing some frequency range.
right.
so it's not
i don't think the filter shape by itself will make a huge difference.
yeah.
right.
so the the other thing that
yeah.
so
we've always viewed it anyway as the major difference between the two is actually in the smoothing.
uhhuh.
that the that the um p l p and and the reason p l p has been advantageous in uh slightly noisy situations is because p l p does the smoothing at the end by an auto regressive model.
uhhuh.
and mel cepstrum does it by just computing the lower cepstral coefficients.
uhhuh.
um so um uhhuh.
okay.
so one thing i haven't done yet is to actually do all of this with a much larger with our full training set.
so right now we're using a
i don't know.
forty?
it's it's it's a training set that's about um you know by a factor of four smaller than what we use when we train the full system.
so some of these smoothing issues are over fitting for that matter.
and the baum welch should be much less of a factor if you go full whole hog.
uhhuh.
could be.
yeah.
and so so just um so the strategy is to first sort of treat things with fast turnaround on a smaller training set and then when you've sort of narrowed it down you try it on a larger training set.
yeah.
and so we haven't done that yet.
now the other related question though is is uh what's the boot models for these things?
the boot models are trained from scratch.
so we compute
um
so we start with a um alignment that we computed with the sort of the best system we have.
and and then we train from scratch.
so we we do a you know um we collect the uh the observations from those alignments under each of the feature sets that that we train.
and then from there we do
um
there's a lot of
actually the way it works you first train a phonetically tied mixture model.
um
you do a total of
first you do a context independent p t m model.
then you switch to a context
you do two iterations of that.
then you do two iterations of of of context dependent phonetically tied mixtures.
and then from that you you do the you you go to a state clustered model.
yeah.
and you do four iterations of that.
so there's a lot of iterations overall between your original boot models and the final models.
i don't think that
huh.
we have never seen big differences.
once i thought oh i can now i have these much better models.
i'll re generate my initial alignments.
then i'll get much better models at the end.
made no difference whatsoever.
it's i think it's uh
right.
the boot models are
well for making things better.
yeah.
but this for making things worse.
this it
the thought is is is possible another possible partial cause is if the boot models used a used a different feature set that
uhhuh.
uhhuh.
but there are no boot models in fact.
you you're not booting from initial models.
you're booting from initial alignments.
which you got from a different feature set.
that's correct.
so those features look at the data differently actually.
yeah but
i mean you know they they will find boundaries a little differently though.
you know all all that sort of thing is actually slightly different.
i'd expect it to be a minor effect.
but but but what i'm what i'm saying is
but
so we for a long time we had used boot alignments that had been trained with a with the same front end but with acoustic models that were like fifteen percent worse than what we use now.
uhhuh.
and with a different dictionary with a considerably different dictionary which was much less detailed and much less well suited.
uhhuh.
yeah.
and so then we switched to new boot alignments.
which which now had the benefit of all these improvements that we've made over two years in the system.
right.
and the result in the end was no different.
right.
so what i'm saying is the exact nature of these boot alignments is probably not a big factor in the quality of the final models.
yeah maybe not.
but it it i still see it as i mean there's there's a history to this too.
yeah.
but i uh i don't want to go into.
uhhuh.
but but i i i i think it could be the things that it the data is being viewed in a certain way uh that a beginning is here rather than there and so forth.
yeah.
right.
because the actual signal processing you're doing is slightly different.
right.
but it's it's that's probably not it.
yeah.
anyway i i i should really reserve uh any conclusions until we've done it on the large training set um and until we've seen the results with the with the v t l in training.
so
yeah.
at some point you also might want to take the same thing and try it on uh some broadcast news data or something else that actually has has some noisy noisy components so we can see if any conclusions we come to holds across different data.
yeah.
right.
uh
and uh with this i have to leave.
huh!
okay.
so is there something quick about absinthe that you
with this said.
uh just what we were talking about before which is that i ported a blass library to absinthe.
and then got got it working with fast forward.
and got a speedup roughly proportional to the number of processors times the clock cycle.
oh.
so that's pretty good.
oh!
cool.
um i'm in the process of doing it for quicknet.
but there's something going wrong.
and it's about half the speed that i was estimating it should be.
and i'm not sure why.
uhhuh.
but i'll keep working on it.
but the what it means is that it's likely that for net training and forward passes we'll absinthe will be a good machine.
especially if we get a few more processors and upgrade the processors.
a few more processors?
how many are you shooting for?
there're five now.
it can hold eight.
oh okay.
yeah we'll just go buy them i guess.
and it's also five fifty megahertz and you can get a gigahertz.
yeah.
so
can you mix uh processors of different speed?
i don't think so.
i think we'd have to do all
okay.
probably just throw away the old ones.
yep.
and
thank you for the box.
and i'll just go buy their process.
oh okay.
huh!
maybe we can stick them in another system.
i don't know.
we'd have to get a almost certainly have to get a uh netfinity server.
i see.
they're pretty pretty specialized.
yeah.
okay.
okay.
is is liz coming back do you know?
or
i don't know.
yeah.
oh you don't.
okay.
all right.
all right.
see you.
um.
all right.
so
uh they're having tea out there.
so i guess the other thing that we were going to talk about is is uh demo.
and um so these are the demos for the uh july uh meeting and um darpa
july what?
early july?
late july?
oh i think it's july fifteenth.
sixteen to eighteen i think.
is that it?
yeah sixteenth eighteenth.
roughly.
yeah.
so we talked about getting something together for that.
but maybe uh maybe we'll just put that off for now given that
but i think maybe we should have a a sub meeting.
i think uh probably uh adam and and uh chuck and me should talk about should get together and talk about that sometime soon.
over a cappuccino tomorrow?
yeah something like that.
um uh you know maybe maybe we'll involve dan ellis at some some level as well.
uhhuh.
um.
okay the the tea is is going so uh i suggest we do uh uh a unison.
a unison digits?
okay.
yeah.
which is going to be a little hard for a couple people because we have different digits forms.
gets our
oops!
we have a i found a couple of old ones.
huh.
oh.
well that'll be interesting.
so
uh
have you done digits before?
no.
i haven't done it.
okay.
so uh the idea is just to read each line with a short pause between lines.
all right.
not between.
and uh since we're in a hurry we were just going to read everyone all at once.
so if you sort of plug your ears and read
okay.
so first read the transcript number and then start reading the digits.
sure.
okay?
one two three.
okay we're on.
okay.
so i mean everyone who's on the wireless check that they're on?
we
all right.
i see.
yeah.
yeah.
okay our agenda was quite short.
oh could you close the door maybe?
sure.
yeah.
two items which was uh digits and possibly stuff on on uh forced alignment.
which jane said that liz and andreas had information on.
uhhuh.
but they didn't.
so
i guess the only other thing uh for which i
we should do that second.
because liz might join us in time for that.
okay.
um
okay so there's digits alignments.
and um
i guess the other thing which i came unprepared for uh is uh to see if there's anything anybody wants to discuss about the saturday meeting.
right.
so
any i mean maybe not.
digits and alignments.
but
uh
talk about aligning people's schedules.
yeah.
yeah.
uhhuh.
yeah.
i mean
right.
yeah i mean it was
yeah it's forced alignment of people's schedules.
yeah.
if we're very
forced align.
yeah.
yeah.
with with whatever it was a month and a half or something ahead of time the only time we could find in common roughly in common was on a saturday.
yeah.
yep.
ugh.
it's pretty sad.
yeah.
yeah.
have have we thought about having a conference call to include him in more of in more of the meeting?
i i mean i don't know.
if we had the if we had the telephone on the table
no.
but i mean he probably has to go do something.
no actually i i have to i have to shuttle kids from various places to various other places.
right?
i see okay.
so
yeah.
and i don't have and i don't um have a cell phone.
a cell phone?
so i can't be having a conference call while driving.
no it's not good.
right.
that's not good.
so we have to we
plus it would make for interesting noise background noise.
yep.
uh
so we have to equip him with a with a with a head mounted uh cell phone.
and we'd have to force you to read lots and lots of digits.
and
oh yeah.
so it could get real real car noise.
yeah.
oh yeah.
take advantage.
and with the kids in the background.
i'll let i'd let
yeah.
i let uh my five year old have a try at the digits.
uh
yeah.
so anyway i can talk about digits.
um did everyone get the results or shall i go over them again?
i mean that it was basically the only thing that was even slightly surprising was that the lapel did so well.
um and in retrospect that's not as surprising as maybe
it shouldn't have been as surprising as i as as i felt it was.
the lapel mike is a very high quality microphone.
and as morgan pointed out that there are actually some advantages to it in terms of breath noises and clothes rustling if no one else is talking.
exactly.
yeah.
uhhuh.
um so uh
it's it
well it's yeah sort of the the breath noises and the mouth clicks and so forth like that the lapel's going to be better on.
or the cross talk.
yeah.
the lapel is typically worse on the on clothes rustling.
but if no one's rustling their clothes
right i mean a lot of people are just sort of leaning over and reading the digits.
it's it's
so it's it's a very different task than sort of the natural.
yeah.
you don't move much during reading digits i think.
so
yeah.
yeah.
right.
probably the fact that it picks up other people's speakers other people's talking is an indication of that it the fact it is a good microphone.
yeah.
right.
right so in the digits in most most cases there weren't other people talking.
so
right.
do the lapel mikes have any directionality to them?
so
there typically don't no.
because i i suppose you could make some that have sort of that you have to orient towards your mouth.
and then it would
they have a little bit.
but they're not noise cancelling.
so uh
they're they're intended to be omni-directional.
right.
uhhuh.
and it's and because you don't know how people are going to put them on you know.
right.
so also andreas on that one the the back part of it should be right against your head.
and that will keep it from flopping up and down as much.
it is against my head.
okay.
yeah.
um yeah we actually talked about this in the uh front end meeting this morning too.
uhhuh.
much the same thing.
and and it was
uh i mean there the point of interest to the group was primarily that um the uh the system that we had that was based on h.t.k. that's used by you know all the participants in aurora was so much worse than the than the s.r.i.
everybody.
and the interesting thing is that even though yes it's a digits task and that's a relatively small number of words and there's a bunch of digits that you train on it's just not as good as having a a very large amount of data and training up a a a nice good big h.m.m.
um also you had the adaptation in the s.r.i. system which we didn't have in this.
um so um
and we know
did i send you some results without adaptation?
no.
or if you did i didn't include them.
i think stephane uh had seen them.
because it was
so
yeah i think i did actually.
so there was a significant loss from not doing the adaptation.
yeah.
um
a a a couple percent or
i mean
well i don't know it overall
uh i i don't remember.
but there was there was a significant um loss or win from adaptation with with adaptation.
and um
that was the phone loop adaptation.
and then there was a very small like point one percent on the natives uh win from doing um you know adaptation to the recognition hypotheses.
and i tried both means adaptation and means and variances.
and the variances added another or subtracted another point one percent.
so it's um that's the number there.
point six i believe is what you get with both uh means and variance adaptation.
right.
but i think one thing is that uh i would presume
have you ever have you ever tried this exact same recognizer out on the actual t.i. digits test set?
this exact same recognizer?
no.
it might be interesting to do that.
because my my because my sense
um
but but i have i mean people people at s.r.i. are actually working on digits.
i bet it would do even slightly better.
i could and they are using a system that's um you know is actually trained on digits.
um but otherwise uses the same you know decoder the same uh training methods and so forth.
uhhuh.
and i could ask them what they get on t.i. digits.
yeah although i'd be i think it'd be interesting to just take this exact actual system.
so that these numbers were comparable.
uhhuh.
well adam knows how to run it.
and try it out on t.i. digits.
yeah no problem.
so you just make a
yeah.
yeah.
yeah.
because our sense from the other from the aurora uh task is that
and try it with t.i. digits.
uhhuh.
i mean because we were getting sub one percent numbers on t.i. digits also with the tandem thing.
uhhuh.
huh.
so one so there were a number of things we noted from this.
one is yeah the s.r.i. system is a lot better than the h.t.k.
huh.
this you know very limited training h.t.k. system.
uhhuh.
uh but the other is that um the digits recorded here in this room with these close mikes uh are actually a lot harder than the studio recording t.i. digits.
uhhuh.
i think you know one reason for that uh might be that there's still even though it's close talking there still is some noise and some room acoustics.
uhhuh.
and another might be that uh i'd i would presume that in the studio uh uh situation recording read speech that if somebody.
did something a little funny or pronounced something a little funny or made a little that they didn't include it.
they didn't include it?
they made them do it again.
whereas i took out the ones that i noticed that were blatant that were correctable.
huh.
yeah.
so that if someone just read the wrong digit i corrected it.
and then there was another one where jose couldn't tell whether i couldn't tell whether he was saying zero or six.
yeah.
and i asked him and he couldn't tell either.
huh.
so i just cut it out.
you know so i just edited out the first uh word of the utterance.
yeah.
um so there's a little bit of correction but it's definitely not as clean as t.i. digits.
so my expectations is t.i. digits would especially
i think t.i. digits is all american english.
right?
so it would probably do even a little better still.
uhhuh.
on the s.r.i. system.
but we could give it a try.
well
but remember we're using a telephone bandwidth front end here uh on this uh on this s.r.i. system.
so um i was i thought that maybe that's actually a good thing.
because it it gets rid of some of the uh the noises.
um you know in the the below and above the um the you know speech bandwidth.
uhhuh.
uhhuh.
and um
i suspect that to get sort of the last bit out of these higher quality recordings you would have to in fact uh use models that uh were trained on wider band data.
and of course we can't do that or
what's t.i. digits?
i thought
it's wide band yeah.
it's in in fact we looked it up.
it is wide band.
okay.
and it was actually twenty kilohertz sampling.
oh that's right.
i i did look that up.
uhhuh.
i couldn't remember whether that was t.i. digits or one of the other digit tasks.
yeah.
right.
but but i would
yeah.
it's it's easy enough to try.
just run it on
uhhuh.
yeah.
so morgan you're getting a little breath noise.
see
now uh does
one one issue
you might want to move the mike down a little bit.
one issue with with that is that um the system has this uh notion of a speaker to which is used in adaptation variance uh you know both in uh mean and variance normalization.
and also in the v.t.l. estimation.
uhhuh.
so
yeah i noticed the script that extracted it.
do
is
so does so so does does um the t.i. digits database have speakers that are known?
yep.
yep.
and is there is there enough data or a comparable comparable amount of data to to what we have in our recordings here?
that i don't know.
i don't know.
i don't know how many speakers there are.
and and how many speakers per utterance.
okay.
yeah.
well the other thing would be to do it without the adaptation and compare to these numbers without the adaptation.
that would
right uh but i'm not so much worried about the adaptation actually than than the um um the uh v.t.l. estimation.
right.
if you have only one utterance per speaker you might actually screw up on estimating the the warping uh factor.
so um
i strongly suspect that they have more speakers than we do.
right but it's not the amount of speakers.
so uh
it's the it's the amount of data per speaker.
right so we we could probably do an extraction that was roughly equivalent.
right.
right.
um
so
so although i i sort of know how to run it there are a little a few details here and there that i'll have to dig out.
okay.
the key
so the system actually extracts the speaker i.d. from the waveform names.
right i saw that.
and there's a there's a script and that is actually all in one script.
so there's this one script that parses waveform names.
and extracts things like the um speaker uh i.d.
or something that can stand in as a speaker i.d.
so we might have to modify that script to recognize the um speakers um in the in the uh um t.i. digits database.
right.
right.
and that uh
or you can fake you can fake names for these waveforms that resemble the names that we use here for the for the meetings.
right.
that would be the sort of probably the safest way to do
i might have to do that anyway to to do
because we may have to do an extract to get the amount of data per speaker about right.
uhhuh.
the other thing is isn't t.i. digits isolated digits?
right.
or is that another one?
i'm i looked through a bunch of the digits corpora.
and now they're all blurring.
uhhuh.
because one of them was literally people reading a single digit.
and then others were connected digits.
yeah most of t.i. digits is connected digits i think.
okay.
the i mean we had a bellcore corpus that we were using.
maybe it's the bell gram.
it was that's that was isolated digits.
bell digits.
all right.
by the way i think we can improve these numbers if we care to improve them by um not starting with the switchboard models but by taking the switchboard models and doing supervised adaptation on a small amount of digit data collected in this setting.
um
yep.
because that would adapt your models to the room acoustics.
and for the far field microphones you know to the noise.
and that should really improve things um further.
and then you use those adapted models which are not speaker adapted but sort of you know channel adapted.
channel adapted.
use that as the starting models for your speaker adaptation.
yeah but the thing is uh i mean when you it depends whether you're were just using this as a a starter task for you know to get things going for conversational or if we're really interested in connected digits.
and i i think the answer is both.
well i don't know.
and for for connected digits over the telephone you don't actually want to put a whole lot of effort into adaptation.
because somebody gets on the phone and says a number.
and then you just want it.
you don't don't uh
this is this that one's better.
right.
uhhuh.
um but you know i uh my impression was that you were actually interested in the far field microphone uh problem.
i mean
so
you want to you want to that's the obvious thing to try.
oh.
oh.
right?
then uh because you you don't have any
right.
yeah.
that's where the most acoustic mismatch is between the currently used models and the the the set up here.
right.
so
yeah so that'd be another interesting data point.
uhhuh.
i mean i i guess i'm saying i don't know if we'd want to do that as the as
other way.
other way.
liz
now you're all watching me.
it it clips over your ears.
all right.
this way.
there you go.
if you have a strong if you have a strong preference you could use this.
you're all watching.
this is terrible.
it's just we we think it has some spikes.
so uh we we didn't use that one.
i'll get it.
but you could if you want.
yeah at any rate i don't know if
i don't know.
and andreas your your microphone's a little bit low.
it is?
yeah.
yeah.
i don't know if we want to use that as the
uh
uh it pivots.
so if you see the picture
i
it it like this.
and then you have to
i i already adjusted this a number of times.
uh
i i
yeah i think these mikes are not working as well as i would like.
can't quite seem to
yeah i think this contraption around your head is not working so well.
too many too many adjustments yeah.
anyway what i was saying is that i i think i probably wouldn't want to see that as sort of like the norm that we compared all things to.
that looks good.
yeah.
to uh the to have have all this all this uh adaptation.
but i think it's an important data point if you're if
right.
yeah.
um
the other thing that that uh of course what barry was looking at was was just that.
the near versus far.
and yeah the adaptation would get some of that.
uhhuh.
but i think even even if there was uh only a factor of two or something like i was saying in the email i think that's that's a big factor.
uhhuh.
so
liz you could also just use the other mike if you're having problems with that one.
well.
okay.
yeah this would be okay.
we we we think that this has spikes on it.
it's this thing's this is too big for my head.
so it's not as good acoustically.
yeah basically your ears are too big.
but
i mean mine are too.
everybody's ears are too big for these things.
no my my but this is too big for my head.
uh
so i mean it doesn't you know it's
well if you'd rather have this one then it's
okay.
yeah.
oh well.
it's great.
so the to get that uh pivoted this way it pivots like this.
no this way.
yeah.
yeah.
there you go.
and there's a screw that you can tighten.
and then it
right.
right.
i already tried to get it close.
good.
so if it doesn't bounce around too much that's actually good placement.
that looks good.
okay.
but it looks like it's going to bounce a lot.
so where were we?
uh yeah.
yeah.
digits.
adaptation.
uh adaptation non adaptation.
um factor of two.
um
what by the way what factor of two did you
oh yeah i know what i was.
i mean
oh no no.
it's that that we were saying you know well is how much worse is far than near you know.
oh okay.
and i mean it depends on which one you're looking at.
that factor of two.
uhhuh.
but for the everybody it's little under a factor or two.
yeah i i know what i was thinking was that maybe uh we could actually try at least looking at uh some of the the large vocabulary speech from a far microphone.
at least from the good one.
uhhuh.
i mean before i thought we'd get you know a hundred and fifty percent error or something.
uhhuh.
but if if uh if we're getting thirty five forty percent or something um
actually if you run though on a close talking mike over the whole meeting during all those silences you get like four hundred percent word error.
uhhuh.
right i understand.
but doing the same kind of limited thing
or or some high number.
yeah sure.
get all these insertions.
but i'm saying if you do the same kind of limited thing as people have done in switchboard evaluations or as
yeah.
where you know who the speaker is and there's no overlap?
yeah.
and you do just the far field for those regions?
yeah the same sort of numbers that we got those graphs from.
could we do exactly the same thing that we're doing now but do it with a far field mike.
right?
yeah do it with one of
because we extract the times from the near field mike.
but you use the acoustics from the far field mike.
right i understand that.
i just meant that so you have three choices.
there's um you can use times where that person is talking only from the transcripts but the segmentations were were synchronized.
or you can do a forced alignment on the close talking to determine that you know within this segment these really were the times that this person was talking.
and elsewhere in the segment other people are overlapping.
and just front end those pieces.
or you can run it on the whole data.
which is which is you know a
but but but how did we get the how did we determine the links uh that we're testing on in the stuff we reported?
in the h.l.t. paper we took segments that are channel time aligned.
which is now being changed in the transcription process.
which is good.
and we took cases where the transcribers said there was only one person talking here.
because no one else had time any words in that segment.
and called that non overlap.
and
and that's what we were getting those numbers from.
yes.
right.
good the good numbers.
the bad numbers were from the segments where there was overlap.
well we could start with the good ones.
but anyway so i think that we should try it once with the same conditions that were used to create those.
yeah.
and in those same segments just use one of the p.z.m.'s.
right so we we can do that.
and then you know i mean the thing is if we were getting uh what thirty five forty percent something like that on on that particular set
yeah.
uh does it go to seventy or eighty?
or does it use up so much memory we can't decode it?
right.
it might also depend on which speaker it is and how close they are to the p.z.m.
uh
i don't know how different they are from each other.
you want to probably choose the p.z.m. channel that is closest to the speaker.
to be best
for this particular digit ones i just picked that one.
yeah.
well
okay.
oh okay.
so we would then use that one too.
so
this is kind of central.
or
you know it's so but i would i'd pick that one.
it'll be less good for some people than for other.
but i i'd like to see it on the same exact same data set that that we did the other thing on.
actually
i actually should've picked a different one.
right?
because that could be why the p.d.a. is worse.
because it's further away from most of the people reading digits.
it's further away yeah.
yeah.
that's probably one of the reasons.
huh.
uhhuh.
well yeah you could look at i guess that p.z.m. or something.
yep.
but the other is it's very uh i mean even though there's i'm sure the the the s.r.i. uh front end has some kind of pre emphasis it's it's uh still it's picking up lots of low frequency energy.
uhhuh.
so even discriminating against it i'm sure some of it's getting through.
um
but yeah you're right.
a part of it is just the distance.
and aren't these pretty bad microphones?
yep.
i mean
well they're bad.
but i mean if you listen to it it sounds okay you know.
yeah when you listen to it uh the p.z.m. and the p.d.a. yeah the p.d.a. has higher sound floor.
yeah.
but not by a lot.
it's really pretty uh pretty much the same.
i just remember you saying you got them to be cheap on purpose.
cheap in terms of their quality.
so
well they're twenty five cents or so.
we wanted them to be to be typical of what would be in a p.d.a.
yeah.
so they are they're not the p.z.m. three hundred dollar type.
uhhuh.
they're the twenty five cent.
yeah.
buy them in packs of thousand type.
but i mean the thing is people use those little mikes for everything.
i see.
everything.
because they're really not bad.
i mean if you're not doing something ridiculous like feeding it to a speech recognizer they they they you know you can hear the hear the sounds just fine.
uhhuh.
right.
you know it's
they i mean it's more or less the same principles as these other mikes are built under.
it's just that there's less quality control.
they just you know churn them out and don't check them.
um so
so that was yeah so that was interesting result.
so like i said the front end guys are very much interested in in this is as as well.
and
so so but where is this now?
i mean what's where do we go from here?
i mean
yeah that was going to be my question.
we so we have a we have a a system that works pretty well.
but it's not you know the system that people here are used to using to working with.
so what what do we do now?
well i think what we want to do is we want to uh
and we've talked about this in other contexts.
we want to have the ability to feed it different features.
uhhuh.
and then um from the point of view of the front end research it would be uh substituting for h.t.k.
okay.
okay.
i think that's the key thing.
and then if we can feed it different features then we can try all the different things that we're trying there.
okay all right.
and then um uh also dave is is thinking about using the data in different ways uh to um uh explicitly work on reverberation.
uhhuh.
starting with some techniques that some other people have found somewhat useful and yeah.
okay.
so so the key thing that's missing here is basically the ability to feed you know other features into the recognizer.
and also then to train the system.
right.
right.
okay.
and uh i don't know when chuck will be back.
but that's exactly what he he's going to
he's he's sort of back.
but he drove for fourteen hours and wasn't going to make it in today.
oh okay.
so i think that's one of the things that he said he would be working on.
um
just sort of to make sure that we can do that.
yeah.
and um
yeah.
right.
it's uh i mean the the front end is that's in the s.r.i. recognizer is very nice in that it does a lot of things on the fly.
but it unfortunately is not designed and um like the uh icsi system is where you can feed it from a pipeline of of the command.
so the what that means probably for the foreseeable future is that you have to uh dump out um
you know if you want to use some new features you have to dump them into individual files.
and give those files to the recognizer.
we do we tend to do that anyway.
okay.
oh.
so although you you can pipe it as well we tend to do it that way.
because that way you can concentrate on one block and not keep redoing it over and over.
oh okay.
all right.
yeah.
yeah so i've i
so that's exactly what the p. file is for.
yeah.
yeah the the the cumbersome thing is is um is that you actually have to dump out little little files.
uh
so for each segment that you want to recognize you have to dump out a separate file.
uhhuh.
just like like as if there were these waveform segments.
but instead you have sort of feature file segments.
but you know
so
cool.
okay so the the the next thing we had on the agenda was something about alignments?
oh.
yes we have
i don't know.
did you want to talk about it?
or
i can give a i was just telling this to jane.
and and we we were able to get some definite improvement on the forced alignments by looking at them first and then realizing the kinds of errors that were occurring.
and um
some of the errors occurring very frequently are just things like the first word being moved to as early as possible in the recognition.
which is a um i think was both a a pruning problem.
and possibly a problem with needing constraints on word locations.
and so we tried both of these things.
we tried saying
i don't know.
i got this whacky idea that just from looking at the data that when people talk their words are usually chunked together.
it's not that they say one word and then there's a bunch of words together.
they're might say one word and then another word far away if they were doing just backchannels.
but in general if there's like five or six words and one word's far away from it that's probably wrong on average.
so um
and then also the pruning of course was too too severe.
so that's actually interesting.
the pruning was the same value that we used for recognition.
and we had lowered that we had used tighter pruning after liz ran some experiments showing that you know it runs slower.
and there's no real difference in
no gain.
actually it was better with slightly better or about
right.
it was the same with tighter pruning.
so for free recognition this the lower pruning value is better.
you
it's probably because the recognition's just bad at a point where it's bad enough that that you don't lose anything.
correct.
right.
um but it turned out for for to get accurate alignments it was really important to open up the pruning significantly.
right.
huh.
um because otherwise it would sort of do greedy alignment um in regions where there was no real speech yet from the foreground speaker.
uhhuh.
um so that was one big factor that helped improve things.
and then the other thing was that
you know as liz said the we enforce the fact that uh the foreground speech has to be continuous.
it cannot be you cannot have a background speech hypothesis in the middle of the foreground speech.
you can only have background speech at the beginning and the end.
yeah i mean yeah it isn't always true.
and i think what we really want is some clever way to do this.
where um you know from the data or from maybe some hand corrected alignments from transcribers that
things like words that do occur just by themselves alone like backchannels or something that we did allow to have background speech around it
yeah.
those would be able to do that.
sorry.
but the rest would be constrained.
so i think we have a version that's pretty good for the native speakers.
i don't know yet about the non native speakers.
and um
we basically also made noise models for the different sort of grouped some of the mouth noises together.
um so and then there's a background speech model.
and we also
there was some neat or interesting cases.
like there's one meeting where um jose's giving a presentation.
and he's talking about um the word mixed signal.
and someone didn't understand uh that you were saying mixed.
i think morgan.
yeah yeah.
and so your speech was saying something about mixed signal.
and the next turn was a lot of people saying mixed.
like he means mixed signal or i think it's mixed.
and the word mixed in this segment occurs like a bunch of times.
and chuck's on the lapel here.
and he also says mixed.
but it's at the last one.
and of course the aligner aligns it everywhere else to everybody else's mixed.
yeah.
because there's no adaptation yet.
so there's i think there's some issues about
we probably want to adapt at least the foreground speaker.
but i guess andreas tried adapting both the foreground and a background generic speaker.
and that's actually a little bit of a funky model.
like it gives you some weird alignments.
just because often the background speakers match better to the foreground than the foreground speaker.
oh
yeah.
oh.
so there's some things there.
especially when you get lots of the same words uh occurring in the.
well the
i i think you can do better by uh cloning
so we have a reject phone.
and you and what we wanted to try with you know once we have this paper written and have a little more time uh cloning that reject model.
and then one copy of it would be adapted to the foreground speaker to capture the rejects in the foreground.
like fragments and stuff.
and the other copy would be adapted to the background speaker.
right i mean in general we actually
and
right now the words like partial words are reject models.
uhhuh.
and you normally allow those to match to any word.
but then the background speech was also a reject model.
and so this constraint of not allowing rejects in between.
you know it needs to differentiate between the two.
so just sort of working through a bunch of debugging kinds of issues.
right.
and another one is turns like people starting with well i think
and someone else is well how about
so the word well is in this in this segment multiple times.
and as soon as it occurs usually the aligner will try to align it to the first person who says it.
but then that constraint of sort of uh proximity constraint will push it over to the person who really said it in general.
is the proximity constraint a hard constraint?
or did you do some sort of probabilistic weighting distance or
we we didn't
no.
right now it's a kluge.
we
okay.
we it's straightforward to actually just have a a penalty that doesn't completely disallows it but discourages it.
but um we just didn't have time to play with you know tuning yet another yet another parameter.
the level.
yeah.
yeah.
and really the reason we can't do it is just that we don't have a we don't have ground truth for these.
so we would need a hand marked um word level alignments.
or at least sort of the boundaries of the speech you know between the speakers.
um and then use that as a reference.
and tune the parameters of the of the model uh to to get the best performance.
yeah.
given i i mean i i i was going to ask you anyway uh how you assessed that things were better?
uhhuh.
i looked at them.
i spent two days um in waves.
okay.
oh it was painful.
because the thing is you know the alignments share a lot in common.
so
and you're you're looking at these segments where there's a lot of speech.
i mean a lot of them have a lot of words.
yeah.
not by every speaker.
but by some speaker there's a lot of words.
yeah.
no not
i mean that if you look at the individual segments from just one person you don't see a lot of words.
yeah.
uhhuh.
yeah.
but altogether you'll see a lot of words up there.
and so the reject is also mapping and pauses.
yeah.
so i looked at them all in waves.
and just lined up all the alignments.
and at first it sort of looked like a mess.
and then the more i looked at it i thought okay well it's moving these words leftward.
and you know it wasn't that bad.
it was just doing certain things wrong.
so
but i don't you know have time to to look at all of them.
and it would be really useful to have like a a transcriber who could use waves.
um just mark like the beginning and end of the foreground speaker's real words.
like the beginning of the first word the end of the last word.
yeah.
and then we could you know do some adjustments.
i okay.
i have to ask you something.
because if we could benefit from what you did incorporate that into the present transcripts that would help.
no.
and then um the other thing is i believe that i did
so one of these transcripts was gone over by a transcriber.
and then i hand marked it myself so that we do have uh the beginning and ending of individual utterances.
uhhuh.
um i didn't do it word level.
but but in terms
so i so for for one of the n.s.a. groups.
uhhuh.
and also i went back to the original one that i first transcribed and and did it uh uh utterance by utterance for that particular one.
so i think you do have
if that's a sufficient unit i think that you do have hand marking for that.
but it'd be wonderful to be able to benefit from your waves stuff.
uhhuh.
we don't care what what tool you use.
yeah i mean if if you can um if you want to
okay.
i used it in transcriber.
uh
and it's it's in the
well jane and i were just in terms of the tool talking about this.
i guess sue had had some reactions.
you know interface wise if you're looking at speech you want to be able to know really where the words are.
yeah that's right.
and so we can give you some examples of sort of what this output looks like.
middle of the word or
um and see if you can maybe incorporate it into the transcriber tool some way.
or
well i i'm thinking just incorporating it into the representation.
um
i mean if it's if it's
you mean like yeah word start insights.
if you have start points if you have like time tags
right.
which is what i assume.
isn't that what what you
well see adam would be
yeah whatever you use.
i mean we convert it to this format that the um nist scoring tool uh c.t.m. conversation time marked file.
yeah.
and and then that's the that's what the
i think transcriber uh outputs c.t.m.
if it
okay.
i think so.
so you would know this more than i would.
yeah.
right.
so i mean
it seems like she if she's if she's moving time marks around
since our representation in transcriber uses time marks it seems like there should be some way of of using that benefitting from that.
right.
yeah it the advantage would just be that when you brought up a bin you would be able if you were zoomed in enough in transcriber to see all the words.
uhhuh.
you would be able to like have the words sort of located in time.
if you wanted to do that.
so so if we even just had a a it sounds like we we almost do.
so
uh if we we have two.
we have two.
yeah.
just uh trying out the alignment procedure that you have on that.
uhhuh.
you could actually get something um uh uh get an objective measure.
uhhuh.
uh
you mean on on the hand marked um
so we we only i only looked at actually alignments from one meeting that we chose.
yeah.
i think m.r. four.
just randomly um
and
actually not randomly.
not randomly.
we knew we knew that it had these insertion errors from.
it had sort of average recognition performance in a bunch of speakers.
yeah.
yeah.
and it was a meeting recorder meeting.
um
but yeah we should try to use what you have.
i did rerun recognition on your new version of m.r. one.
oh good.
i i mean the the one with dan ellis in it and eric.
good.
uhhuh yeah exactly yeah yeah.
i don't think that was the new version.
um
that
yeah actually it wasn't the new new.
okay.
it was the medium new.
but but we would we should do the the latest version.
okay.
yeah.
you did you adjust the the utterance times um for each channel?
it was the one from last week.
yes.
yes i did.
and furthermore i found that there were a certain number where not not a lot but several times i actually moved an utterance from adam's channel to dan's or from dan's to adam's.
so there was some speaker
and the reason was because i transcribed that at a point before uh before we had the multiple audio available.
so i couldn't switch between the audio.
i i transcribed it off of the mixed channel entirely.
which meant in overlaps i was at a at a terrific disadvantage.
right.
right.
in addition it was before the channelized uh possibility was there.
and finally i did it using the speakers of my um of you know off the c.p.u. on my on my machine.
because i didn't have a headphone.
so it like i mean
right.
yeah i i mean in retrospect it would've been good to have i should've gotten a headphone.
but in any case um this is this was transcribed in a in a uh less optimal way than than the ones that came after it.
and i was able to you know and this meant that there were some speaker identifications.
well i know there were some speaker labelling problems um after interruptions.
which were changes
yeah.
fixed that.
is that what you're referring to?
i mean because there's this one instance when for example you're running down the stairs.
oh well
i remember this meeting really well.
yeah.
right.
don don has had he knows he can just read it like a play.
it's a
yeah i've i've i'm very well acquainted with this meeting.
yeah i can
yeah.
and then she said and then he said.
yeah i know it by heart.
so um there's one point when you're running down the stairs.
uhoh!
right?
and like there's an interruption.
you interrupt somebody.
but then there's no line after that.
for example there's no speaker identification after that line.
uhhuh.
is that what you're talking about?
or were there mislabelings as far as like the adam was?
that was fixed um before.
i think i think i understood that pretty
yeah because i thought i let you know about that.
thank you for mentioning.
yeah.
yeah no that that i think went away a couple of versions ago.
okay.
but you're actually saying that certain uh speakers were mis identified.
but it's good to know.
yeah so with under um uh listening to the mixed channel there were times when as surprising as that is i got adam's voice confused with dan's and vice versa.
okay.
not for long utterances.
okay.
yeah.
but just a couple of places.
uhhuh.
and embedded in overlaps.
the other thing that was interesting to me was that i picked up a lot of um backchannels which were hidden in the mixed signal.
which you know i mean you not not too surprising.
right.
but the other thing that
i i hadn't thought about this.
but i i wanted to raise this when you were uh with respect to also a strategy which might help with the alignments potentially.
but that's
when i was looking at these backchannels they were turning up usually very often in well i won't say usually but anyway very often i picked them up in a channel which was the person who had asked a question.
so like someone says and have you done the so-and-so.
and then there would be backchannels.
but it would be the person who asked the question.
other people weren't really doing much backchanneling.
and you know sometimes you have the yeah uhhuh.
well that's interesting.
i mean it wouldn't be perfect.
yeah.
but but it does seem more natural to give a backchannel when when you're somehow involved in the topic.
no that's really interesting.
and the most natural way is for you to have initiated the topic by asking a question.
uhhuh.
well
i think
that's interesting.
no i think it's actually i think what's going on is backchanneling is something that happens in two party conversations.
uhhuh.
and if you ask someone a question you essentially initiating a little two party conversation.
yeah.
well yeah when we looked at this.
so then you're so and then you're expected to backchannel.
exactly.
because the person is addressing you directly and not everybody.
exactly.
exactly my point.
yeah.
and so this is the expectation thing that uh uh
yeah.
uhhuh.
right.
right.
just the dyadic
but in addition you know if someone has done this analysis himself and isn't involved in the dyad but they might also give backchannels to verify what what the answer is that this that the the answerer's given
i tell you i say i say uhhuh a lot.
right.
it's
there you go.
well but it's interesting.
because uh
while people are talking to each other.
there you go.
but there are fewer i think there are fewer uhhuh.
yeah.
yeah.
i mean just from we were looking at word frequency lists to try to find the cases that we would allow to have the reject words in between in doing the alignment.
you know the ones we wouldn't constrain to be next to the other words.
oh yeah.
and uhhuh is not as frequent.
as it sort of would be in switchboard.
if you looked at just a word frequency list of one word short utterances.
and yeah is way up there.
but not uhhuh.
and so i was thinking
it's not like you're being encouraged by everybody else to keep talking in the meeting.
and uh that's all i'll stop there.
because i think what you say makes a lot of sense.
well that's right.
and that would
well
but it was sort of
and what you say is the is the uh other side of this.
which is that you know so there are lots of channels where you don't have these backchannels when a question has been asked.
and and these
right.
there's just probably less backchanneling in general.
uhhuh.
so that's good news really.
even if you consider every other person altogether one person in the meeting.
but we'll find out anyway.
we were i guess the other thing we're we're i should say is that we're going to um try compare this type of overlap analysis to switchboard.
and callhome.
where
and callhome.
where we have both sides so that we can try to answer this question of you know is there really more overlap in meetings or is it just because we don't have the other channel in switchboard?
uhhuh.
uhhuh.
and we don't know what people are doing.
try to create a paper out of that.
yeah i mean you folks have probably already told me.
but were were you intending to do a eurospeech submission?
or
um you mean the one due tomorrow?
yeah.
yeah well we're still like writing the scripts for doing the research.
and we will yes we're going to try.
uhhuh.
and i was telling don do not take this as an example of how people should work.
do as i say.
that's
so we will try.
don't do as i do.
yeah.
well
it'll probably be a little late.
it is different.
but i'm going to try it.
in previous years eurospeech only had the abstract due by now.
not the full paper.
right.
right.
and so all our timing was off.
i've given up on trying to do digits.
i just don't think that what i have so far makes a eurospeech paper.
well i'm we may be in the same position.
and i figured we'll try.
because that'll at least get us to the point where we have we have this really nice database format that andreas and i were working out that
it it's not very fancy.
it's just a ascii line by line format.
but it does give you information.
it's the it's the spurt format.
it yeah we're calling these spurts after chafe.
i was trying to find what's a word for a continuous region with pauses around it.
huh.
yeah i know that the telecom people use use spurt for that.
good.
they do?
oh.
yes.
oh.
oh.
and that's i mean i i was using that for a while when i was doing the rate of speech stuff.
i would
because i because i looked up in some books and i found okay i want to find a spurt in which
uh right.
it's just like defined by the acoustics.
and because because it's another question about how many pauses they put in between them.
horrible.
right.
but how fast do they do the words within the spurt?
right.
yeah.
you know burst also?
it's going to
well that's what we were calling spurt.
burst.
isn't burst is used also?
so
spurt has the horrible name overloading with other with hardware at icsi.
here.
just very locally yeah.
but but that just
well well chafe had this i think it was chafe or somebody had a the word spurt originally.
here
and so i
actually
but that's good to know.
was it's chafe?
so maybe we should talk
well see i know sue wrote about spurts of development.
maybe it was sue
but in any case i think it's a good term.
huh!
and uh
so we have spurts and we have spurt-ify dot shell and spurt-ify.
yeah.
and maybe maybe chafe did.
uh
yeah.
so
i know i know chafe dealt with
and then it's got all it's a verb now.
that's cool.
uh
chafe speaks about intonation units.
yes right.
but maybe he speaks about spurts as well.
and i just don't know.
yeah go ahead.
so what we're doing
i've heard burst also.
uh this this is just maybe someone has some some ideas about how to do it better.
huh.
but we so we're taking these uh alignments from the individual channels.
we're
from each alignment we're producing uh one of these c.t.m. files.
which essentially has it's just a linear sequence of words with the begin times for every word and the duration.
great.
and and and of course
it looks like a waves label file almost right?
right but it has one the first column has the meeting name.
it's just
so it could actually contain several meetings.
um
and the second column is the channel.
third column is the um start times of the words and the fourth column is the duration of the words.
and then we're
um
okay then we have a messy alignment process where we actually insert into the sequence of words the uh tags.
for like where where sentence ends of sentence.
question marks.
um various other things.
yeah these are things that we had don
uh
so don sort of um propagated the punctuation from the original transcriber.
right.
so whether it was like question mark or period or um you know comma and things like that.
and we kept the disfluency dashes uh kept those in because we sort of want to know where those are relative to the spurt overlaps.
uhhuh.
right.
so so those are actually sort of retro fitted into the time alignment.
overlaps.
or
and then we merge all the alignments from the various channels.
and we sort them by time.
and then there's a then there's a process where you now determine the spurts.
that is actually no you do that before you merge the various channels.
so you you identify by some criterion.
which is pause length.
you identify the beginnings and ends of these spurts.
and you put another set of tags in there to keep those straight.
uhhuh.
and then you merge everything in terms of you know linearizing the sequence based on the time marks.
and then you extract the individual channels again.
but this time you know where the other people start and end talking.
you know where their spurts start and end.
and so you extract the individual channels uh one spurt by spurt as it were.
um and inside the words or between the words you now have begin and end tags for overlaps.
so you you basically have everything sort of lined up and in a form where you can look at the individual speakers and how their speech relates to the other speakers' speech.
right.
uh i mean i think that's actually really useful also.
and
because even if you weren't studying overlaps if you want to get a transcription for the far field mikes how are you going to know which words from which speakers occurred at which times relative to each other?
you have to be able to get a transcript like like this anyway just for doing far field recognition.
so
you know it's it's sort of
yeah.
i it's just an issue we haven't dealt with before.
how you time align things that are overlapping anyway.
so
that's wonderful.
well
and and we
i mean i never thought about it before.
yes.
in
but
i mean when i came up with the original data suggested data format based on the transcription graph there's capability of doing that sort of thing in there.
right.
uhhuh.
but you can't get it directly from the transcription.
right.
yeah that's right.
well this is this is just
yeah this is like a poor man's formatting version.
but it's you know it's clean.
it's just not fancy.
right.
well there's lots of little things.
um
it's like there're twelve different scripts which you run.
and then at the end you have what you want.
but um
at the very last stage we throw away the actual time information.
all we care about is whether that there's a certain word was overlapped by someone else's word.
so you sort of at that point you discretize things into just having overlap or no overlap.
because we figure that's about the level of analysis that we want to do for this paper.
uhhuh.
but if you wanted to do a more fine-grained analysis and say you know how far into the word is the overlap you could do that.
it's just it'll just require more
yeah.
just sort of huge.
you know slightly different.
what's interesting is it's exactly what um in discussing with um sue about this.
yeah.
um she um indicated that that you know that's very important for overlap analysis.
yeah it's it's nice to know.
right.
and also i think as a human like i don't always hear these in the actual order that they occur.
so i can have two foreground speakers.
you know morgan and um adam and jane could all be talking.
and i could align each of them to be starting their utterance at the correct time.
and then look where they are relative to each other.
and that's not really what i heard.
and that's another thing she said.
because it's just hard to do.
this is this is bever's bever's effect.
when where in psycholinguistics you have these experiments where people have perceptual biases as to what they hear.
yeah.
it's sort of
that that not the best
yeah you sort of move things around until you get to a low information point.
and then you can bring in the other person.
so it's actually not even possible i think for any person to listen to a mixed signal.
even equalize and make sure that they have all the words in the right order.
uhhuh.
so i guess we'll try to write this eurospeech paper.
superb.
i mean we will write it.
whether they accept it late or not i don't know.
um and the good thing is that we have it's sort of a beginning of what don can use to link the prosodic features from each file to each other.
yeah.
yeah that's the good thing about these
plus
so
huh?
you know might as well.
i otherwise we won't get the work done on our deadline.
i don't know
i mean jane likes to look at data.
yeah.
maybe you know you could you could look at this format and see if you find anything interesting.
i don't know.
yeah.
well what i'm thinking is
no it's that's the good thing about these paper deadlines and uh you know class projects and and things like that.
yeah.
yeah.
yeah.
uhhuh.
right.
well my
well the other thing that that that that you usually don't tell your graduate students is that these deadlines are actually not that um you know strictly enforced.
because you you really get
yeah.
forces you to do the work.
yeah.
exactly.
strict.
because the
oh now it's out in the public.
this this this secret information.
because
i think we can
yeah.
nah
right.
no.
so
no.
nah.
because these the conference organizers actually have an interest in getting lots of submissions.
right.
i mean a a monetary interest.
right.
yeah.
so um
that's that's true.
and good ones.
good ones.
and good
which sometimes means a little extra time.
that's
right.
well
that's true.
that's another issue.
but
by by the way this is totally unfair you may you may feel.
but the the uh the morning meeting folks actually have an an extra month or so.
uhhuh.
yep.
yep the aurora there's a special aurora
when
uh
there's a special aurora session.
and the aurora people involved in aurora have till uh early may or something to turn in their paper.
oh.
huh.
oh.
huh.
well then you can just
oh well maybe we'll submit to actually
maybe you can submit the digits paper on for the aurora session.
yeah.
yeah.
yeah.
oh i could!
if it
yeah.
i could submit that to aurora.
yeah.
well
that would be pretty pretty
it has
yeah.
that wouldn't work.
no it wouldn't work.
it's not aurora.
it's it's not the aurora
i mean it it's it's actually the aurora task.
maybe they'll get
aurora's very specific.
well maybe it won't be after this deadline extension.
but but the people i mean
a a paper that is not on aurora would probably be more interesting at that point.
maybe they'll
because everybody's so sick and tired of the aurora task.
yeah.
oh i thought you meant this was just the digits section.
i didn't know you meant it was aurora digits.
yeah.
well no if you if you have it's to if you discuss some relation to the aurora task.
like if you use the same
this is not the aurora task.
so they just do a little grep for
do uh
um
do not do not we are not setting a good example.
well a relation other than negation maybe.
this is not a
um
so
i don't know.
anyway
but the good thing is this does
well i don't know.
i mean you could you could do a paper on what's wrong with the aurora task by comparing it to other ways of doing it.
how well does an aurora system do on on you know on digits collected in a in this environment?
different way.
yeah.
yeah.
maybe.
maybe.
pretty hokey.
i think it's a little farfetched.
nah i mean the thing is aurora's pretty closed community.
yep.
i mean you know the people who were involved in the the only people who are allowed to test on that are people who who made it above a certain threshold in the first round.
uhhuh.
it's very specific.
uh in ninety nine.
and it's it's sort of a it's
well that's maybe why they don't know that they have a crummy system.
not like a
i mean a crummy back end.
no i mean i mean seriously.
if you if you have a very
no i'm sorry.
uh beep!
no i didn't mean anybody any particular system.
i mean
i meant this h.t.k. back end.
if they
oh you don't like h.t. k?
yeah.
i don't i don't have any stock in h.t.k. or entropic or anything.
no i mean this it's the h.t.k. that is trained on a very limited amount of data.
it's it's very specific.
right.
yeah.
but so if you but maybe you should you know consider more using more data.
or i mean
oh yeah i i really think that that's true.
if if you sort of hermetically stay within one task and don't look left and right then you're going to
and they
but they they had
but
they had something very specific in mind when they designed it right?
right.
well
and so so you can you can argue about maybe that wasn't the right thing to do.
but you know they they they had something specific.
but one of the reasons i have chuck's messing around with with the back end that you're not supposed to touch
uhhuh.
i mean for the evaluations yes we'll run a version that hasn't been touched.
uhhuh.
but uh one of the reasons i have him messing around with that because i think it's sort of an open question that we don't know the answer to.
people always say very glibly that if you show improvement on a bad system that doesn't mean anything.
because it may not be show uh because you know it doesn't tell you anything about the good system.
uhhuh.
and i i've always sort of felt that that depends.
you know that if some if you're actually are getting at something that has some conceptual substance to it it will port.
uhhuh.
and in fact most methods that people now use were originally tried with something that was not their absolute best system at some level.
but of course sometimes it doesn't uh port.
so i think that's that's an interesting question.
if we're getting three percent error on uh uh english uh native speakers um using the aurora system and we do some improvements and bring it from three to two do those same improvements bring uh you know the s.r.i. system from one point three to you know to point eight
huh.
uhhuh.
zero.
well
huh.
you know so that's that's something we can test.
right.
so
anyway
okay.
i think we've we've covered that one up extremely well.
uhhuh.
whew!
okay.
so um
yeah.
so so we'll you know maybe you guys will have have one.
uh you you and uh and dan have have a paper that that's going in.
yeah.
yeah.
you know that's that's pretty solid on the segmentation stuff.
yeah.
i will send you the the final version.
yeah.
and the aurora folks here will will definitely get something in on aurora.
which is not
actually this this um so there's another paper.
so
it's a eurospeech paper but not related to meetings.
but it's on digits.
so um
uh a colleague at s.r.i. developed a improved version of m.m.i.e. training.
uhhuh.
and he tested it mostly on digits.
because it's sort of a you know it doesn't take weeks to train it.
right.
um
and got some very impressive results um with you know discriminative uh gaussian training um you know like um error rates go from i don't know in very noisy environment like from
uh uh
i now i okay now i have the order of
i'm not sure about the order of magnitude.
was it like from ten percent to eight percent or from you know point you know from one percent to point eight percent?
it got it got better.
i mean it's a
it got better.
yeah.
that's the important thing.
yeah yeah.
yeah.
hey that's the same percent relative.
but it's
so
yeah right.
yeah.
it's uh something in
yeah.
right.
twenty percent relative gain.
yeah.
yeah.
yeah.
um let's see.
i think the only thing we had left was unless somebody else
well there's a couple things.
uh one is anything that um anybody has to say about saturday?
anything we should do in prep for saturday?
um
i guess everybody knows about
i mean um mari was asking was trying to come up with something like an agenda.
and we're sort of fitting around people's times a bit.
but um clearly when we actually get here we'll move things around this as we need to.
but
okay.
so you can't absolutely count on it.
yeah.
but but uh
are we meeting in here probably?
or
yeah.
okay.
that was my thought.
i think this is
are we recording it?
yeah.
we won't have enough microphones.
no i i hadn't intended to.
but
there's no way.
okay.
we we want to i mean they're there's going to be uh jeff katrin mari and two students.
so there's five from there.
and brian.
but you know
and brian's coming.
so that's six.
uhhuh.
and plus all of us.
can use the oprah mike.
uh
depends how fast you can throw it.
it seems like too many too much coming and going.
it's just
uhhuh.
yeah.
well
we don't even have enough channel
because it would be a different kind of meeting.
yeah.
that's what i'm
yeah.
well
but
i hadn't really thought of it.
maybe just maybe not the whole day.
but
but just you know maybe some i mean
maybe part of it.
part of it?
maybe part of it.
make everyone read digits.
at the same time.
at the same time.
please.
at the same time.
yeah.
we
i don't know.
that's their initiation into our
into our our our cult.
yeah our yeah our
maybe the sections that are not right you know after lunch when everybody's still munching.
and
okay.
so can you send out a schedule once you know it
well
okay yeah i guess i sent it around a little bit.
is is there a
there's a
but
is it changed now or
i hadn't heard back from mari after i i uh brought up the point about andreas's schedule.
so um maybe when i get back there'll be some some mail from her.
okay.
so i'll make a
i'm looking forward to seeing your representation.
that'd be uh
and we should get the two meetings from
i'd like to see that.
yeah.
i mean i know about the first meeting.
um
but the other one that you did.
the n.s.a. one.
which we hadn't done because we weren't running recognition on it.
uhhuh.
because the non native speaker
there were five non native speakers.
uhhuh.
i see.
uhhuh.
but it would be useful for the to see what we get with that one.
great.
so
okay.
it's uh two thousand eleven twenty one one thousand?
yeah three.
right.
so
great.
i sent email when i finished the that one.
n.s.a. three i think.
that was sort of
yeah that's right that's right.
that's much simpler.
i don't know what they said.
but i know the number.
that part's definitely going to confuse somebody who looks at these later.
right.
um
i mean this is we're recording secret n.s.a. meetings.
not the
i mean it's
yeah.
uh
yeah not that n.s.a.
the the
it's network services and applications.
they are hard to understand.
wait.
they're very uh out there.
the
i have no idea what they're talking about.
the um
yeah.
the other good thing about the alignments is that um it's not always the machine's fault if it doesn't work.
so you can actually find um
it's the person's fault.
problem uh
it's morgan's fault.
you can find
you can find uh problems with with the transcripts.
it's always morgan's fault.
um you know
oh.
and go back and fix them.
yeah.
but
there are some cases like where the the wrong speaker uh these not a lot but where the the wrong person the the speech is attached to the wrong speaker.
and you can tell that when you run it.
or at least you can get clues to it.
interesting.
i guess it does
so these are from the early transcriptions that people did on the mixed signals like what you have.
uhhuh.
it also raises the possibility of um using that kind of representation i mean i don't know this would be something we'd want to check but maybe using that representation for data entry.
and then displaying it on the channelized uh representation.
because it i think that the i mean my my preference in terms of like looking at the data is to see it in this kind of musical score format.
uhhuh.
and also you know sue's preference as well.
yeah if you can get it to
and and
but i mean this if this is a better interface for making these kinds of uh you know local changes then that'd be fine too.
i don't i have no idea.
i think this is something that would need to be checked.
yeah.
okay.
the other thing i had actually was
i i didn't realize this till today.
but uh this is uh jose's last day.
is my last my last day.
yeah.
oh!
oh!
oh!
my my last meeting about meetings.
you're not going to be here tomorrow?
oh that's right.
yeah.
tomorrow
because uh i leave uh the next sunday.
the last meeting meeting?
it's off.
i will come back to home to spain.
uhhuh.
oh.
yeah.
i so i i
uhhuh.
and i i would like to to to say thank you very much uh to all people in the group and at icsi.
oh.
uhhuh.
yeah.
it was good having you.
huh.
yeah.
because i i enjoyed very much.
huh.
uh
and i'm sorry by the result of overlapping.
because uh i haven't good results uh yet.
but uh i i pretend to to continuing out to spain uh during the the following months.
uh because i have uh another ideas.
uhhuh.
but uh i haven't enough time to to with six months it's not enough to to to research.
yep.
yeah.
and i mean if uh the topic is uh so difficult.
uh in my opinion there isn't
yeah maybe somebody else will come along and will be uh interested in working on it.
and could start off from where you are also you know.
they'd make use of of what you've done.
yeah.
yeah.
yeah.
but uh i i will try to recommend uh at uh the spanish government.
but uh the following scholarship uh uh uh will be here more time.
because uh in my opinion is is better uh for us to to spend more time here.
and to work more time in a topic.
no?
yeah it's a very short time.
but uh
yeah.
yeah six months is hard.
yeah.
yeah it is.
i think a year is a lot better.
yeah.
yeah.
it's difficult.
you you have uh you are lucky.
and you you find a solution in in in some few uh months uh?
okay.
but uh i think it's not uh common.
but uh anyway thank you.
thank you very much.
uh i i bring the chocolate uh to to tear uh with with you.
huh.
uh.
oh.
nice.
uh.
i i hope if you need uh something uh from us in the future i i will be at spain to you help.
uh
great.
well
great.
thank you jose.
thank you.
right.
and thank you very much.
have a good trip.
yeah.
thank you.
yeah.
keep in touch.
yeah.
okay.
i guess uh unless somebody has something else we'll read read our digits.
digits?
and we'll get our
uh
are we going to do them simultaneously?
oops.
get our last bit of uh jose's jose jose's digit.
or
you uh
uh i'm sorry?
you prefer uh to eat uh chocolate uh at the coffee break uh at the or you prefer now before after
well we have a time
no we prefer to keep it for ourselves.
during
yeah yeah.
well we have a a time time constraint.
during digits.
so keep it away from that end of the table.
yeah.
yeah.
yeah.
yeah.
why is it that i can read your mind?
well we've got to wait until after after we take the mikes off.
no no.
so are we going to do digits simultaneously?
well
you this is our reward if we do our
yeah.
or what?
yeah.
okay.
simultaneous digit chocolate task.
i i think uh it's enough uh for more for more people after.
we're going to we're going to do digits at the same
oh!
huh.
but uh
that's nice.
uhhuh.
um
oh thanks jose.
wow.
to andreas the idea is is good to eat here.
well
huh.
wow.
oh.
very nice.
that's that looks great.
oh wow.
oh yeah.
it doesn't it won't leave this room.
all right so in the interest of getting to the
we could do digits while other people eat.
yeah.
yeah.
so it's background crunching.
yeah.
huh.
is uh a another acoustic event.
nice.
we don't have background chewing.
background crunch yeah.
no we don't have any data with background eating.
huh.
yeah.
i'm serious.
she's she's serious.
it's just the rest of the digits the rest of the digits are very clean.
i am serious.
she is serious.
huh.
are you
oh they're clean.
well
um without a lot of background noise.
yeah.
and it
so i'm just not sure.
you have to write down like while what you're what chocolate you're eating.
because they might make different sounds.
like nuts chocolate with nuts chocolate without nuts.
oh.
crunchy frogs.
um
chocolate adaptation.
actually actually kind of careful.
because i have a strong allergy to nuts.
so i have to sort of figure out one without
that oh yeah they they might.
it's hard to hard to say.
maybe those?
they're so
i don't know.
i don't know.
um
this is you know this is a different kind of speech.
well
take take several.
looking at chocolates deciding
huh.
you know it's another style.
huh.
yeah i may i may hold off.
but if i was uh but maybe i'll get some later.
huh.
thanks.
well well why don't we
he he's worried about a ticket.
why don't we do a simultaneous one?
okay.
okay.
okay.
huh.
simultaneous one?
okay.
remember to read the transcript number please.
and you laughed at me too the first time i said that.
right.
okay.
oops.
i have to what?
yeah.
you laughed at me too the first time i said.
i did.
and now i love it so much.
you really shouldn't uh
okay everyone ready?
you have to sort of um jose if you haven't done this you have to plug your ears while you're talking.
wait wait a minute wait a minute.
we want we want
so that you don't get confused i guess.
we want it synchronized.
yeah.
hey you've done this before haven't you?
yeah.
oh you've done this one before?
that's
you've read digits together with us haven't you?
together.
i mean at the same time.
no.
oh you haven't.
i'm not we we oh and you haven't done this either.
okay.
oh okay.
oh yeah.
the first time is traumatic.
yeah.
but
oh and the groupings are important.
huh.
so you're supposed to pause between the groupings.
the grouping.
yeah.
yeah.
okay.
so uh
you mean that the the grouping is supposed to be synchronized?
yeah sure.
no.
no no.
no?
no?
synchronized digits.
no.
that'd be good.
we'll give everybody the same sheet.
it's like a like a greek like a greek choir?
but they say different
you know?
like
yes.
hey what a good idea.
yeah.
we could do the same sheet for everyone.
have them all read them at once.
well different digits.
uh
but same groupings.
or or just same digits.
so they would all be
see if anyone notices.
yeah.
yeah that'd be good.
there's so many possibilities.
and then then we can sing them next time.
uh okay why don't we go?
uh one two three go.
okay.
and andreas has the last word.
did you read it twice or what?
he's no he's trying to get good recognition performance.
he had the
yeah.
yeah.
he had the the long form.
no.
and we're off.
okay.
so uh i also copied uh the results that we all got in the mail i think from uh from o g i.
and we'll go go through them also.
so where are we on on uh our runs?
uh uh we so as i was already said we we mainly focused on uh four kind of features.
excuse me.
the p l p the p l p with j rasta the m s g and the m f c c from the baseline aurora.
uhhuh.
uh and we focused for the the test part on the english and the italian.
um we've trained uh several neural networks on
so on the t i digits english and on the italian data.
and also on the broad uh english uh french and uh spanish databases.
huh so there's our result tables here for the tandem approach.
and um actually what we we observed is that if the network is trained on the task data it works pretty well.
okay.
chicken on the grill.
our our uh there's a we're pausing for a photo.
try that corner.
how about over from the front of the room?
yeah it's longer.
we're pausing for a photo opportunity here.
uh uh
so
oh wait wait wait wait wait wait.
get out of the
yeah.
hold on.
hold on.
okay.
let me give you a black screen.
he's facing this way.
what?
okay this this would be a good section for our silence detection.
okay.
uhhuh.
um oh.
musical chairs everybody.
okay so um you were saying about the training data.
yeah.
yeah.
so if the network is trained on the task data um tandem works pretty well.
and uh actually we have uh
results are similar only on
do you mean if it's trained only on on data from just that task?
yeah.
that language?
just that task.
but actually we didn't train network on uh both types of data.
i mean uh phonetically
phonetically balanced uh data and task data.
we only did either task task data or uh broad data.
huh.
uhhuh.
um yeah.
so
so how
i mean clearly it's going to be good then.
so what's
but the question is how much worse is it if you have broad data?
i mean my
from what i saw from the earlier results uh i guess last week was that um if you trained on one language and tested on another say that the results were were relatively poor.
huh yeah.
but but the question is if you train on one language but you have a broad coverage and then test in another does that is that improve things in comparison?
if we use the same language?
no no no.
different
so um if you train on t i digits and test on italian digits you do poorly let's say.
uhhuh.
i don't have the numbers in front of me.
but yeah but i did not uh do that.
so i'm just imagining.
so you didn't train on timit and test on on italian digits say?
we no we did four four kind of of testing actually.
the first testing is with task data.
so with nets trained on task data.
so for italian on the italian speech.
the second test is trained on a single language um with broad database.
but the same language as the task data.
okay.
but for italian we choose spanish which we assume is close to italian.
the third test is by using um the three language database.
and the fourth is
which in
it has three languages.
that's including the the the
this includes
the one that it's
yeah.
but not digits.
i mean it's
right.
the three languages is not digits.
it's the broad data.
yeah.
okay.
and the fourth test is uh excluding from these three languages the language that is the task language.
oh okay.
yeah so that is what i wanted to know.
yeah.
i just wasn't saying it very well i guess.
uh yeah.
so um for uh t i digits for
example uh when we go from t i digits training to timit training uh we lose uh around ten per cent.
uh the error rate increase of of of ten per cent relative.
relative.
right.
so this is not so bad.
and then when we jump to the multilingual data it's uh it become worse.
and well around uh let's say twenty twenty per cent further.
about how much?
so
yeah.
twenty per cent further?
twenty to to thirty per cent further.
yeah.
and so remind me.
the multilingual stuff is just the broad data.
right?
yeah.
it's not the digits.
so it's the combination of two things there.
it's removing the task specific training and it's adding other languages.
yeah yeah.
okay.
but the first step is already removing the task specific from from
already.
right right right.
so
and we lose
so they were sort of building here.
yeah.
okay.
uh so basically when it's trained on the the multilingual broad data um or number.
so
the the ratio of
our error rates uh with the baseline error rate is around uh one point one.
so
yes and it's something like one point three of of the uh
if you compare everything to the first case at the baseline.
you get something like one point one for the for the using the same language but a different task.
and something like one point three for three three languages broad stuff.
no no no.
uh same language we are at uh for at english at o point eight.
so it improves compared to the baseline.
but
so
let me
i i i'm sorry.
task data
we are
i i i meant something different by baseline.
yeah.
so let me let me
um so um
huh.
okay fine.
let's let's use the conventional meaning of baseline.
huh.
i i by baseline here i meant uh using the task specific data.
oh yeah the yeah okay.
yeah.
but uh uh because that's what you were just doing with this ten per cent.
so i was just i just trying to understand that.
yeah.
sure.
so if we call a factor of just one just normalized to one the word error rate that you have for using t i digits as as training and t i digits as test
huh.
uhhuh.
uh different words i'm sure.
but but uh uh the same task and so on.
uhhuh.
if we call that one then what you're saying is that the word error rate for the same language but using uh different training data than you're testing on say timit and so forth it's one point one.
uhhuh.
yeah it's around one point one.
right.
and if it's
yeah.
you do go to three languages including the english it's something like one point three.
that's what you were just saying i think.
uh more actually.
one point four?
if i yeah.
so it's an additional thirty per cent?
what would you say?
around one point four.
yeah.
okay.
and if you exclude english from this combination what's that?
if we exclude english um there is not much difference with the data.
with english.
so yeah.
aha!
that's interesting that's interesting.
do you see because uh
uh
so no that that's important.
so what what it's saying here is just that
yes.
there is a reduction in performance when you don't um have the when you don't have um
task data.
wait a minute.
the
huh.
no actually it's interesting.
so it's so when you go to a different task there's actually not so different.
it's when you went to these
so what's the difference between two and three?
between the one point one case and the one point four case?
i'm confused.
it's multilingual.
yeah.
the only difference it's is that it's multilingual.
um
because in both in both both of those cases you don't have the same task.
yeah.
yeah sure.
so is is the training data for the for this one point four case
does it include the training data for the one point one case?
uh yeah.
yeah a fraction of it.
a part of it.
yeah.
how how much bigger is it?
um it's two times.
yeah um
actually
yeah.
um the english data no the multilingual databases are two times the broad english data.
we just wanted to keep this well not too huge.
so
so it's two times.
but it includes the but it includes the broad english data.
i think so.
do you
uh yeah.
and the broad english data is what you got this one point one with.
so that's timit basically.
right?
yeah.
uhhuh.
so it's band limited timit.
uhhuh.
uhhuh.
this is all eight kilohertz sampling.
yeah.
right.
so you have band limited timit gave you uh as good as a result as using t i digits on a t i digits test.
okay.
huh.
um and um
but when you add in more training data but keep the neural net the same size it um performs worse on the t i digits.
okay now all of this is this is noisy t i digits i assume.
yup.
both training and test?
yeah.
okay.
um
okay well we we we may just need to uh
so i mean it's interesting that going to a different different task didn't seem to hurt us that much.
and going to a different language um
it doesn't seem to matter
the difference between three and four is not particularly great.
so that means that whether you have the language in or not is not such a big deal.
huh.
it sounds like um uh we may need to have more of uh things that are similar to a target language.
or i mean you have the same number of parameters in the neural net.
you haven't increased the size of the neural net.
and maybe there's just just not enough complexity to it to represent the increased variability in the in the training set.
that that could be
um so what about
so these are results with uh that you're describing now that they are pretty similar for the different features or or uh
uh let me check.
uh
yeah.
so this was for the p l p.
yeah.
um the
yeah.
for the p l p with j rasta the the we
this is quite the same tendency with a slight increase of the error rate uh if we go to to timit.
and then it's it gets worse with the multilingual.
um yeah.
there there is a difference actually with between p l p and j rasta.
is that j rasta seems to perform better with the highly mismatched condition but slightly slightly worse for the well matched condition.
huh.
i have a suggestion actually.
even though it'll delay us slightly.
would would you mind running into the other room and making copies of this?
because we're all sort of
yeah yeah.
if we if we could look at it while we're talking i think it'd be
okay.
uh uh i'll i'll sing a song or dance or something while you do it too.
all right.
so um
go ahead.
uh while you're gone i'll ask some of my questions.
yeah.
um
yeah.
uh this way and just slightly to the left.
yeah.
the um what was was this number forty?
or it was roughly the same as this one he said?
um
when you had the two language versus the three language.
that's what he was saying.
that's where he removed english.
yeah.
right?
right.
it sometimes actually depends on what features you're using.
yeah.
but but it sounds like
um but he uhhuh.
i mean that's interesting.
because it it seems like what it's saying is not so much that you got hurt uh because you uh didn't have so much representation of english.
because in the other case you don't get hurt any more.
at least when it seemed like uh it it might simply be a case that you have something that is just much more diverse.
uhhuh.
but you have the same number of parameters representing it.
uhhuh.
i wonder.
were um all three of these nets using the same output?
this multi language uh labelling labelling?
he was using uh sixty four phonemes from sampa.
okay okay.
yeah.
so this would from this you would say well it doesn't really matter if we put finnish into the training of the neural net if there's going to be you know finnish in the test data.
right?
well it's it sounds i mean we have to be careful.
because we haven't gotten a good result yet.
yeah.
and comparing different bad results can be tricky.
huh.
but i i i i think it does suggest that it's not so much uh uh cross language as cross type of speech.
uhhuh.
it's it's uh
um but we did
oh yeah.
the other thing i was asking him though is that i think that in the case
yeah.
you you do have to be careful.
because of compounded results.
i think we got some earlier results in which you trained on one language and tested on another.
and you didn't have three but you just had one language.
so you trained on one type of digits and tested on another.
wasn't there something of that?
where you say you trained on spanish and tested on on t i digits?
or the other way around?
something like that?
no.
i thought there was something like that that he showed me last week.
we'll have to wait till he gets back.
until we get
yeah that would be interesting.
um this may have been what i was asking before stephane.
but but um wasn't there something that you did where you trained on one language and tested on another?
i mean no no mixture but just
i'll get it for you.
uh no no.
we've never just trained on one
training on a single language you mean?
and testing on the other one?
yeah.
not yet.
uh no.
so the only task that's similar to this is the training on two languages.
and that
but we've done a bunch of things where we just trained on one language.
right?
i mean you haven't you haven't done all your tests on multiple languages.
uh no.
either this is test with uh the same language but from the broad data.
or it's test with uh different languages.
also from the broad data.
excluding the
the early experiment that
so it's it's three or three and four.
did you do different languages from digits?
uh no.
you mean training digits on one language and using the net to recognize on the other?
digits on another language.
no.
see i thought you showed me something like that last week.
you had a you had a little
uh no i don't think so.
um what
these numbers are uh ratio to baseline?
so i mean what's the
so
this this chart this table that we're looking at is um is all testing for t i digits?
or
bigger is worse.
so you have uh basically two uh parts.
this is error rate i think.
ratio.
no no.
the upper part is for t i digits.
yeah yeah yeah.
and it's divided in three rows of four four rows each.
uhhuh.
yeah.
and the first four rows is well matched.
then the the second group of four rows is mismatched.
and finally highly mismatched.
and then the lower part is for italian.
and it's the same the same thing.
so so the upper part is training t i digits?
so it's it's the h t k results i mean.
so it's h t k training testings with different kind of features.
uh.
and what appears in the uh left column is the networks that are used for doing this.
huh.
so uh yeah.
well what was is that what was it that you had done last week when you showed
do you remember?
when you showed me the your table last week.
it it was part of these results.
huh.
huh.
so where is the baseline for the t i digits located in here?
you mean the h t k aurora baseline?
yeah.
it's uh the one hundred number.
it's well all these numbers are the ratio with respect to the baseline.
uh!
uh okay okay.
so this is word word error rate.
so a high number is bad.
yeah this is a word error rate ratio.
so it's yeah.
yeah.
okay i see.
so seventy point two means that we reduced the error rate uh by thirty thirty per cent.
so
okay okay gotcha.
huh.
okay so if we take
uh um
let's see.
p l p uh with online normalization and delta l
so that's this thing you have circled here in the second column.
yeah.
um and multi english refers to what?
to timit.
huh.
then you have uh m f m s and m e.
which are for french spanish and english.
and
yeah.
actually i i uh forgot to say that the multilingual net are trained on uh features without the derivatives.
uh but with increased frame numbers.
huh.
and we can we can see on the first line of the table that it it it's slightly slightly worse when we don't use data.
delta but it's not not that much.
right.
so so i'm sorry i missed that.
what's m f m s and m e?
multi french.
so multi french multi spanish and multi english.
multi spanish.
uh okay.
so it's uh broader vocabulary.
yeah.
then and
okay.
so i think what i'm what i saw in your smaller chart that i was thinking of was was there were some numbers i saw i think that included these multiple languages.
and the it and i was seeing that it got worse.
i i think that was all it was.
you had some very limited results that at that point.
yeah.
which showed having in these these other languages.
in fact it might have been just this last category having two languages broad that were where where english was removed.
so that was cross language.
and the and the result was quite poor.
what i we hadn't seen yet was that if you added in the english it's still poor.
yeah.
uh um now what's the noise condition um of the training data?
still poor.
well i think this is what you were explaining.
the noise condition is the same.
it's the same.
uh aurora noises uh in all these cases for the training.
yeah yeah.
so there's not a statistical a strong statistically different noise characteristic between uh the training and test.
no these are the same noises.
and yet we're seeing some kind of effect
yeah.
at least at least for the first for the well matched
well matched condition.
yeah.
right.
so there's some kind of a a an effect from having these uh this broader coverage.
um
now i guess what we should try doing with this is try testing these on this same sort of thing.
on
you probably must have this lined up to do.
to try the same with the exact same training do testing on the other languages.
huh.
on on um
so um
oh i well wait a minute.
you have this here for the italian.
that's right.
yeah.
okay.
so so
yeah so for the italian the results are stranger.
uh um huh
so what appears is that perhaps spanish is not very close to italian.
because uh well when using the the network trained only on spanish it's the error rate is almost uh twice the baseline error rate.
uhhuh.
huh uh.
well i mean let's see.
is there any difference in
so it's in the uh
so you're saying that when you train on english and uh and and test on
yeah.
no you don't have training on english testing.
there there is another difference is that the noise the noises are different.
well for for the italian part i mean.
in in what?
the uh the um networks are trained with noise from aurora t i digits.
aurora two.
huh.
yeah.
and the noise is different in
and perhaps the noise are quite different from the noises in the speech that italian.
do we have any um test sets uh in any other language that um have the same noise as in the aurora?
and
huh no.
no.
can i ask something real quick?
in in the upper part in the english stuff it looks like the very best number is sixty point nine.
and that's in the uh the third section in the upper part under p l p j rasta.
sort of the middle column.
yeah.
is that a noisy condition?
yeah.
so that's matched training.
is that what that is?
it's
no.
the third part.
so it's uh highly mismatched.
so training and test noise are different.
so why do you get your best number in
wouldn't you get your best number in the clean case?
well it's relative to the um baseline mismatching.
yeah.
uh!
yeah yeah.
okay.
so these are not
okay.
all right i see.
yeah.
okay.
and then so in the in the um in the non mismatched clean case your best one was under m f c c?
that sixty one point four?
yeah but it's not a clean case.
it's a noisy case.
but uh training and test noises are the same.
oh so this upper third
so yeah.
uh that's still noisy?
yeah.
uh okay.
so it's always noisy basically.
uhhuh.
and well the
i see.
huh.
okay.
um so uh
i think this will take some looking at thinking about.
but what is uh what is currently running that's uh that just filling in the holes here?
or or pretty much?
uh no we don't plan to fill the holes.
but actually there is something important.
okay.
is that um we made a lot of assumption concerning the online normalization.
and we just noticed uh recently that uh the approach that we were using was not uh leading to very good results when we used the straight features to h t k.
um huh.
so basically if you look at the at the left of the table the first uh row with eighty six one hundred and forty three and seventy five
these are the results we obtained for italian uh with straight huh p l p features using online normalization.
uhhuh.
huh and the huh what's in the table just at the left of the p l p twelve online normalization column.
so the numbers seventy nine fifty four and uh forty two are the results obtained by uh pratibha with uh his online normalization it's uh error her online normalization approach.
where is that?
seventy nine fifty?
uh it's just sort of sitting right on the uh the column line.
fifty one?
so
this
uh yeah.
oh i see okay.
just uh
yeah.
so these are the results of o g i with online normalization.
and straight features to h t k.
and the previous result eighty six and so on are with our features straight to h t k.
yes.
yes.
so what we see that is there is that um uh the way we were doing this was not correct.
but still the networks are very good.
when we use the networks our number are better that uh pratibha results.
we improve.
so do you know what was wrong with the online normalization or?
yeah.
there were there were different things.
and basically the first thing is the huh alpha uh value.
so the recursion uh part
um i used point five per cent which was the default value in the in the programs here.
and pratibha used five per cent.
uh uhhuh.
so it adapts more quickly.
yes.
yeah.
um but
yeah.
i assume that this was not important because uh previous results from from dan and show that basically the both both values give the same same uh results.
it was true on uh as t i digits but it's not true on italian.
uhhuh.
uh second thing is the initialization of the stuff.
actually uh what we were doing is to start the recursion from the beginning of the utterance.
and using initial values that are the global mean and variances measured across the whole database.
right.
right.
and pratibha did something different is.
that he uh she initialed the um values of the mean and variance by computing this on the twenty five first frames of each utterance.
huh there were other minor differences.
the fact that she used fifteen dissities instead instead of thirteen.
and that she used c zero instead of log energy.
uh but the main difference is differences concerns the recursion.
so uh i changed the code.
uh and now we have a baseline that's similar to the o g i baseline.
okay.
we it it's slightly uh different.
because i don't exactly initialize the same way she does.
actually i start huh i don't wait to a fifteen twenty five twenty five frames before computing a mean and the variance to to to start the recursion.
uhhuh.
yeah.
i i use the online scheme.
and only start the recursion after the twenty five twenty fifth frame.
but well it's similar.
so uh i retrained the networks with these.
well the the the networks are retaining with these new features.
uhhuh.
and
yeah.
okay.
so basically what i expect is that these numbers will a little bit go down.
but perhaps not not so much.
right.
because i think the neural networks learn perhaps to
to even if the features are not normalized it it will learn how to normalize.
right.
and uh
okay but i think that given the pressure of time we probably want to draw because of that especially we want to draw some conclusions from this.
do some reductions in what we're looking at.
yeah.
and make some strong decisions for what we're going to do testing on before next week.
yeah i i'd
so do you are you did you have something going on on the side with uh multi band or on on this?
no.
or
i we plan to start this.
uh so actually we have discussed uh um these.
what we could do more as a as a research.
and and we were thinking perhaps that uh the way we use the tandem is not
uh well there is basically perhaps a flaw in the in the the stuff.
because we trained the networks
if we trained the networks on the on a language and a or a specific task
uhhuh.
um what we ask is to the network is to put the bound the decision boundaries somewhere in the space.
huh.
and uh huh and ask the network to put one at one side of the for for a particular phoneme at one side of the boundary decision boundary.
and one for another phoneme at the other side.
and so there is kind of reduction of the information there that's not correct.
because if we change task and if the phonemes are not in the same context in the new task obviously the decision boundaries are not should not be at the same place.
but the way the feature gives the the way the network gives the features is that it reduce completely the it removes completely the information a lot of information from the the features by uh uh placing the decision boundaries at optimal places for one kind of data.
i
but this is not the case for another kind of data.
it's a trade off.
so
right?
anyway go ahead.
yeah.
so uh we were thinking about is perhaps um one way to solve this problem is increase the number of outputs of the neural networks.
doing something like um um phonemes within context and
well basically context dependent phonemes.
maybe.
i mean i i think you could make the same argument.
it'd be just as legitimate for hybrid systems as well.
yeah but we know that
right?
and in fact things get better with context dependent versions.
right?
yeah but here it's something different.
we want to have features.
uh well um
yeah.
yeah but it's still true that what you're doing is
you're ignoring
you're you're coming up with something to represent whether it's a distribution part of the probability distribution or features.
you're coming up with a set of variables that are representing uh things that vary over context.
uhhuh.
uh and you're putting it all together.
ignoring the differences in context.
that that's true for the hybrid system.
it's true for a tandem system.
so for that reason when you in in in a hybrid system when you incorporate context one way or another you do get better scores.
yeah.
okay.
but i it's it's a big deal to get that.
i i'm i'm sort of
and once you the other thing is that once you represent start representing more and more context it is uh much more um specific to a particular task in language.
so uh the the acoustics associated with uh a particular context
for instance you may have some kinds of contexts that will never occur in one language and will occur frequently in the other.
so the
the issue of getting enough training for a particular kind of context becomes harder.
we already actually don't have a huge amount of training data.
um
yeah but huh i mean the
the way we we do it now is that we have a neural network.
and basically the network is trained almost to give binary decisions.
right.
and uh binary decisions about phonemes.
nnn uh it's
almost.
but i mean it it it does give a distribution.
yeah.
it's and and it is true that if there's two phones that are very similar that uh the it may prefer one but it will give a reasonably high value to the other too.
yeah.
yeah sure.
but
uh so basically it's almost binary decisions.
and um the idea of using more classes is to get something that's less binary decisions.
oh no.
but it would still be even more of a binary decision.
it it'd be even more of one.
because then you would say that in that this phone in this context is a one but the same phone in a slightly different context is a zero.
but yeah but
that would be even even more distinct of a binary decision.
i actually would have thought you'd want to go the other way and have fewer classes.
yeah but if
uh i mean for instance the the thing i was arguing for before but again which i don't think we have time to try is something in which you would modify the code so you could train to have several outputs on and use articulatory features.
huh.
uhhuh.
because then that would that would go that would be much broader and cover many different situations.
but if you go to very very fine categories it's very binary.
huh.
yeah but i think
yeah perhaps you're right.
but you have more classes.
so you you have more information in your features.
so um you have more information in the uh posterior spectrum.
uhhuh.
true.
posteriors vector
um
which means that
but still the information is relevant.
uhhuh.
because it's it's information that helps to discriminate.
uhhuh.
if it's possible to be able to discriminate amongst uh among the phonemes in context.
well it's
but the
it's it's an interesting thought.
i mean we we could disagree about it at length.
huh.
huh.
but the the real thing is if you're interested in it you'll probably try it.
and and we'll see.
but but what i'm more concerned with now as an operational level is uh you know
huh.
what do we do in four or five days?
uh and so we have to be concerned with
are we going to look at any combinations of things?
you know once the nets get retrained so you have this problem out of it.
huh.
um are we going to look at multi band?
are we going to look at combinations of things?
uh what questions are we going to ask?
uh now that
i mean we should probably turn shortly to this o g i note.
um how are we going to combine with what they've been focusing on?
uh uh we haven't been doing any of the l d a rasta sort of thing.
uhhuh.
and they although they don't talk about it in this note.
um there's um the issue of the um new mu law business uh versus the logarithm.
uhhuh.
um so
so what what is going on right now?
what's
right.
you've got nets retraining.
are there is there are there any h t k trainings testings going on?
i i i'm trying the h t k with uh p l p twelve online delta delta and m s g filter together.
the combination.
i see.
the combination yeah.
but i haven't result at this moment.
m s g and and p l p.
yeah.
yeah.
and is this with the revised online normalization?
uh with the old old older.
yeah.
old one.
yeah.
so it's using all the nets for that.
but again we have the hope that it we have the hope that it maybe it's not making too much difference.
yeah.
but we can
know soon.
maybe.
but but
yeah.
i don't know.
yeah.
uh okay.
uh so there is this combination.
yeah.
working on combination obviously.
uhhuh.
um i will start work on multi band.
and we plan to work also on the idea of using both features and net outputs.
yup.
um and we think that with this approach perhaps we could reduce the number of outputs of the neural network.
um so get simpler networks.
because we still have the features.
so we have um come up with um different kind of broad phonetic categories.
and we have basically we have three types of broad phonetic classes.
well something using place of articulation.
which which leads to nine i think broad classes.
uh another which is based on manner.
which is is also something like nine classes.
and then something that combine both.
and we have twenty twenty five?
twenty seven.
twenty seven broad classes.
so like uh
oh i don't know.
like back vowels front vowels
so what you do
um for the moments we do not don't have nets.
um um i just want to understand.
so you have two net or three nets?
was this
how many how many nets do you have?
i mean it's just were we just changing the labels to retrain nets with fewer outputs.
no nets.
begin to work in this.
we are
right.
and then
but but i didn't understand
uhhuh.
uh the software currently just has uh a allows for i think the one one hot output.
so you're having multiple nets and combining them?
or
uh how are you how are you coming up with
if you say uh if you have a place characteristic and a manner characteristic how do you
it it's the single net.
i think they have one output.
yeah.
oh it's just one net.
yeah.
uhhuh.
it's one net with um twenty seven outputs.
if we have twenty seven classes.
i see.
yeah.
i see okay.
so it's well it's basically a standard net with fewer classes.
so you're sort of going the other way of what you were saying a bit ago.
instead of
yeah.
yeah.
but i think
yeah.
but including the features.
including the features yeah.
yeah.
i don't think this will work alone.
i think it will get worse.
because well i believe the effect that of of too reducing too much the information is basically basically what happens.
uhhuh.
and
but you think if you include that plus the other features
but
yeah.
because there is perhaps one important thing that the net brings.
and o g i showed that this is the distinction between speech and silence.
because these nets are trained on well controlled condition.
i mean the labels are obtained on clean speech and we add noise after.
so this is one thing.
and
but perhaps something intermediary using also some broad classes could could bring so much more information.
uh
so so again then we have these broad classes.
and well somewhat broad.
i mean it's twenty seven instead of sixty four basically.
yeah.
and you have the original features.
which are p l p or something.
yeah.
uhhuh.
and then uh just to remind me all of that goes into uh that all of that is transformed by uh uh k k l or something or?
there will probably be
mu.
yeah.
one single k l to transform everything.
or uh
right.
no.
transform the p l p.
and only transform the other.
i'm not sure.
this is still something that
well no.
i think
yeah.
we don't know
i see.
so there's a question of whether you would
two it's one.
yeah.
right.
whether you would transform together or just one.
yeah.
might want to try it bother both ways.
but that's interesting.
so that's something that you're you haven't trained yet but are preparing to train and
yeah.
yeah.
um yeah.
huh.
so i think hynek will be here monday.
on their
monday or tuesday.
so
uh yeah.
so i think you know we need to choose the choose the experiments carefully.
so we can get uh key key questions answered uh before then.
uhhuh.
and leave other ones aside even if it leave leaves incomplete tables some place.
someplace uh uh it's it's really time to time to choose.
uhhuh.
um let me pass this out by the way.
um these are
did did did i interrupt you?
yeah i have one.
were there other things that you wanted to
uh no i don't think so.
yeah i have one.
oh thanks.
uh okay okay we have lots of them.
we have one.
okay so um something i asked
so they're they're doing the the v a d.
i guess they mean voice activity detection so again it's the silence.
so they've just trained up a net which has two outputs i believe.
um i asked uh hynek whether
i haven't talked to sunil.
i asked hynek whether they compared that to just taking the nets we already had and summing up the probabilities.
uhhuh.
uh to get the speech voice activity detection or else just using the silence if there's only one silence output.
um and he didn't think they had.
um but on the other hand maybe they can get by with a smaller net.
then and maybe sometimes you don't run the other.
maybe there's a computational advantage to having a separate net anyway.
uhhuh.
so um their uh the results look pretty good.
yeah.
um i mean not uniformly.
i mean there's a an example or two that you can find where it made it slightly worse but uh in in all but a couple examples.
huh.
uh
but they have a question of the result.
um how are trained the the l d a filter?
how obtained the l d a filter?
huh.
i'm sorry.
i don't understand your question.
yes.
um the l d a filter needs some training set to obtain the filter.
maybe
i don't know exactly how they are obtained.
it's on training.
training with the training test of each
you understand me?
no.
yeah.
uh for example l d a filter need a set of a set of training to obtain the filter.
yes.
and maybe for the italian for the t d t e on for finnish these filter are are obtained with the um chinese their own training set.
yes i don't know.
that's that's so that's a that's a very good question.
then now that it i understand it it's
yeah.
where does the l d a come from?
in the in earlier experiments they had taken l d a from a completely different database.
yeah.
right?
yeah because maybe it the same situation that the neural network training with their own
huh.
set.
so that's a good question.
where does it come from?
yeah i don't know.
um but uh to tell you the truth i wasn't actually looking at the l d a so much when i i was looking at it.
i was mostly thinking about the the v a d.
and um it it
oh what does what does a s p
oh that's
the features.
yeah.
yeah.
i don't understand also.
it says baseline a s p.
what this
is what is the difference between a s p and uh over?
yeah i don't know.
a s p.
this is
oh.
anybody know any
there it is.
um because there's baseline aurora above it.
uhhuh.
and it's this is mostly better than baseline.
although in some cases it's a little worse in a couple cases.
well it says baseline a s p is twenty three mill minus thirteen.
yeah.
yeah it says what it is.
but i don't how that's different from
from the baseline okay.
i think this was i think this is the same point we were at when when we were up in oregon.
yeah.
i think i think it's the c zero using c zero instead of log energy.
uh okay uhhuh.
yeah it's this.
yeah.
oh okay.
it should be that.
yeah.
because
shouldn't it be
they they say in here that the v a d is not used as an additional feature.
does does anybody know how they're using it?
yeah so so what they're doing here is
yeah.
if you look down at the block diagram um they estimate they get a they get an estimate of whether it's speech or silence.
but that
and then they have a median filter of it.
uhhuh.
and so um basically they're trying to find stretches.
the median filter is enforcing a it having some continuity.
uhhuh.
you find stretches where the combination of the frame wise v a d and the the median filter say that there's a stretch of silence.
and then it's going through and just throwing the data away.
huh.
right?
so um
so it's it's
i don't understand.
you mean it's throwing out frames?
before
it's throwing out chunks of frames yeah.
there's the the median filter is enforcing that it's not going to be single cases of frames or isolated frames.
yeah.
so it's throwing out frames.
and the thing is um what i don't understand is how they're doing this with h t k.
yeah.
this is
that's what i was just going to ask.
how can you just throw out frames?
yeah.
well you you can.
i
right?
i mean you you
yeah.
it stretches again.
for single frames i think it would be pretty hard.
yeah.
but if you say speech starts here speech ends there.
uhhuh.
right?
huh.
yeah.
yeah you can basically remove the the frames from the feature feature files.
yeah.
yeah so i mean in the in the in the decoding you're saying that we're going to decode from here to here.
i
uhhuh.
i think they're they're they're treating it you know like uh
well it's not isolated word.
but but connected.
you know the the
in the text they say that this this is a tentative block diagram of a possible configuration we could think of.
so that sort of sounds like they're not doing that yet.
well no they they have numbers though.
right?
so i think they're they're doing something like that.
i think that they're they're
i think what i mean by that is they're trying to come up with a block diagram that's plausible for the standard.
in other words it's uh
i mean from the point of view of of uh reducing the number of bits you have to transmit it's not a bad idea to detect silence anyway.
yeah yeah.
um
i'm just wondering what exactly did they do up in this table if it wasn't this.
but it's the thing is it's that that that that's that's i i
certainly it would be tricky about it in in transmitting voice uh is uh for listening to is that these kinds of things uh cut speech off a lot.
right?
uhhuh.
and so um
plus it's going to introduce delays.
it does introduce delays.
but they're claiming that it's it's uh within the the boundaries of it.
huh.
and the l d a introduces delays.
and what he's suggesting this here is a parallel path.
so that it doesn't introduce uh anymore any more delay.
it introduces two hundred milliseconds of delay.
but at the same time the l d a down here.
i don't know
what's the difference between t l d a and s l d a?
temporal and spectral.
uh thank you.
temporal l d a.
yeah you would know that.
yeah
so um the temporal l d a does in fact include the same.
so that i think he well by by saying this is a a tentative block diagram i think means if you construct it this way this this delay would work in that way.
and then it'd be okay.
uh.
they they clearly did actually remove silent sections.
in order because they got these word error rate results.
so um i think that it's it's nice to do that in this.
because in fact it's going to give a better word error result.
and therefore will help within an evaluation.
whereas to whether this would actually be in a final standard i don't know.
um uh as you know part of the problem with evaluation right now is that the word models are pretty bad.
and nobody wants has has approached improving them.
so it's possible that a lot of the problems with so many insertions and so forth would go away if they were better word models to begin with.
so this might just be a temporary thing.
but but on the other hand and maybe maybe it's a decent idea.
so um the question we're going to want to go through next week when hynek shows up i guess is given that we've been
if you look at what we've been trying
we're uh looking at uh
by then i guess combinations of features and multi band.
uh and we've been looking at cross language cross task issues.
and they've been not so much looking at the cross task uh multiple language issues that
but they've been looking at uh at these issues.
at the online normalization and the uh voice activity detection.
and i guess when he comes here we're going to have to start deciding about um what do we choose from what we've looked at to um blend with some group of things in what they've looked at?
and once we choose that how do we split up the effort?
uh because we still have even once we choose we've still got uh another month or so.
i mean there's holidays in the way but but uh i think the evaluation data comes january thirty first.
so there's still a fair amount of time to do things together.
it's just that they probably should be somewhat more coherent between the two sites in that that amount of time.
when they removed the silence frames did they insert some kind of a marker so that the recognizer knows it's knows when it's time to back trace or something?
well see they
i i think they're
um i don't know the the specifics of how they're doing it.
they're they're getting around the way the recognizer works because they're not allowed to um change the scripts for the recognizer i believe.
oh right.
so um
maybe they're just inserting some nummy frames or something?
uh you know that's what i had thought.
but i don't i don't think they are.
huh.
i mean that's sort of what the way i had imagined would happen is that on the other side
yeah.
you put some low level noise or something.
probably don't want all zeros.
most recognizers don't like zeros.
huh.
but but you know put some epsilon in or some
yeah.
sorry.
epsilon random variable in or something.
some constant vector.
i mean
maybe not a constant but it doesn't uh don't like to divide by the variance of that.
or something
but i mean it's
that's right.
but something that
what i mean is something that is very distinguishable from speech.
uhhuh.
so that the the silence model in h t k will always pick it up.
yeah.
so i i that's what i thought they would do.
or else uh uh maybe there is some indicator to tell it to start and stop.
i don't know.
huh.
but whatever they did i mean they have to play within the rules of this specific evaluation.
yeah.
we we can find out.
because you got to do something otherwise if it's just a bunch of speech stuck together
no they're
yeah.
it would do badly.
and it didn't so badly.
yeah right.
right?
so they did something.
yeah yeah.
yeah.
uh so okay.
so i think this brings me up to date a bit.
it hopefully brings other people up to date a bit.
and um um i think uh i want to look at these numbers off line a little bit.
and think about it and and talk with everybody uh outside of this meeting.
um but uh
no i mean it sounds like i mean there there there are the usual number of of little little problems and bugs and so forth.
but it sounds like they're getting ironed out.
and now we're seem to be kind of in a position to actually uh look at stuff and and and compare things.
so i think that's that's pretty good.
um i don't know what the
one of the things i wonder about coming back to the first results you talked about is is how much uh things could be helped by more parameters.
and uh and uh how many more parameters we can afford to have in terms of the uh computational limits.
because and anyway when we go to twice as much data and have the same number of parameters
particularly when it's twice as much data and it's quite diverse.
um i wonder if having twice as many parameters would help.
uhhuh.
uh just have a bigger hidden layer.
uh but
i doubt it would help by forty per cent.
but but uh
yeah.
just curious.
how are we doing on the resources?
disk and
i think we're all right.
um not much problems with that.
okay.
computation?
it's okay.
well this table took uh more than five days to get back.
we
yeah.
yeah well
but
yeah.
are were you folks using gin?
that's a that just died.
you know?
huh no.
you were using gin perhaps?
yeah.
no.
no.
no no.
oh that's good.
it just died.
okay.
yeah we're going to get a replacement server that'll be a faster server actually.
yes.
huh uhhuh.
that'll be it's a seven hundred fifty megahertz uh sun.
tonic.
uh but it won't be installed for a little while.
go ahead.
do we do we have that big new i b m machine?
the i think in
we have the little tiny i b m machine that might someday grow up to be a big i b m machine.
it's got slots for eight.
uh i b m was donating five.
i think we only got two so far.
processors.
we had originally hoped we were getting eight hundred megahertz processors.
they ended up being five fifty.
so instead of having eight processors that were eight hundred megahertz we ended up with two that are five hundred and fifty megahertz.
and more are supposed to come soon.
and there's only a moderate amount of of memory.
so i don't think anybody has been sufficiently excited by it to spend much time um uh with it.
but uh hopefully they'll get us some more parts soon.
and
uh yeah i think that'll be
once we get it populated that'll be a nice machine.
i mean we will ultimately get eight processors in there.
and uh and uh a nice amount of memory.
uh so it'll be a pretty fast linux machine.
and if we can do things on linux some of the machines we have going already like swede.
uhhuh.
um it seems pretty fast.
uhhuh.
but i think fudge is pretty fast too.
yeah i mean you can check with uh dave johnson.
i mean it it's i think the machine is just sitting there.
and it does have two processors.
you know?
and somebody could do you know uh check out uh the multi threading libraries.
and i mean it's possible that the
and i mean i guess the prudent thing to do would be for somebody to do the work on on getting our code running on that machine with two processors even though there aren't five or eight.
there's there's there's going to be debugging hassles.
and then we'd be set for when we did have five or eight to have it really be useful.
but notice how i said somebody in
and turned my head your direction.
that's one thing you don't get in these recordings.
you don't get the don't get the visuals.
but
is it um um the neural network trainings that are um us down or the h t k runs that are slowing us down?
uh i think yes.
uh isn't that right?
i mean i think you're you're sort of held up by both.
right?
if the if the neural net trainings were a hundred times faster you still wouldn't be anything.
running through these a hundred times faster.
because you'd be stuck by the h t k trainings.
huh.
yeah.
right?
but if the h t k
i mean i think they're both it sounded like they were roughly equal.
is that about right?
yeah.
yeah.
because um think that'll be running linux.
and swede and fudge are already running linux.
so um could try to get um the neural network trainings or the h t k stuff running under linux and to start with i'm wondering which one i should pick first.
uh probably the neural net.
because it's probably it it's it's um
well i i don't know.
they both
h t k we use for uh um this aurora stuff.
um um i think it's not clear yet what we're going to use for trainings.
uh
well there's the trainings.
uh is it the training that takes the time or the decoding?
uh is it about equal between the two?
for for aurora?
for h t k?
for
yeah for the aurora?
uh training is longer.
okay.
yeah.
okay.
well i don't know how we can
i don't know how to
do we have h t k source?
is that
huh.
yeah.
you would think that would fairly trivially
the training would anyway.
the testing uh i don't i don't think would parallelize all that well.
but i think that you could certainly do um distributed sort of uh
no it's the each individual sentence.
is pretty tricky to parallelize.
but you could split up the sentences in a test set.
they have a they have a thing for doing that.
and they have for awhile in h t k.
yeah.
and you can parallelize the training.
and run it on several machines.
aha!
and it just basically keeps counts.
and there's something a final thing that you run.
and it accumulates all the counts together.
huh.
i see.
i don't what their scripts are set up to do for the aurora stuff.
but
yeah.
something that we haven't really settled on yet is other than this aurora stuff uh what do we do large vocabulary training slash testing for uh tandem systems?
because we hadn't really done much with tandem systems for larger stuff.
because we had this one collaboration with c m u and we used sphinx.
uh we're also going to be collaborating with s r i.
and we have their have theirs.
um so i don't know.
um so i i think the the advantage of going with the neural net thing is that we're going to use the neural net trainings no matter what.
okay.
for a lot of the things we're doing.
whereas exactly which h m m gaussian-mixture-based h m m thing we use is going to depend uh
so with that maybe we should uh go to our digit recitation task.
and it's about eleven fifty.
canned.
uh i can i can start over here.
so everybody everybody's on?
yeah.
so you guys had a a meeting with uh with hynek which i unfortunately had to miss.
um
huh.
and uh somebody
uh and uh
i guess chuck you weren't there either.
so uh
i was there.
oh you were there.
with hynek?
yeah.
yeah.
so everybody knows what happened except me.
maybe somebody should tell me.
oh yeah.
all right.
well uh first we discussed about some of the points that i was addressing in the mail i sent last week.
uhhuh.
so
yeah.
about the um well the downsampling problem.
yeah.
uh and about the the length of the filters.
and
yeah.
what was the what was the downsampling problem again?
so we had
i forget.
so the fact that there there is no uh low pass filtering before the downsampling.
well.
uhhuh.
there is because there is l d a filtering but that's perhaps not uh the best.
depends what it's frequency characteristic is yeah.
well.
uhhuh.
is the system on?
so you could do a you could do a stricter one.
maybe.
yeah.
yeah.
so we discussed about this about the um
was there any conclusion about that?
uh try it.
yeah.
i see.
i guess.
uh
yeah.
so again this is this is the downsampling uh of the uh the feature vector stream.
and
um yeah i guess the the uh l d a filters they were doing do have
um uh let's see so the the the feature vectors are calculated every ten milliseconds.
so
uh the question is how far down they are at fifty fifty hertz.
yeah.
uh um
uhhuh.
sorry at twenty five hertz since they're downsampling by two.
so does anybody know what the frequency characteristic is?
we don't have yet.
um so yeah.
oh okay.
we should have a look first at perhaps the modulation spectrum.
okay.
yeah.
um so there is this
there is the um length of the filters.
um so the this idea of trying to find filters with shorter delays.
um we started to work with this.
huh huh.
huh.
and the third point um was the um
yeah the online normalization where
well the recursion recursion for the mean estimation is a filter with some kind of delay.
yeah.
and that's not taken into account right now.
um yeah.
and there again yeah.
for this the conclusion of hynek was well we can try it but
uhhuh.
um
try try what?
so try to um um take into account the delay of the recursion for the mean estimation.
okay.
huh.
and this we've not uh worked on this yet.
um yeah.
and so while discussing about these these l d a filters some issues appeared like
well the fact that if we look at the frequency response of these filters it's
uh well we don't know really what's the important part in the frequency response.
and there is the fact that in the very low frequency these filters don't don't really remove a lot compared to the to the uh standard rasta filter.
uh and that's probably a reason why yeah online normalization helps because it it
right.
yeah it removed this mean.
um yeah but perhaps everything could should be could be in the filter.
i mean uh the the mean normalization and
yeah.
so
yeah.
so basically that was that's all we discussed about.
we discussed about good things to do.
also uh well generally good stuff to do for the research.
uhhuh.
and this was this l d a uh tuning perhaps and hynek proposed again to his uh traps.
so
okay.
yeah.
um
i mean i i guess the key thing for me is is figuring out how to better coordinate between the two sides.
uhhuh.
because because um
uh i was talking with hynek about it later.
and the the sort of had the sense sort of that that neither group of people wanted to to bother the other group too much.
and and i don't think anybody is you know closed in in their thinking or are unwilling to talk about things.
but i think that you were sort of waiting for them to tell you that they had something for you.
and and that and expected that they would do certain things.
uhhuh.
and they were they didn't want to bother you.
and they were sort of waiting for you.
and and and uh we ended up with this thing where they they were filling up all of the possible latency themselves.
and they just hadn't thought of that.
so
uh i mean it's true that maybe maybe no one really thought about that that this latency thing would be such a a strict issue.
yeah.
well but.
yeah.
yeah.
well
in in uh the other
yeah.
i don't know what happened really but
yeah.
i guess it's it's also so uh the time constraints.
because well we discussed about that about this problem.
and they told us well we will do all that's possible to have enough space for a network.
but then yeah perhaps they were too short with the time and
then they couldn't.
i see.
uh yeah.
but there was also problem perhaps a problem of communication.
so yeah.
now we will try to
just talk more.
yeah slikes and send mails.
uh so
yeah.
yeah.
yeah.
uh
okay.
so there's um
all right.
well maybe we should just
uh i mean you're you're other than that you folks are busy doing all the all the things that you're trying that we talked about before.
right?
and this machines are busy.
and you're busy.
yeah.
and
basically.
yeah.
okay.
um
oh.
let's let's i mean i think that as as we said before that one of the things that we're imagining is that uh there there will be uh in the system we end up with there'll be something to explicitly uh uh do something about noise.
uh.
in addition to the uh other things that we're talking about.
and that's probably the best thing to do.
and there was that one email that said that it sounded like uh uh things looked very promising up there.
in terms of uh i think they were using ericsson's approach or something.
and in addition to
they're doing some noise removal thing.
right?
yeah yeah.
so yeah we're will start to do this also.
yeah.
uh so carmen is just looking at the ericsson ericsson code.
yeah.
we
uhhuh.
yeah i modified it.
and
well modify no.
i studied barry's sim code more or less.
to take only the first step the spectral subtraction.
and we have some the feature for italian database.
and we will try with this feature with the filter to find the result.
uhhuh.
uhhuh.
but we haven't result until this moment.
yeah sure.
but well we are working in this also.
yeah.
and maybe try another type of spectral subtraction i don't
when you say you don't have a result yet you mean it's it's just that it's in process?
or that you it finished and it didn't get a good result?
no.
no no we have we have do the experiment.
only have the feature the feature but the experiment have
yeah.
we have not make the experiment.
oh.
and
okay.
maybe will be good result or bad result.
we don't know.
yeah.
yeah.
yeah.
okay.
so um i suggest actually now we we we sort of move on and and hear what's what's what's happening in in other areas.
like what's what's happening with your investigations about echos and so on.
oh um
well um i haven't started writing the test yet i'm meeting with adam today.
uhhuh.
um and he's going show me the scripts he has for um running recognition on meeting recorder digits.
uhhuh.
uh i also um haven't got the code yet.
i haven't asked hynek for for the for his code yet.
because i looked at uh avendano's thesis and i don't really understand what he's doing yet.
but it it it sounded like um the channel normalization part um of his thesis um was done in a a bit of i don't know what the word is a a bit of a rough way.
um it sounded like he um
he he it it wasn't really fleshed out.
and maybe he did something that was interesting for the test situation.
but i i'm not sure if it's what i'd want to use.
so i have to i have to read it more.
i don't really understand what he's doing yet.
okay.
it's my
yeah i haven't read it in a while so i'm not going to be too much help unless i read it again.
i know this is mine here.
oh yeah.
so
okay.
um the um
so you and then you're also going to be doing this echo cancelling between the the close mounted and the and the the the what we're calling a cheating experiment uh of sorts.
uh i'm
between the distant
right.
well or i'm hoping i'm hoping espen will do it.
um
uh.
okay.
um
delegate.
that's good.
it's good to delegate.
i i think he's at least planning to do it for the close mike cross talk.
and so maybe i can just take whatever setup he has and use it.
great.
great.
yeah actually um he should uh
i wonder who else is
i think maybe it's dan ellis is going to be doing uh a different cancellation.
um one of the things that people working in the meeting task want to get at is they would like to have cleaner close miked recordings.
so uh this is especially true for the lapel.
but even for the close close miked uh cases.
um we'd like to be able to have um other sounds from other people and so forth removed from
so when someone isn't speaking you'd like the part where they're not speaking to actually be
so what they're talking about doing is using uh echo cancellation like techniques.
it's not really echo but uh just um uh
taking the input from other mikes and using uh uh a uh an adaptive filtering approach to remove the effect of that uh other speech.
so
um what was it?
there was there was some some some point where uh uh eric or somebody was was speaking and he had lots of silence in his channel.
and i was saying something to somebody else uh which was in the background.
and it was not
it was recognizing my words which were the background speech on the close close mike.
huh.
oh the what we talked about yesterday?
yes.
yeah that was actually my
i was wearing the i was wearing the lapel and you were sitting next to me.
oh you.
it was you i was
yeah.
and i only said one thing but you were talking and it was picking up all your words.
yeah.
yeah.
so they would like clean channels.
uh and for that huh uh that purpose uh they'd like to pull it out.
so i think i think dan ellis or somebody who was working with him was going to uh work on that.
so
okay.
right?
um and uh i don't know if we've talked lately about the the plans you're developing that we talked about this morning.
uh i don't remember if we talked about that last week or not.
but maybe just a quick reprise of of what we were saying this morning.
okay.
um so continuing to um extend
uh
what about the stuff that um mirjam has been doing?
and and shawn yeah.
oh.
so they're training up nets to try to recognize these acoustic features?
i see.
but that's uh uh all that's is a a certainly relevant uh study.
and you know what are the features that they're finding?
we have this problem with the overloading of the term feature.
so
yeah.
uh what are the variables?
what we're calling this one?
what are the variables that they're found finding useful?
huh.
um
and their their targets are based on canonical mappings of phones to acoustic features.
for
right.
and that's certainly one thing to do and we're going to try and do something more more fine than that.
but uh
um
so
um
so i guess you know what i was trying to remember some of the things we were saying.
do you still have that
oh yeah.
yeah.
there's those that
uh
yeah some of some of the issues we were talking about was in just getting a good handle on on uh what good features are.
and
what does what did um larry saul use for it was the sonorant uh detector?
right?
how did he
how did he do that?
what was his detector?
uhhuh.
uhhuh.
oh okay.
uhhuh.
so how did he combine all these features?
what what huh classifier did he use?
huh.
oh right.
you were talking about that yeah.
i see.
and the other thing you were talking about is is is where we get the targets from.
so i mean there's these issues of what are the what are the variables that you use.
and do you combine them using the soft and or or you do something you know more complicated.
um and then the other thing was so where do you get the targets from.
the initial thing is just the obvious that we're discussing is starting up with phone labels from somewhere and then uh doing the transformation.
but then the other thing is to do something better.
and uh why don't you tell us again about this this database?
this is the
huh!
and then tell them to talk naturally?
yeah yeah.
pierced tongues and
yeah.
you could just mount it to that and they wouldn't even notice.
weld it.
zzz.
maybe you could go to these parlors and and you could you know you know have have you know reduced rates if you if you can do the measurements.
yeah.
i
that's right.
you could what you could do is you could sell little rings and stuff with embedded you know transmitters in them and things.
yeah.
and
yeah be cool and help science.
yeah.
okay.
huh!
there's a bunch of data that around
that people have done studies like that way way back.
right?
i mean i can't remember where uh wisconsin or someplace that used to have a big database of
yeah.
i remember there was this guy at a t and t.
randolph?
or what was his name?
do you remember that guy?
um researcher at a t and t a while back that was studying trying to do speech recognition from these kinds of features.
i can't remember what his name was.
dang.
now i'll think of it.
do you mean uh but you i mean
that's interesting.
well he was the guy the guy that was using
you mean when was was mark randolph there or
mark randolph.
yeah he's he's he's at motorola now.
oh is he?
oh okay.
yeah.
yeah.
yeah.
is it the guy that was using the pattern of pressure on the tongue?
or
i can't remember exactly what he was using now.
but i know i just remember it had to do with you know uh positional parameters.
what
yeah.
uhhuh.
and trying to you know do speech recognition based on them.
yeah.
so the only the only uh hesitation i had about it since i mean i haven't see the data is it sounds like it's it's continuous variables and a bunch of them.
huh.
and so
i don't know how complicated it is to go from there.
what you really want are these binary labels and just a few of them.
and maybe there's a trivial mapping if you want to do it.
and it's but it
i i i worry a little bit that this is a research project in itself.
whereas um if you did something instead that like um having some manual annotation by uh you know linguistics students.
this would there'd be a limited set of things that you could do as per our discussions with with john before.
uhhuh.
but the things that you could do like nasality and voicing and a couple other things you probably could do reasonably well.
uhhuh.
and then there would it would really be uh this uh uh binary variable.
course then that's the other question is do you want binary variables.
so
i mean the other thing you could do is boot trying to to uh get those binary variables.
and take the continuous variables from uh the uh uh the data itself there.
but i i'm not sure
could you cluster the just do some kind of clustering?
guess you could yeah.
bin them up into different categories and
yeah.
so anyway that's that's uh that's another whole direction that could be looked at.
um um i mean in general it's going to be for new data that you look at.
it's going to be hidden variable because we're not going to get everybody sitting in these meetings to wear the pellets and
right.
um so
right.
so you're talking about using that data to get
uh
instead of using canonical mappings of phones.
right.
so you'd use that data to give you sort of what the the true mappings are for each phone?
uhhuh.
i see.
uhhuh.
yeah.
so yeah where this fits into the rest in in my mind i guess is that um we're looking at different ways that we can combine uh different kinds of of front end representations um in order to get robustness under difficult or even you know typical conditions.
and part of it this robustness seems to come from uh multi stream or multi band sorts of things.
and saul seems to have a reasonable way of looking at it at least for one one um articulatory feature.
the question is is can we learn from that to change some of the other methods we have since
i mean one of the things that's nice about what he had i thought was that that it it um
the decision about how strongly to train the different pieces is based on uh a a reasonable criterion with hidden variables.
rather than um just assuming that you should train every detector uh with equal strength towards uh it being this phone or that phone.
huh.
right?
so it so um he's got these um uh uh
he and's between these different features.
it's a soft and i guess.
but in in principle you you want to get a strong concurrence of all the different things that indicate something.
and then he or's across the different soft or's across the different uh multi band channels.
and um the weight
yeah the target for the training of the and and'ed things is something that's kept uh as a hidden variable.
and is learned with e m.
so he doesn't have
whereas what we were doing is is uh taking the phone target and then just back propagating from that.
which means that it's it's uh it could be for instance that for a particular point in the data you don't want to um uh train a particular band train the detectors for a particular band.
you you want to ignore that band because that's a band is a noisy noisy measure.
uhhuh.
and we don't
we're we're still going to try to train it up.
in our scheme we're going to try to train it up to do as well well as it can at predicting.
uh maybe that's not the right thing to do.
so he doesn't have to have truth marks?
or
right.
and uh he doesn't have to have hard labels.
well at the at the tail end yeah he has to know what's where it's sonorant.
right.
for the full band.
but he's but what he's but what he's not training up uh what he doesn't depend on as truth is
um i guess one way of describing would be
if if a sound is sonorant is it sonorant in this band?
is it sonorant in that band?
is it sonorant in that band?
right.
it's hard to even answer that.
what you really mean is that the whole sound is sonorant.
uhhuh.
so
okay.
then it comes down to you know to what extent should you make use of information from particular band towards making your decision.
i see.
and um uh we're making in a sense sort of this hard decision that you should you should use everything uh with with uh equal strength.
and uh because in the ideal case we would be going for posterior probabilities.
if we had uh enough data to really get posterior probabilities.
and if the if we also had enough data so that it was representative of the test data.
then we would in fact be doing the right thing to train everything as hard as we can.
but um this is something that's more built up along an idea of robustness from from the beginning.
and so you don't necessarily want to train everything up towards the
so where did he get his uh his his uh high level targets about what's sonorant and what's not?
from uh canonical mappings um at first.
okay.
yeah.
and then it's unclear um uh
using timit?
or using
using timit.
right right.
uhhuh.
yeah.
and then uh he does some fine tuning um for um special cases.
yeah.
yeah.
i mean we we have a kind of iterative training because we do this embedded viterbi.
uh so there is something that's suggested based on the data.
but it's it's not
i think it doesn't seem like it's quite the same because of this.
because then whatever that alignment is it's that for all all bands.
uhhuh.
well no that's not quite right we did actually do them separate tried to do them separately.
so that would be a little more like what he did.
um
but it's still not quite the same because then it's it's um setting targets based on where you would say the sound begins in a particular band.
where he's this is not a labeling per se.
might be closer i guess if we did a soft soft target uh uh embedded neural net training like we've done a few times.
uh the forward um do the forward calculations to get the gammas and train on those.
huh.
uh what's next?
i could say a little bit about stuff i've been playing with.
oh.
i um
you're playing?
huh?
you're playing?
yes i'm playing.
um so i wanted to do this experiment to see um uh what happens if we try to uh improve the performance of the back end recognizer for the aurora task.
and see how that affects things.
and so i had this um i think i sent around last week a this plan i had for an experiment.
this matrix where i would take the um the original um the original system.
so there's the original system trained on the mel cepstral features.
and then and then uh optimize the h t k system and run that again.
so look at the difference there.
and then uh do the same thing for the i.c.s.i.o.g.i. front end.
what which test set was this?
this is
that i looked at?
uhhuh.
uh i'm looking at the italian right now.
uhhuh.
so as far as i've gotten is i've uh been able to go through from beginning to end the um full h t k system for the italian data.
and got the same results that um that uh stephane had.
so um i started looking to and now i'm i'm sort of at the point where i want to know what should i change in the h t k back end in order to try to uh to improve it.
so
one of the first things i thought of was the fact that they use the same number of states for all of the models.
uhhuh.
and so i went online and i uh found a pronunciation dictionary for italian digits.
uhhuh.
and just looked at you know the number of phones in each one of the digits.
um you know sort of the canonical way of setting up a an h m m system is that you use um three states per phone.
and um so then the the total number of states for a word would just be you know the number of phones times three.
and so when i did that for the italian digits i got a number of states ranging on the low end from nine to the high end eighteen.
um now you have to really add two to that because in h t k there's an initial null and a final null.
so when they use uh models that have eighteen states there're really sixteen states.
they've got those initial and final null states.
and so um their guess of eighteen states seems to be pretty well matched to the two longest words of the italian digits.
the four and five which um according to my you know sort of off the cuff calculation should have eighteen states each.
uhhuh.
and so they had sixteen.
so that's pretty close.
um but for the most of the words are much shorter.
so the majority of them want to have nine states.
and so theirs are sort of twice as long.
so my guess uh
and then if you i i printed out a confusion matrix um uh for the well matched case.
and it turns out that the longest words are actually the ones that do the best.
so my guess about what's happening is that you know if you assume a fixed the same amount of training data for each of these digits.
and a fixed length model for all of them.
but the actual words for some of them are half as long.
you really um have you know half as much training data for those models.
because if you have a long word and you're training it to eighteen states uh you've got you know you've got the same number of gaussians you've got to train in each case.
uhhuh.
but for the shorter words you know the total number of frames is actually half as many.
uhhuh.
so it could be that you know for the short words there's because you have so many states you just don't have enough data to train all those gaussians.
so um i'm going to try to um create more word specific um uh prototype h m m's to start training from.
yeah i mean it's not at all uncommon you do worse on long on short words than long words anyway just because you're accumulating more evidence for the for the longer word.
uhhuh.
but
yeah so i'll i'll the next experiment i'm going to try is to just um you know create uh models that seem to be more matched to my guess about how long they should be.
uhhuh.
and as part of that um i wanted to see sort of how the um
how these models were coming out you know what when we train up uh you know the model for one which wants to have nine states you know.
what is the uh what do the transition probabilities look like in the self loops look like in in those models?
and so i talked to andreas and he explained to me how you can calculate the expected duration of an h m m just by looking at the transition matrix.
uhhuh.
and so i wrote a little matlab script that calculates that.
and
so i'm going to sort of print those out for each of the words to see what's happening you know how these models are training up.
uhhuh.
you know the long ones versus the short ones.
uhhuh.
i i did
quickly i did the silence model
and and um
that's coming out with about one point two seconds as its average duration.
and the silence model's the one that's used at the beginning and the end of each of the string of digits.
wow!
lots of silence.
yeah yeah.
and so the s p model which is what they put in between digits i i haven't calculated that for that one yet.
but um
so they basically their their model for a whole digit string is silence digit s p digit s p blah blah blah and then silence at the end.
and so
are the s p's optional?
i mean skip them?
i have to look at that but i'm not sure that they are.
now the one thing about the s p model is really it only has a single emitting state to it.
uhhuh.
so if it's not optional you know it's it's not going to hurt a whole lot.
i see.
and it's tied to the center state of the silence model so it's not its own.
um it doesn't require its own training data.
uhhuh.
it just shares that state.
uhhuh.
so it i mean it's pretty good the way that they have it set up.
but um
so i want to play with that a little bit more.
i'm curious about looking at you know how these models have trained and looking at the expected durations of the models.
and i want to compare that in the the well matched case to the unmatched case and see if you can get an idea of
just from looking at the durations of these models you know what what's happening.
yeah i mean i think that uh as much as you can it's good to sort of not do anything really tricky.
uhhuh.
not do anything that's really finely tuned.
but just sort of uh you know you you
yeah.
the premise is kind of if you have a a good person look at this for a few weeks and what do you come up with.
huh.
uhhuh.
and uh
and hynek when i told him about this he had an interesting point.
and that was um the the final models that they end up training up have i think probably something on the order of six gaussians per state.
so they're fairly you know hefty models.
and hynek was saying that well probably in a real application you wouldn't have enough compute to handle models that are very big or complicated.
so in fact what we may want are simpler models.
could be.
and compare how they perform to that.
but you know it depends on what the actual application is.
and it's really hard to know what your limits are in terms of how many gaussians you can have.
right.
and that i mean at the moment that's not the limitation.
so
uhhuh.
i mean i i i what i thought you were going to say but which i was thinking was um
where did six come from?
probably it came from the same place eighteen came from.
yeah.
you know?
so
right.
uh that's another parameter.
right?
yeah yeah.
that that maybe you know uh you really want three or nine or
well one thing i mean if i if if i start um reducing the number of states for some of these shorter models that's going to reduce the total number of gaussians.
so in a sense it'll be a simpler system.
right.
yeah.
yeah.
but i think right now again the idea is doing just very simple things.
yeah.
how much better can you make it?
uhhuh.
and um since they're only simple things there's nothing that you're going to do that is going to blow up the amount of computation.
right.
um so
right.
if you found that nine was better than six that would be okay i think actually.
uhhuh.
yeah.
doesn't have to go down.
i really wasn't even going to play with that part of the system yet.
i was just going to change the the
uhhuh okay.
yeah just work with the models?
yeah just look at the length of the models and just see what happens.
yeah.
yeah.
so
cool.
okay.
so uh what's uh
i guess your plan for you you you guys' plan for the next next week is just continue on these these same things we've been talking about for aurora and
yeah.
i guess we can try to have some kind of new baseline for next week perhaps.
with all these minor things modified.
and then do other things.
play with the spectral subtraction.
and retry the m s g and things like that.
yeah.
yeah.
yeah we we have a big list.
big list?
you have a big list of of things to do.
so
well that's good.
i think that after all of this uh um confusion settles down in another
some point a little later next year there will be some sort of standard and it'll get out there.
and hopefully it'll have some effect from something that that has uh been done by our group of people.
but uh
even if it doesn't there's there's there'll be standards after that.
so
does anybody know how to um run matlab?
sort of in batch mode like you send it a bunch of commands to run and it gives you the output.
is it possible to do that?
i i think uh mike tried it.
yeah?
and he says it's impossible so he went to octave.
octave is the um unix clone of of matlab which you can batch.
octave.
uh.
okay.
great.
thanks.
yeah.
i was going crazy trying to do that.
huh.
yeah.
what is octave so?
what's that?
it's a free software?
uh octave?
yeah.
yeah it's it's it's free.
i think we have it here running somewhere.
great.
yeah.
um it's a little behind.
it's the same syntax but it's a little behind in that matlab went to these like um
you can have cells and you can you can uh implement object oriented type things with matlab.
uh octave doesn't do that yet.
so i think you
octave is kind of like matlab.
um four point something or
if it'll do like a lot of the basic matrix and vector stuff.
the basic stuff right.
that's perfect.
yeah.
great!
okay guess we're done.
okay.
well although by the way
