okay.
so uh
you can fill those out uh after actually.
so
so i got uh these results from uh stephane.
also um i think that uh um we might hear later today about other results.
i think that uh there were some other very good results that we're going to want to compare to.
but our results from other other places.
yeah.
i'm sorry?
i didn't
um i got this from you.
yeah.
and then i sent a note to sunil about because he has been running some other systems.
other than the the icsi o g i one.
uhhuh.
oh yeah.
so um i want to want to see what that is.
but uh you know so we'll see what it is comparatively later.
but it looks like um
yeah.
you know most of the time even i mean even though it's true that the overall number for danish we didn't improve it.
if you look at it individually what it really says is that there's um uh
looks like out of the six cases between the different kinds of uh matching conditions out of the six cases there's basically um a couple where it stays about the same.
uh three where it gets better and one where it gets worse.
yeah.
uh go ahead.
actually uh um for the danish there's still some kind of mystery.
because um um when we use the straight features we are not able to get these nice number.
with the icsi o g i one i mean.
we don't have this ninety three seventy eight we have
eighty nine forty four.
yeah.
uh so uh that's probably something wrong with the features that we get from o g i.
uh and sunil is working on on trying to to check everything.
oh.
and and we have a little time on that and actually
so
huh?
we have a little bit of time on that actually.
yeah.
we have a day or so.
so
when when when do you folks leave?
uh sunday.
sunday.
so
so uh
yeah.
until saturday midnight or something we have
we we have time.
yeah.
well that would be good.
that'd be good.
yeah.
yeah.
uh and you know whenever anybody figures it out they should also for sure email hynek.
because hynek will be over there telling people what we did.
so he should know.
huh.
good.
yeah.
okay.
so um
so we'll we'll hold off on that a little bit.
i mean even with these results as they are it's it's it's really not that bad.
but but uh um
and it looks like the overall result as they are now even without you know any any bugs being fixed is that uh on the the other tasks we had this average of uh uh nine percent or so improvement.
and here we have somewhat better than that than the danish and somewhat worse than that on the german.
but i mean it sounds like uh one way or another the methods that we're doing can reduce the error rate from from mel cepstrum down by you know a fourth of them to uh a half of them.
somewhere in there depending on the exact case.
so
so that's good.
i mean i think that uh one of the things that hynek was talking about was understanding what was in the other really good proposals.
and
and trying to see if what should ultimately be proposed is some uh combination of things.
um if uh
because there's things that they are doing there that we certainly are not doing.
and there's things that we're doing that they're not doing.
and and they all seem like good things.
yeah.
huh yeah.
so
so
how much how much better was the best system than ours?
well we don't know yet.
huh.
uh i mean first place there's still this thing to to work out.
and second place second thing is that the only results that we have so far from before were really development set results.
oh okay.
so i think in this community that's of interest.
it's not like everything is being pinned on the evaluation set.
but um for the development set our best result was a little bit short of fifty percent.
and the best result of any system was about fifty four.
where these numbers are the uh relative uh reduction in uh word error rate.
oh.
okay.
and um the other systems were uh somewhat lower than that.
there was actually there was much less of a huge range than there was in aurora one.
in aurora one there were there were systems that basically didn't improve things.
huh.
and here the the worst system still reduced the error rate by thirty three percent or something in development set.
oh wow!
so so you know sort of everybody is doing things between well roughly a third of the errors and half the errors being eliminated uh and varying on different test sets and so forth.
uhhuh.
so i think
um it's probably a good time to look at what's really going on and seeing if there's a there's a way to combine the best ideas.
while at the same time not blowing up the amount of uh resources used.
because that's that's critical for this this test.
do we know anything about
who's was it that had the lowest on the dev set?
um uh the uh there were two systems that were put forth by a combination of of uh french telecom and alcatel.
and um they they differed in some respects.
but they one was called the french telecom alcatel system the other was called the alcatel french telecom system uh which is the biggest difference i think.
but but there're there're there're some other differences too.
uh and and uh they both did very well.
uhhuh.
you know.
so um my impression is they also did very well on on the the uh evaluation set.
but um i i we haven't seen
you've you haven't seen any final results for that.
yeah?
and they used the main thing that that they used was spectral subtraction?
or
there is a couple pieces to it.
there's a spectral subtraction style piece it was basically you know wiener filtering.
and then then there was some some modification of the cepstral parameters where they
yeah.
actually something that's close to cepstral mean subtraction.
but uh the way the mean is adapted um it's signal dependent.
i'm uh
so basically the mean is adapted during speech and not during silence.
yeah.
but it's very close to to cepstral mean subtraction.
but some people have done exactly that sort of thing.
of of and the i mean it's not to to look in speech only to try to to measure these things during speech.
yeah.
yeah.
that's that's not that uncommon.
but it it so it looks like they did some some uh reasonable things.
uh and they're not things that we did precisely.
we did unreasonable things which because we like to try strange things.
and and uh and our things worked too.
huh.
and so um
uh it's possible that some combination of these different things that were done would be the best thing to do.
but the only caveat to that is that everybody's being real conscious of how much memory and how much c p u they're using.
because these uh standards are supposed to go on cell phones with moderate resources in both respects.
uhhuh.
did anybody uh do anything with the models as an experiment?
or
uh they didn't report it if they did.
nobody reported it?
yeah.
i think everybody was focused elsewhere.
um now one of the things that's nice about what we did is we do have a a uh a filtering which leads to a a uh a reduction in the bandwidth in the modulation spectrum which allows us to downsample.
so uh as a result of that we have a reduced um transmission rate for the bits.
uhhuh.
that was misreported the first time out.
it it said the same amount because for convenience sake in the particular way that this is being tested uh they were repeating the packets.
so it was they were they they had twenty four hundred bits per second but they were literally creating forty eight hundred bits per second um even though it was just repeated.
oh.
uhhuh.
right.
so uh in practice
so you could've had a repeat count in there or something.
well i mean this was just a phoney thing just to to fit into the the software that was testing the errors channel errors and so on.
oh.
so
oh.
so in reality if you put this this system into uh the field it would be twenty four hundred bits per second not forty eight hundred.
so
um so that's a nice feature of what what we did.
um
but um well we still have to see how it all comes out.
huh.
um and then there's the whole standards process which is another thing altogether.
when is the development set i mean the uh uh test set results due?
like the day before you leave or something?
uh probably the day after they leave.
but we'll have to we'll have to stop it the day before we leave.
yeah yeah.
so
huh.
i think i think the the meeting is on the thirteenth or something.
yeah.
this tuesday.
yeah.
and uh
they uh
right.
and the the uh results are due like the day before the meeting or something.
so
yeah.
probably.
well
i i think i i think they are.
yeah well
yeah.
so um since we have a bit farther to travel than some of the others uh we'll have to get done a little quicker.
but um i mean it's just tracing down these bugs.
i mean just exactly this sort of thing of you know why why these features seem to be behaving differently uh in california than in oregon.
uhuh.
huh.
might have something to do with electricity shortage.
uh we didn't we didn't have enough electrons here.
and
uh but um
uh i think you know the main reason for having
i mean it only takes to run the the two test sets in just in computer time is just a day or so.
right?
so
yeah.
it's very short interval.
yeah.
so i think the the whole reason for having as long as we have which was like a week and a half is is because of bugs like that.
so
huh.
so we're going to end up with these same kind of sheets that have the the percentages and so on just for the
yeah so there are two more columns in the sheets too.
oh i guess it's the same sheets.
yeah.
yeah.
it's the same sheets.
yeah.
just with the missing columns filled in.
yeah.
yeah.
yeah.
well that'll be good.
so i'll i'll disregard these numbers.
that's that's that's good.
so hynek will try to push for trying to combine uh different things?
or
uh well that's um
huh
yeah
i mean i think the question is is there is there some advantage.
i mean you could just take the best system and say that's the standard.
but the thing is that if different systems are getting at good things um again within the constraint of the resources if there's something simple that you can do.
now for instance uh it's i think very reasonable to have a standard for the terminal's side and then for the server's side say here's a number of things that could be done.
so um everything that we did could probably just be added on to what alcatel did.
and it'd probably work pretty well with them too.
so
um uh that's one one aspect of it.
and then on the terminal's side i don't know how much um memory and and c p u it takes but it seems like the filtering uh
i mean the v a d stuff they both had.
right?
and um so and they both had some kind of online normalization.
right?
uh yeah.
of sorts.
yeah?
so
so it seems like the main different there is the is the uh filtering.
and the filtering i think if you can
it shouldn't take a lot of memory to do that uh and i also wouldn't think the c p u uh would be much either for that part.
so if you can if you can add those in um then uh you can cut the data rate in half.
yeah.
so it seems like the right thing to do is to on the on the terminal's side take what they did if it if it does seem to generalize well to german and danish.
uh take what they did add in a filter and add in some stuff on the server's side.
and and and that's probably a reasonable standard.
um uh
they are working on this already?
because yeah sunil told me that he was trying already to put some kind of uh filtering in the france telecom.
yeah so that's that's that's what
that would be ideal would be is that they could you know they could actually show that in fact a combination of some sort uh would work even better than what what any of the systems had.
and um then it would it would uh be something to to discuss in the meeting.
but uh not clear what will go on.
um i mean on the one hand um sometimes people are just anxious to get a standard out there.
i mean you can always have another standard after that.
but this process has gone on for a while on already.
and and people might just want to pick something and say okay this is it.
and then that's a standard.
uh standards are always optional.
it's just that uh if you disobey them then you risk not being able to sell your product.
or uh um
and people often work on new standards while an old standard is in place and so on.
so it's not final even if they declared a standard.
the other hand they might just say they just don't know enough yet to to declare a standard.
so
you you you will you will become experts on this and know more far more than me about the this particular standards process once you you go to this meeting.
so
be interested in hearing.
so uh i'd be uh interested in hearing uh your thoughts now.
i mean you're almost done.
i mean you're done in the sense that um you may be able to get some new features from sunil and we'll re run it.
uh but other than that you're you're basically done.
right?
so uh i'm interested in hearing hearing your thoughts about where you think we should go from this.
yeah.
i mean we tried a lot of things in a hurry.
and uh if we can back off from this now and sort of take our time with something and not have doing things quickly be quite so much the constraint what what you think would be the best thing to do.
uh well
huh
well first uh to really have a look at at the speech from these databases.
because well we tried several thing.
but we did not really look at what's happening and where is the noise and
okay.
uh
it's a novel idea.
look at the data.
okay.
yeah.
or more generally i guess what what is causing the degradation.
yeah.
yeah.
actually there is one thing that well um generally we we think that most of the errors are within phoneme classes.
and
so i think it could be interesting to to see if it i don't think it's still true when we add noise.
and so we have i i guess the confusion the confusion matrices are very different when when we have noise and when it's clean speech.
and probably there is much more between classes errors for noisy speech.
uhhuh.
and so um
yeah so perhaps we could have a a large gain uh just by looking at improving the uh recognition not of phonemes but of phoneme classes simply.
uhhuh.
and which is a a a simpler problem perhaps but which is perhaps important for noisy speech.
the other thing that strikes me just looking at these numbers is just taking the best cases.
i mean some of these of course even with all of our our wonderful processing still are horrible kinds of numbers.
but just take the best case the well matched uh german case after uh well matched danish after we
uhhuh.
the kind of numbers we're getting are about eight or nine uh percent error per digit.
uhhuh.
yeah.
this is obviously not usable.
right?
no.
i mean if you have ten digits for a phone number i mean every now and then you'll get it right.
sure.
i mean it's it's uh um
so i mean the other thing is that uh
and and and and also um part of what's nice about this is that this is uh um a realistic almost realistic database.
i mean it's still not people who are really trying to accomplish something.
but but uh within the artificial setup it isn't noise artificially added you know simulated uh additive noise.
uhhuh.
it's real noise condition.
and um the the training the training i guess is always done on the close talking?
no.
actually actually the well matched condition is still quite still quite difficult.
no?
i mean it's they have all these data from the close mike and from the distant mike from different driving condition open window closed window.
yeah.
and they take all of this.
and they take seventy percent i think for training and thirty percent for testing.
uhhuh.
so training is done on different conditions and different microphones.
and testing also is done on different microphone and conditions.
so probably if we only take the close microphones i guess the results should be much much better than this.
i see.
huh.
oh okay.
that explains it partially.
uh
what about
in so the the
yeah.
so there is this the mismatched is um the same kind of thing.
go ahead.
but the driving conditions i mean the speed and the kind of road is different for training and testing.
is that right?
yeah.
and the last condition is close microphone for training and distant for testing.
yeah.
uh okay.
so
so so
i see.
so yeah.
so the high so the right so the highly mismatched case is in some sense a good model for what we've been you know typically talking about when we talk about additive noise in
and so and it does correspond to a realistic situation in the sense that um people might really be trying to uh call out telephone numbers or or something like that in in their cars.
yeah.
and they're trying to connect to something.
huh.
um
actually yeah it's very close to clean speech training because well because the close microphone and noisy speech testing.
yeah.
yeah.
yeah.
yeah.
huh.
and the well matched condition is what you might imagine that you might be able to approach if you know that this is the application.
you're going to record a bunch on people in cars and so forth and do these training.
and then uh when you sell it to somebody they will be a different person with a different car and so on.
so it's this is an somewhat optimistic view on it.
uh so you know the real thing is somewhere in between the two.
uh
yeah.
uh but
but the i mean the
even the optimistic one is
it doesn't work.
yeah.
right.
it
right.
it doesn't work.
so in a way that's you know that's sort of the dominant thing.
is that even say on the development set stuff that we saw the uh the numbers that uh that alcatel was getting when choosing out the best single numbers it was just you know it wasn't good enough for for a a for a real system.
huh.
uhhuh.
you you
um
so uh we still have stuff to do.
yeah.
uh and uh
i don't know.
so looking at the data where you know what's the what's what's what's characteristic.
yeah i think that's that's a good thing.
does you have any thoughts about what else you're thinking that you didn't get to that you would like to do if you had more time?
uh
oh a lot of those thing.
because we trying a lot of thing.
and we doesn't work we remove these.
maybe we trying again with the articulatory feature.
i don't know exactly.
because we tried we some one experiment that doesn't work.
um forgot it something i don't know exactly.
uhhuh.
because maybe do better some step the general uh diagram.
uhhuh.
i don't know exactly to think what we can improve.
yeah because a lot of time it's true.
there were a lot of times when we've tried something and it didn't work right away even though we had an intuition that there should be something there.
and so then we would just stop it.
um
and uh one of the things i don't remember the details on but i remember at some point when you were working with a second stream and you tried a low pass filtering to cepstrum in some case you got
m s g yeah.
well but it was an m s g like thing but it wasn't m s g.
right?
uh you i think in some case you got some little improvement.
but it was you know sort of a small improvement.
and it was a a big added complication so you dropped it.
but um that was just sort of one try.
right?
you just took one filter threw it there.
yeah.
right?
and it seems to me that um if that is an important idea which you know might be that one could work at it for a while as you're saying.
huh.
and uh
uh and you had you know you had the multi band things also and you know there was issue of that.
yeah.
uhhuh.
um barry's going to be uh continuing working on multi band things as well.
we were just talking about um some uh some work that we're interested in.
kind of inspired by the stuff by larry saul with the uh uh learning articulatory feature in i think in the case of his paper with sonorance based on uh multi band information where you have a a combination of gradient learning and uh e m.
uhhuh.
um and um
so i think that you know this is a uh this is a neat data set.
um and then uh as we mentioned before we also have the the new uh digit set coming up from recordings in this room.
so there's a lot of things to work with.
um
and uh
what i like about it in a way is that uh the results are still so terrible.
uh uh i mean they're much better than they were you know.
we're talking about thirty to sixty percent uh error rate reduction.
that's that's really great stuff to to do that in relatively short time.
but even after that it's still you know so poor that that uh no one could really use it.
so um
i think that's great that because and also because again it's not something
sometimes we've gotten terrible results by taking some data and artificially you know convolving it with some room response or something.
and we take a very
uh at one point uh brian and i went downstairs into the the basement where it was it was in a hallway where it was very reverberant.
and we we made some recordings there.
and then we we uh uh made a simulation of the of the room acoustics there and and applied it to other things.
uhhuh.
and uh
but it was all pretty artificial.
and and you know how often would you really try to have your most crucial conversations in this very reverberant hallway.
um so
uh this is what's nice about the aurora data and the data here is that is that it's sort of a realistic room situation uh acoustics acoustic situation both terms in noise and reflections and so on.
and and uh
uh with something that's still relatively realistic it's still very very hard to do very well.
so
yeah.
yeah.
so
well
actually this is that's why we
well it's a different kind of data.
we're not we're not used to work with this kind of data.
yeah.
that's why we should have a more closer look at what's going on.
uhhuh.
um
yeah.
so this would be the first thing.
and then of course try to well kind of debug what was wrong uh when we do aurora test on the m s g particularly and on the multi band.
yeah.
yeah.
yeah.
uh
yeah.
yeah.
no i i think there's lots of lots of good things to do with this.
so
um
so let's
i guess you were going to say something else?
oh okay.
what do you think?
about
anything.
about other experiments?
uh now i'm interested in um uh looking at the experiments where you use um uh data from multiple languages to train the neural net.
and i don't know how far or if you guys even had a chance to try that but that would be it'd be interesting to me.
yeah but
again it's the kind of of thing that uh we were thinking thinking that it would work but it didn't work.
and uh so there is kind of a of not a bug but something wrong in what we are doing perhaps.
yeah.
right.
right.
right.
uh something wrong perhaps in the just in the the fact that the labels are
well
uhhuh.
what worked best is the hand labeled data.
uhhuh.
um
uh so yeah.
i don't know if we can get some hand labeled data from other languages.
yeah.
it's not so easy to find.
right.
but that would be something interesting to to see.
yeah.
yeah.
yeah.
also uh i mean there was just the whole notion of having multiple nets that were trained on different data.
so one form of different data was is from different languages but the other
well in fact uh in those experiments it wasn't so much combining multiple nets it was a single net that had different
yeah.
so first thing is would it be better if they were multiple nets for some reason.
second thing is never mind the different languages just having acoustic conditions rather than training them all up in one.
would it be helpful to have different ones?
so um
that was a question that was kind of raised by mike shire's thesis and on in that case in terms of reverberation.
right?
that that sometimes it might be better to do that.
but um i don't think we know for sure.
so um
right.
so next week we uh won't meet because you'll be in europe.
when are you two getting back?
um i'm
you on friday or on saturday?
sunday.
or
because it's it's less expensive the price the price the ticket.
oh yeah sunday yeah.
yeah that's right.
oh.
you've got to have a saturday overnight.
right?
i'll be back on tuesday.
tuesday.
where's the meeting?
uh amsterdam i think.
yeah amsterdam.
yeah.
uhhuh.
yeah.
yeah yeah.
yep.
um so we'll skip next week and we'll meet two weeks from now.
and uh i guess the main topic will be uh you telling us what happened.
yeah.
yeah.
uh so
yeah.
well if we don't have anything else to discuss we should uh turn off the machine and then say the real nasty things.
should we do digits first?
oh yeah digits.
oh yeah digits.
yeah.
yeah.
good point.
yeah.
good thinking.
why don't you go ahead?
okay.
okay.
uh
somebody else should run this.
i'm sick of being the one to sort of go through and say well what do you think about this.
you want to
should we take turns?
yeah.
you want me to run it today?
yeah why don't you run it today.
okay.
okay.
okay.
um
let's see maybe we should just get a list of items.
things that we should talk about.
um
i guess there's the usual updates.
everybody going around and saying uh you know what they're working on.
the things that happened the last week.
but aside from that is there anything in particular that anybody wants to bring up?
huh.
for today.
no.
okay.
so why don't we just around and people can give updates.
oh.
uh do you want to start stephane?
all right.
um
well the first thing maybe is that the eurospeech paper is uh accepted.
um
yeah.
this is what what do you uh what's in the paper there?
so it's the paper that describe basically the um system that were proposed for the aurora.
the one that we we submitted the last round?
right yeah.
uhhuh.
yeah.
um yeah.
so and the fff comments seems from the reviewer are good.
so
huh.
huh
yeah.
where where's it going to be this year?
it's uh aalborg in denmark.
oh okay.
and it's
yeah.
september.
huh.
huh
yeah.
then uh whhh
well i've been working on on mainly on on line normalization this week.
uh i've been trying different slightly slightly different approaches.
um the first thing is trying to play a little bit again with the um time constant.
uh second thing is uh the training of uh on line normalization with two different means.
one mean for the silence and one for the speech.
um
and so i have two recursions which are controlled by the um probability of the voice activity detector.
huh
this actually don't doesn't seem to help.
although it doesn't hurt.
so
but well both on line normalization approach seems equivalent.
are the means pretty different for the two?
well they
yeah.
they can be very different.
yeah uhhuh
huh?
so do you maybe make errors in different places?
different kinds of errors.
i didn't look uh more closely.
um it might be yeah.
uhhuh.
um
well uh there is one thing that we can observe is that the mean are more different for for c zero and c one than for the other coefficients.
and
yeah.
and yeah it the c one is
there are strange strange thing happening with c one is that when you have different kind of noises the mean for the the silence portion is can be different.
huh.
and
so when you look at the trajectory of c one it's has a strange shape.
and
i was expecting the that these two mean helps.
especially because of the the strange c c one shape.
uh which can like you can have um a trajectory for the speech.
and then when you are in the silence it goes somewhere.
but if the noise is different it goes somewhere else.
oh.
so which would mean that if we estimate the mean based on all the signal even though we have frame dropping but we don't frame uh drop everything.
but uh this can hurts the estimation of the mean for speech.
and
huh but i still have to investigate further i think.
um a third thing is um that instead of having a fixed time constant i try to have a time constant that's smaller at the beginning of the utterances.
to adapt more quickly to the something that's closer to the right mean.
um
yeah.
and then this time constant increases.
and i have a threshold that
uhhuh.
well if it's higher than a certain threshold i keep it to this threshold to still uh adapt um the mean when if the utterance is uh long enough to to continue to adapt after like one second.
uhhuh.
or
uhhuh.
huh
uh well this doesn't help neither.
but this doesn't hurt.
so well.
it seems pretty
wasn't there some experiment you were going to try?
where you did something differently for each um uh i don't know whether it was each mel band or each uh um f f t bin or
there was something you were going to uh some parameter you were going to vary depending on the frequency.
i don't know if that was
i guess it was
i don't know.
no.
maybe it's this this idea of having different on line normalization um tunings for the different m f c c's.
for each uh
uhhuh.
but
uhhuh.
yeah.
i i thought morgan you brought it up a couple meetings ago.
and then it was something about uh
and then somebody said yeah it does seem like you know c zero is the one that's you know the major one or uh
i can't remember exactly what it was now.
huh.
yeah there
uh actually yeah.
um it's very important to normalize c zero.
and much less to normalize the other coefficients.
and um
uh well at least with the current on line normalization scheme.
and
we i think we kind of know that normalizing c one doesn't help with the current scheme.
and
and yeah.
in my idea i i was thinking that the the the reason is maybe because of these funny things that happen between speech and silence which have different means.
um
yeah.
but maybe it's not so so easy to
um
i really would like to suggest looking um a little bit at the kinds of errors.
i know you can get lost in that and go forever and not see too much but sometimes.
uhhuh.
but but um
just seeing that each of these things didn't make things better may not be enough.
it may be that they're making them better in some ways and worse in others.
yeah.
uhhuh.
or increasing insertions and decreasing deletions.
or
or um
um
you know helping with noisy case.
but hurting in quiet case.
and if you saw that then maybe you it would something would occur to you of how to deal with that.
uhhuh.
uhhuh.
huh.
all right.
huh.
yeah.
um
so that's it i think for the on line normalization.
um
yeah i've been playing a little bit with some kind of thresholding.
and
huh
as a first experiment i think
yeah.
well what i did is is to take um to measure the average
no the maximum energy of each utterance.
and then put a threshold
well this for each mel band.
then put a threshold that's fifteen d b below
well uh a couple of d b below this maximum.
uhhuh.
huh.
and
actually it was not a threshold.
it was just adding noise.
uhhuh.
so i was adding a white noise energy.
uh that's fifteen d b below the maximum energy of the utterance.
and
yeah.
when we look at at the um m f c c that result from this they are a lot more smoother.
um
when we compare like a channel zero and channel one utterance
um so a clean and uh the same noisy utterance.
well there is almost no difference between the cepstral coefficients of the two.
huh?
um
and yeah.
and the result that we have in term of speech recognition actually it's not it's not worse.
it's not better neither.
huh.
but it's um kind of surprising that it's not worse.
because basically you add noise that's fifteen d b just fifteen d b below the maximum energy.
sorry.
and
so why does that smooth things out?
at least
i don't i don't understand that.
well there's less difference right?
it's i think it's whitening this the portion that are more silent.
because it's
as you add a white noise that are has a very high energy it whitens everything.
huh oh okay.
and
and the high energy portion of the speech don't get much affected anyway by the other noise.
and as the noise you add is the same is the shape it's also the same.
huh?
yeah.
so they have the trajectory are very very similar.
and and
so i mean again if you trained in one kind of noise and tested in the same kind of noise you'd you know given enough training data you don't do do badly.
the reason that we that we have the problems we have is because it's different in training and test.
even if the general kind is the same the exact instances are different.
uhhuh.
and and
so when you whiten it then it's like you the the only noise to to first order the only noise that you have is white noise.
and you've added the same thing to training and test.
uhhuh.
so it's
huh?
uh
so would that be similar to like doing the smoothing then over time?
or
uhhuh.
well it's a kind of smoothing.
i think it's i think it's different.
but
it's it's something that yeah that affects more or less the silence portions.
because
uhhuh.
well anyway the the portion of speech that have high energy are not a lot affected by the noises in the aurora database.
uhhuh.
if if you compare the two shut channels of speechdat car during speech portion it's the m f c c are not very different.
they are very different when energy's lower.
like during fricatives or during speech pauses.
and
uh
yeah but you're still getting more recognition errors.
which means that the differences even though they look like they're not so big are are hurting your recognition.
right?
yeah so it distort the speech.
right.
um
yeah.
so performance went down?
no.
it didn't.
oh.
but
yeah.
so but in this case i i really expect that maybe the the two these two stream of features they are very different.
i mean and maybe we could gain something by combining them.
well the other thing is that you just picked one particular way of doing it.
or
uh i mean first place it's fifteen d b uh down across the utterance.
and maybe you'd want to have something that was a little more adaptive.
secondly you happened to pick fifteen d b.
huh.
and maybe twenty would be better.
yeah.
or or twelve.
yeah right.
so what was the what was the threshold part of it?
was the threshold uh how far down?
yeah.
well he yeah he had to figure out how much to add.
so he was looking he was looking at the peak value.
uhhuh.
right?
uhhuh.
and then
and and so what's
i don't understand.
how does it go?
if it if if the peak value's above some threshold then you add the noise?
or if it's below
i systematically add the noise.
but the um noise level is just some kind of threshold below the peak.
oh oh i see.
huh
i see.
um
yeah.
yeah.
which is not really noise actually.
it's just adding a constant to each of the mel uh energy.
uhhuh.
to each of the mel filter bank.
yeah.
i see.
so yeah it's really uh white noise.
i
uhhuh.
yeah.
so then afterwards a log is taken.
and that's sort of why the the little variation tends to go away.
uhhuh.
um
yeah so
well the this threshold is still a factor that we have to look at.
and i don't know maybe a constant noise addition would would be fine also.
or
um
or or not constant but but uh varying over time in fact is another way to go.
uhhuh.
uhhuh.
um
yeah.
um
were you using the the normalization in addition to this?
i mean what was the rest of the system?
um
yeah it was it was uh the same system.
uhhuh.
okay.
it was the same system.
huh
oh yeah.
a third thing is that um i play a little bit with the um finding what was different between um
and there were a couple of differences.
like the l d a filters were not the same.
um
he had the france telecom blind equalization in the system.
um
the number of m f c c that was were used was different.
you used thirteen.
and we used fifteen.
well a bunch of differences.
and um actually the result that he he got were much better on t i digits especially.
so i'm kind of investigated to see what was the main factor for this difference.
and it seems that the l d a filter is is was hurting.
um so when we put some noise compensation the um l d a filter that that's derived from noisy speech is not more anymore optimal.
and it makes a big difference um on t i digits.
trained on clean.
uh if we use the the old l d a filter i mean the l d a filter that was in the proposal we have like eighty two point seven percent recognition rate.
um
on noisy speech when the system is trained on clean speech.
but
and when we use the filter that's derived from clean speech we jumped.
so from eighty two point seven to eighty five point one.
which is a huge leap.
uhhuh.
um
yeah.
so now the results are more similar.
and
i don't i will not i think investigate on the other differences.
which is like the number of m f c c that we keep and other small things.
that we can i think optimize later on anyway.
sure.
but on the other hand if everybody is trying different kinds of noise suppression things and so forth it might be good to standardize on the piece that we're not changing.
right?
so if there's any particular reason to pick one or the other
i mean
which which one is closer to what the proposal was that was submitted to aurora?
are they
they both
well i mean
i think
yeah i think uh the new system that i tested is i guess closer.
because it doesn't have it have less of of france telecom stuff.
you mean the
i
the whatever you uh tested with recently right?
huh?
yeah.
yeah?
well no i i'm i
yeah you're trying to add in france telecom.
but we
tell them about the rest of it.
like you said the number of filters might be different or something.
right?
the number of cepstral coefficients is what?
or
uhhuh.
yeah.
so i mean i think we'd want to standardize there.
wouldn't we?
yeah yeah.
so you guys should pick something.
yeah.
and
yeah.
well all all three of you.
i think we were going to work with with this or this new system.
or with
uh so the the right now the the system that is there in the what we have in the repositories with uses fifteen.
so
right.
yeah.
yeah.
so yeah.
so yep.
but we will use the the l d a filters derived from clean speech.
yeah yeah.
well yeah actually it's it's not the the l d a filter.
so
it's something that's also short enough in in latency.
yeah.
well.
so
yeah.
so we haven't we have been always using uh fifteen coefficients.
not thirteen.
yeah.
uhhuh.
yeah.
well uh that's something's
um
yeah.
then
huh
i think as long as you guys agree on it it doesn't matter.
i think we have a maximum of sixty uh features that we're allowed.
so
yeah.
yeah maybe we can i mean at least
um i'll run some experiments to see whether once i have this noise compensation to see whether thirteen and fifteen really matters or not.
uhhuh.
uhhuh.
never tested it with the compensation.
but without uh compensation it was like fifteen was slightly better than thirteen.
yeah.
so that's why we stuck to thirteen.
yeah.
and there is there is also this log energy versus c zero.
sorry.
fifteen.
yeah the log energy versus c zero.
well.
uh that's that's the other thing.
if if
i mean without noise compensation certainly c zero is better than log energy.
i mean because the there are more uh mismatched conditions than the matching conditions for testing.
uhhuh.
you know.
always for the matched condition you always get a slightly better performance for log energy than c zero.
uhhuh.
but not for
i mean for matched and the clean condition both you get log energy.
i mean you get a better performance with log energy.
uhhuh.
well um maybe once we have this noise compensation i don't know we have to try that also whether we want to go for c zero or log energy.
uhhuh.
we can see that.
yeah.
huh
huh.
so do you have more stephane?
or
uh that's it i think.
huh
do you have anything morgan?
or
uh
no i'm just you know being a manager this week.
so
how about you barry?
um still working on my my quals preparation stuff.
um so i'm i'm thinking about um starting some uh cheating experiments to uh determine the um the relative effectiveness of um some intermediate categories that i want to classify.
so for example um if i know where voicing occurs and everything um i would do a phone um phone recognition experiment.
um somehow putting in the the uh the perfect knowledge that i have about voicing.
so um in particular i was thinking um in in the hybrid framework just taking those l n a files and um setting to zero those probabilities that um that these phones are not voicing.
so say like i know this particular segment is voicing.
uhhuh.
um i would say uh go into the corresponding l n a file and zonk out the the posteriors for um those phonemes that um are not voiced.
uhhuh.
and then see what kinds of improvements i get.
huh?
and so this would be a useful thing um to know in terms of like which which um which of these categories are are good for um speech recognition.
uhhuh.
so that's
i hope to get those uh those experiments done by by the time quals come come around in july.
so do you just take the probabilities of the other ones and spread them out evenly among the the remaining ones?
yeah.
i i i was thinking
okay so just set to set to some really low number the the non voiced um phones.
uhhuh.
right?
and then renormalize.
huh.
right.
cool.
uhhuh.
yeah.
that will be really interesting to see.
you know?
so then you're going to feed the those into some standard recognizer?
uhhuh.
uh are you going to do digits?
yeah.
or
um well i'm going to work with timit.
with timit okay.
timit uh phone recognition with timit.
uhhuh.
and um
oh so then you'll feed those
sorry.
so where do the outputs of the net go into if you're doing phone recognition?
oh.
um the outputs of the net go into the standard um icsi hybrid um recognizer.
so maybe um chronos.
and you're going to the you're going to do phone recognition with that?
or
phone recognition.
okay okay.
right right.
i see.
so
and uh another thing would be to extend this to uh digits or something where i can look at whole words.
uhhuh.
and i would be able to see uh not just like phoneme events but um inter phoneme events.
uhhuh.
so like this is from a stop to to a a vocalic.
segment.
you know?
something that is transitional in nature.
right.
cool.
yeah.
great.
uh
so that's that's it.
okay.
um
yeah.
let's see i haven't done a whole lot on anything related to this this week.
i've been focusing mainly on meeting recorder stuff.
oh.
so um i guess i'll just pass it on to dave.
uh okay.
well in my lunch talk last week i i said i'd tried phase normalization and gotten garbage results using that um long term mean subtraction approach.
it turned out there was a bug in my matlab code.
so i tried it again.
um
and um the results were were better.
i got intelligible speech back.
but they still weren't as good as just subtracting the magnitude the log magnitude means.
and also i've been talking to um andreas and thilo about the um smartkom language model.
and about coming up with a good model for um far mike use of the smartkom system.
so
i'm going to be working on um implementing this mean subtraction approach in the far mike system.
for the smartkom system i mean.
and um
one of the experiments we're going to do is um we're going to um train the a broadcast news net.
which is because that's what we've been using so far.
and um adapt it on some other data um andreas wants to use.
um
data that resembles read speech.
like these digit readings.
because he feels that the smartkom system interaction is not going to be exactly conversational.
uhhuh.
so actually i was wondering how long does it take to train that broadcast news net?
the big one takes a while.
yeah that takes two three weeks.
two three weeks.
so but you know uh you can get
i don't know if you even want to run the big one uh um in the in the final system.
because you know it takes a little while to run it.
so um you can scale it down by
i'm sorry.
it was two three weeks for training up for the large broadcast news test set training set.
oh.
i don't know how much you'd be training on.
okay.
the full.
uh so if you trained on half as much and made the net uh uh half as big then it would be one fourth the amount of time.
okay.
and it'd be nearly as good.
so
okay.
yeah.
also i guess we had we've had these uh little discussions.
i guess you haven't had a chance to work with it too much.
about about uh uh uh other ways of taking care of the phase.
uhhuh.
so i mean i i guess that was something i could say would be that we've talked a little bit about.
you just doing it all with complex arithmetic.
and uh
and not not uh doing the polar representation with magnitude and phase.
but it looks like there's ways that one could potentially just work with the complex numbers and and and in principle get rid of the effects of the average complex spectrum.
but
and um
actually regarding the phase normalization
so i did two experiments.
and one is
so phases get added modulo two pi.
and because you only know the phase of the complex number to a value modulo two pi.
and so i thought at first um that uh what i should do is unwrap the phase.
because that will undo that.
um but i actually got worse results doing that unwrapping using the simple phase unwrapper that's in matlab than i did not unwrapping at all.
huh?
uhhuh.
yeah.
so.
and that's all i have to say.
huh.
yeah.
so i'm i'm still hopeful that that
i mean we we don't even know if the phase is something the average phase is something that we do want to remove.
i mean maybe there's some deeper reason why it isn't the right thing to do.
but um
at least in principle it looks like there's there's uh a couple potential ways to do it.
one one being to just work with the complex numbers.
um
and uh in rectangular kind of coordinates.
and the other is to uh do a taylor series.
well.
so you work with the complex numbers.
and then when you get the spectrum the average complex spectrum um actually divide it out.
um as opposed to taking the log and subtracting.
so then
um
um
you know there might be some numerical issues.
we don't really know that.
the other thing we talked a little bit about was taylor series expansion.
and um
uh actually i was talking to dick karp about it a little bit.
and and and since i got thinking about it.
and and uh
so one thing is that you'd have to do i think
uh
we may have to do this on a whiteboard.
but i think you have to be a little careful about scaling the numbers that you're taking the complex numbers that you're taking the log of.
because the taylor expansion for it has you know a square and a cube and and so forth.
and and so if if you have a a number that is modulus you know uh very different from one it should be right around one.
if it's
because it's a expansion of log one.
one minus epsilon.
or is is one plus epsilon.
or is it one plus
well there's an epsilon squared over two.
okay.
and an epsilon cubed over three.
and so forth.
so if epsilon is bigger than one then it diverges.
oh.
so you have to do some scaling.
but that's not a big deal.
because it's the log of of k times a complex number.
then you can just that's the same as log of k plus log of the complex number.
oh.
okay.
uh
so there's
converges.
but
huh.
okay.
how about you sunil?
so um i've been uh implementing this uh wiener filtering for this aurora task.
and uh
i i actually thought it was it was doing fine when i tested it once.
it's like using a small section of the code.
and then i ran the whole recognition experiment with italian.
and i got like worse results than not using it.
then i
so i've been trying to find where the problem came from.
and then it looks like i have some problem in the way.
there is some some very silly bug somewhere.
and ugh!
i i mean uh it actually it actually made the whole thing worse.
i was looking at the spectrograms that i got.
and it's like it's it's very horrible.
like when i
i i missed the
i'm sorry.
i was i was distracted.
i missed the very first sentence.
so then i'm a little lost on the rest.
oh i mean
what what what
oh yeah.
i actually implemented the wiener filtering as a module and then tested it out separately.
yeah i see.
oh okay.
and it it it gave like i just got the signal out.
and it it was okay.
so i plugged it in somewhere.
and then i mean it's like i had to remove some part.
and then plugging it in somewhere.
and then i in that process i messed it up somewhere.
so
okay.
so it was
i mean i thought it was all fine.
and then i ran it and i got something worse than not using it.
so
i was like i'm trying to find where the problem came.
uhhuh.
and it seems to be like somewhere.
okay.
some silly stuff.
and um the other thing uh was uh uh
well hynek showed up one suddenly on one day.
and then i was talking
right.
yeah.
as as he is wont to do.
yeah.
uh yeah.
so i was actually that day i was thinking about doing something about the wiener filtering and then carlos matter of stuff.
and then he showed up.
and then i told him.
and then he gave me a whole bunch of filters.
what carlos used for his uh uh thesis.
and then that was something which came up.
and then um
so uh i'm actually uh thinking of using that also in this uh wiener filtering.
because that is a modified wiener filtering approach.
where instead of using the current frame it uses adjacent frames also in designing the wiener filter.
so instead of designing our own new wiener filters i may just use one of those carlos filters in in this implementation.
uhhuh.
and see whether it it actually gives me something better.
than using just the current current frame.
which is in a way uh something like the smoothing the wiener filter.
uhhuh.
but
so i don't know.
i was
i'm i'm i'm like
that so that is the next thing once this i once i sort this uh problem out maybe i'll just go into that also.
and
the the other thing was about the subspace approach.
so um
i like plugged some groupings for computing this uh uh uh values and eigenvectors.
so just i just some small block of things which i needed to put together for the subspace approach.
and i'm in the process of like building up that stuff.
and um
uh yeah.
i guess yep i guess that's it.
and uh that's where i am right now.
so
oh how about you carmen?
huh i'm working with v t s.
um i do several experiment with the spanish database first.
only with v t s and nothing more.
not v a d.
no l d a.
nothing more.
what what is v t s again?
new
uh vectorial taylor series.
oh yes.
right right.
to remove the noise too.
i think i ask you that every single meeting.
don't i?
what?
i ask you that question every meeting.
yeah.
if well
so that'd be good from for analysis.
it's good to have some uh cases of the same utterance at different different times.
yeah.
what is v t s?
yeah.
v t s.
i'm
well um the question is that
well.
remove some noise but not too much.
and when we put the the them v a d the result is better.
and we put everything the result is better.
but it's not better than the result that we have without v t s.
no no.
i see.
so that given that you're using the v a d also the effect of the v t s is not so far
is not.
do you how much of that do you think is due to just the particular implementation and how much you're adjusting it?
or how much do you think is intrinsic to
pfft i don't know.
because
hhh
are you still using only the ten first frame for noise estimation?
or
uh i do the experiment using only the uh to use only one fair estimation of the noise.
or
yeah.
huh.
and also i did some experiment uh doing um a lying estimation of the noise.
and well it's a little bit better but not
maybe you have to standardize this thing also.
noise estimation.
because all the thing that you are testing use a different
huh.
huh.
they all need some some noise noise spectra.
no i do that two did two time.
but they use every all use a different one.
i have an idea.
if if uh uh
you're right.
i mean each of these require this.
um given that we're going to have for this test at least of uh boundaries what if initially we start off by using known sections of nonspeech for the estimation?
uhhuh.
uhhuh.
right?
yeah.
so um
uhhuh.
first place i mean even if ultimately we wouldn't be given the boundaries uh this would be a good initial experiment to separate out the effects of things.
i mean how much is the poor you know relatively uh unhelpful result that you're getting in this or this or this?
is due to some inherent limitation to the method for these tasks?
and how much of it is just due to the fact that you're not accurately finding enough regions that that are really noise?
huh.
uhhuh.
uhhuh.
um
so maybe if you tested it using that you'd have more reliable stretches of nonspeech to do the estimation from.
and see if that helps.
yeah.
another thing is the them the codebook.
the initial codebook.
that maybe
well it's too clean.
and
uhhuh.
because it's a
i don't know.
the methods
if you want you i can say something about the method.
uhhuh.
yeah in the
because it's a little bit different of the other method.
well we have
if this if this is the noise signal uh in the log domain we have something like this.
now we have something like this.
and the idea of these methods is to given a um
huh huh.
how do you say?
i will read because it's better for my english.
given
is the estimate of the p d f of the noise signal.
when we have a um a statistic of the clean speech and an statistic of the noisy speech.
and the clean speech the statistic of the clean speech is from a codebook.
huh this is the idea.
well like this relation is not linear.
the methods propose to develop this in a vectorial taylor series approximation.
i'm actually just confused about the equations you have up there.
so uh
the top equation is is is
no this in the it's this is the log domain.
i i must to say that.
which is which is the log domain?
is the t is egual is equal to uh log of
and
but y is what?
y of the spectrum?
uh this this is this.
or
and this is this.
no no.
the top y is what?
uhhuh.
is that power spectrum?
uh this is the noisy speech.
this
no is that power spectrum?
is it
yeah i guess it's the power spectrum of noisy speech.
yeah it's the power spectrum.
oh okay.
yeah and
so that's uh
this is the noisy
yeah it's
of the value.
okay.
yeah.
okay.
so this it's the magnitude squared or something.
yeah.
okay so you have power spectrum added there.
and down here you have you you put the
depends on t.
but all of this is just you just mean
yeah it's the same.
you just mean the log of the of the one up above.
yeah.
uhhuh.
and uh so that is x times
uh
yeah maybe
one one plus n by x.
but
well we can we can put this expression
x times one plus uh n uh n n n minus x?
the
yeah.
and then
and the noise signal.
uh so that's log of x plus log of one plus uh
well.
is that right?
log of
one plus n by x.
well huh
i actually don't see how you get that.
well if we apply the log we have e is
uh
huh.
uh log e is equal oh to log of x plus n.
uh and
yeah.
and well
and log of
uh we can say that e is equal to log of um exponential of x plus exponential of n.
uh
uhhuh.
no.
no.
that doesn't follow.
well if e restricts
it is
well this is this is in the the time domain.
well we have that
um
we have first that for example x is equal
uh
well.
this is the frequency domain.
and we can put that the log domain.
yeah.
log of x omega.
but well in the time domain we have an exponential.
no?
no?
oh maybe it's i am
i'm problem.
yeah.
i mean just never mind what they are.
uh it's just if x and n are variables.
right?
what is uh
the the the log of x plus n is not the same as the log of e to the x plus e to the n.
yeah.
but this
well i don't
well uh
maybe we can take it off line.
maybe
but i i don't know.
i i can do this incorrectly.
well the expression that appear in the in the paper is uh
the log.
the taylor series expansion for log one plus n by x is
is x
is it the first order expansion.
okay.
yeah the first one.
yeah i guess.
yeah.
yeah.
okay.
yeah.
yeah because it doesn't just follow what's there.
uhhuh.
yeah if if you take log x into log one plus n by x and then expand the log one plus n by x into taylor series.
it has to be some uh taylor series.
yeah.
now this is the
and then
yeah but the the second expression that you put is the first order expansion of the nonlinear relation between
not exactly.
no.
no no no.
it's not the first space.
well we have pfft uh them
well we can put that x is equal i is equal to log of uh
huh
that doesn't follow.
well we can put uh this?
huh.
no.
that i mean that the top one does not imply the second one.
the top?
because because the log of a sum is not the same as
yeah yeah yeah yeah yeah.
i mean as
yeah.
but we can
uh we we know that for example the log of e plus b is equal to log of e plus log to b.
right.
and we can say here it
right so you could
what is that?
and we can uh put this inside.
yeah.
and then we can
uh
you know?
no.
but
yeah.
uh.
i don't see how you get the second expression from the top one.
the i mean just more generally here if you say log of um a plus b
the log of log of a plus b is not
or a plus b is not the um log of e to the a plus e to the b.
no no no no no no no.
this not.
no.
right?
and that's what you seem to be saying.
no.
it's not but this is the same
oh.
right?
because you because you up here you have the a plus b.
no.
i say if i apply log i have uh log of e is equal to log of uh in this side is equal to log of x.
plus n.
plus n.
right.
no?
right.
right.
and then how do you go from there to the
this is right.
and then if i apply exponential to have here e
look.
okay.
so let's
i mean c equals a plus b .
it's log of capital y.
yeah right.
and then
yeah.
capital y.
x x.
this is x inside.
uhhuh.
we have this.
right.
no?
yeah.
that one's right.
uhhuh.
one and
uh we can put here the set transformation.
oh.
i see.
no?
i see.
okay i understand now.
all right thanks.
yeah.
in this case well we can put here a y.
okay.
so yeah.
it's just by definition that the individual that the uh
so capital x is by definition the same as e to the little x.
because she's saying that the little x is is the uh is the log.
all right.
now we can put this.
yeah.
no?
and here we can multiply by x.
all right.
i think these things are a lot clearer when you can use fonts different fonts there.
oh yes.
so you know which is which.
yeah yeah.
but i i i understand what you mean now.
that's true.
that's true.
okay.
but this this is correct.
sure.
and now i can do it
uh
pfff!
i can put log of e x plus log.
oh.
yes i understand now.
and this is
and that's where it comes from.
yeah.
yeah right.
right.
now it's correct.
right.
okay.
thanks.
well the idea
well.
we have fixed this
okay so now once you get that that one then you then you do a or second order or something taylor series expansion of this.
yeah.
this is another linear relation that this to develop this in vectors taylor series.
yeah sure.
right.
uhhuh.
and for that well the goal is to obtain um estimate a p d f for the noisy speech.
when we have a a statistic for clean speech and for the noisy speech.
huh?
and when
the way to obtain the p d f for the noisy speech is
well we know this statistic.
and we know the noisy
well we can apply first order of the vector taylor series of the of the of well the order that we want increase the complexity of the problem.
uhhuh.
and then when we have a expression uh for the mean and variance of the noisy speech we apply a technique of minimum mean square estimation.
uhhuh.
to obtain the expected value of the clean speech given the this statistic for the noisy speech.
uhhuh.
the statistic for clean speech and the statistic of the noisy speech.
this only that.
but the idea is that.
and the the model of clean speech is a codebook right?
yeah.
we have our codebook with different density gaussian.
uhhuh.
we can we can put that the p d f for the clean test probability of the clean speech is equal to
yeah.
uhhuh.
so um
how
how much in in the work they reported how much noisy speech did you need to get uh good enough statistics?
for the to get this mapping.
i don't know exactly.
yeah.
i i need to
yeah.
i don't know exactly.
because i think what's certainly characteristic of a lot of the data in this test is that um you don't have the
the training set may not be a a great estimator for the noise in the test set.
sometimes it is.
and sometimes it's not.
yeah.
i the clean speech the codebook for clean speech i am using timit.
and i have now uh sixty four gaussian.
uhhuh.
and what are you using for the noisy
doing that strictly
of the noise
i estimate the noises
well for the noises i only use one gaussian.
uhhuh.
and and you and you train it up entirely from uh nonspeech sections in the test?
huh.
uh yes.
the first experiment that i do it is solely to calculate the huh well this value.
yeah.
uh the compensation of the dictionary one time.
using the the noise at the beginning of the sentence.
this is the first experiment.
uhhuh.
yeah.
and i fix this for all the all the sentences.
uh because
well the v t s methods
in fact the first thing that i do is to to obtain uh an expression for e.
probability expression of of e.
that mean that the v t s huh with the v t s we obtain
uh
well we we obtain the means for each gaussian and the variance.
uhhuh.
this is one.
uh this is the composition of the dictionary.
uhhuh.
this one thing.
and the other thing that this with these methods is to uh obtain to calculate this value.
uhhuh.
because we can write
uh we can write that the estimation of the clean speech is equal at an expected value of the clean speech conditional to uh the noise signal the probability of the the statistic of the clean speech and the statistic of the noise.
uhhuh.
uhhuh.
this is the methods that say that we're going obtain this.
uhhuh.
and we can put that this is equal to the estimated value of e minus a function that conditional to e to the t to the noise signal.
well this is this function is the the term after develop this the term that we we take.
give p x and uh p the noise.
x k c noise.
huh.
and i can put that this is equal to the noise signal minus
well i put before this name
uh
and i can calculate this.
what is the first variable in that probability?
uh this is the gaussian.
no no i'm sorry.
in in the one you pointed at.
what's that variable?
uh this is the
weak.
so probably it it would do that.
like this.
it's one mixture of the model right?
but conditional.
no it's
it's not exactly this.
it's modify.
uh if we have clean speech we have the dictionary for the clean speech we have a probability of our our weight for each gaussian.
no?
and now this weight is different now.
okay.
yes.
because it's conditional.
and this i need to to
i know this.
uhhuh.
and i know this.
because this is from the dictionary that you have.
uhhuh.
i need to calculate this.
and for calculate this i have an i i can develop an expression that is
yes.
that.
i can calculate i can i calculated this value uh with the statistic of the noisy speech that i calculated before with the v t s approximation.
uhhuh.
and well normalizing.
and i know everything.
uh with the
nnn
when i develop this in taylor taylor series i can't um calculate the mean and the variance of the for each of the gaussian of the dictionary for the noisy speech.
and this is fixed.
uhhuh.
if i never do an a newer estimation of the noise this mean as mean and the variance are fixed.
uhhuh.
and for each uh frame of the speech the only thing that i need to do is to calculate this.
in order to calculate the estimation of the clean speech given our noisy speech.
so i'm i'm not following this perfectly.
but um
i
are you saying that all of these estimates are done using um estimates of the probability density for the noise that are calculated only from the first ten frames?
yeah.
and never change throughout anything else.
never
this is one of the approximations that i am doing.
per per per utterance?
or per
per utterance yes.
per utterance.
per utterance yes.
okay.
so it's done it's done new for each new utterance.
and
yeah.
so this changes the whole mapping for every utterance.
it's not
yeah.
yeah.
it's fixed the dictionary.
okay.
and the other estimation is when i do the uh on line estimation i change the means and variance of for the noisy speech.
okay.
yeah?
each time that i detect noise.
uhhuh.
i do it uh again this.
develop.
estimate the new mean and the variance of the noisy speech.
and with with this new new mean and variance i estimate again this.
so you estimated uh completely forgetting what you had before?
um
uh or is there some adaptation?
no no no it's not completely noise.
i am doing something like an adaptation of the noise.
okay.
now do we know either from their experience or from yours that uh just having uh two parameters the the mean and variance is enough?
yeah.
i mean i know you don't have a lot of data to estimate with.
but but uh
um
i estimate mean and variance for each one of the gaussian of the codebook.
no i'm talking about the noise.
oh.
um well only one
there's only one gaussian.
i am only using only one.
right.
i don't know
and you and and it's
uh uh
right.
it's only
it's only one
wait a minute.
this is
what's the dimensionality of the gaussian?
this is
uh it's in after the mel filter bank.
so this is twenty or something?
twenty three.
twenty?
so it's
yeah so it's actually forty numbers that you're getting.
yeah maybe maybe you don't have a
uh the original paper say that only one gaussian for the noise.
well yeah.
but i mean no no paper is is a bible.
yeah maybe isn't the right thing.
you know?
this is this is uh
yeah yeah yeah.
the question is um whether it would be helpful particularly if you used if you had more
so suppose you did
this is almost cheating.
it certainly isn't real time.
but if suppose you use the real boundaries that that you were in fact were given by the v a d and so forth.
or i i guess we're going to be given even better boundaries than that.
and you look you take all all of the nonspeech components in an utterance.
so you have a fair amount.
do you benefit from having a better model for the noise?
that would be another question.
maybe.
so first question would be to what extent are the errors that you're still seeing based on the fact that you have poor boundaries for the uh uh nonspeech.
and the second question might be given that you have good boundaries could you do better if you used more parameters to characterize the noise.
um
also another question might be
um they are doing they're using first term only of the vector taylor series?
yeah.
um if you do a second term does it get too complicated because of the nonlinearity?
yeah it's quite complicated.
yeah okay.
no i won't ask the next question then.
oh it's it's the for me it's the first time that i am working with v t s.
uh
yeah.
no it's interesting.
uh we haven't had anybody work with it before.
so it's interesting to get your get your feedback about it.
it's another type of approximation because because it's a statistic statistic approximation to remove the noise.
i don't know.
right.
great.
okay.
well i guess we're about done.
um
so some of the digit forms don't have digits.
uh we ran out.
and there were some blanks in there.
so not everybody will be reading digits.
but um
i guess you've got some right morgan?
i have some.
so
why don't you go ahead and start?
and i think it's just us down here at this end that have them.
so
uh okay.
so we switch off with this?
whenever you're ready.
or
uh leave it on.
no okay.
uh
they prefer to have them on.
and the
just so that they're continuing to get the distant uh information.
yeah.
okay.
okay.
so
let's see.
yeah barry's not here and dave's not here.
um i can say about just just quickly to get through it that dave and i submitted this a.s.r.u.
this is for
a.s.r.u.
yeah.
so
um
yeah it's it's interesting.
i mean basically we're dealing with reverberation.
and um when we deal with pure reverberation the technique he's using works really really well.
uh and when they had the reverberation here uh we'll measure the signal to noise ratio.
and it's uh about nine d.b.
huh.
so
um
a fair amount of
you mean from the actual uh recordings?
yeah.
it's nine d. b?
yeah.
um
and actually it brought up a question which may be relevant to the aurora stuff too.
um i know that when you figured out the filters that we're using for the mel scale there was some experimentation that went on at at uh at o.g.i.
um
but one of the differences that we found between the two systems that we were using the the aurora h.t.k. system baseline system and the system that we were the the uh other system we were using the uh the s.r.i. system was that the s.r.i. system had maybe a um hundred hertz high pass.
yep.
and the uh aurora h.t.k. it was like twenty.
sixty four.
sixty four.
uh
sixty four?
yeah.
uh
if you're using the baseline.
is that the band center?
no.
the edge.
the edge is really uh sixty four?
yeah.
for some reason uh dave thought it was twenty.
so the uh center would be somewhere around like hundred.
but
and hundred and hundred hundred and maybe it's like hundred hertz.
but do you know for instance how far down it would be at twenty hertz?
what the how much rejection would there be at twenty hertz let's say?
at twenty hertz?
yeah.
any idea what the curve looks like?
twenty hertz frequency.
oh it's it's zero at twenty hertz right?
the filter.
actually the left edge of the first filter is at sixty four.
sixty four.
so
so anything less than sixty four is zero.
huh.
it's actually set to zero?
what kind of filter is that?
yeah.
yeah.
is this oh from the from
it
this is the filter bank in the frequency domain that starts at sixty four.
oh so you uh so you really set it to zero the f.f.t.
yeah.
yeah.
yeah.
so it's it's a weight on the ball spectrum.
triangular weighting.
right.
okay.
um
okay.
so that's that's a little different than dave thought i think.
but but
um
still it's possible that we're getting in some more noise.
so i wonder is it was there their experimentation with uh say throwing away that filter or something?
and uh
uh throwing away the first?
yeah.
um
yeah we we've tried including the full full bank.
right?
from zero to four k. .
uhhuh.
and that's always worse than using sixty four hertz.
right.
but the question is whether sixty four hertz is is uh too uh low.
yeah.
i mean make it a hundred or so?
yeah.
i i think i've tried a hundred and it was more or less the same or slightly worse.
on what test set?
on the same uh speechdat-car.
aurora.
um it was on the speechdat-car.
yeah.
so i tried a hundred to four k. .
yeah.
um
so it was
and on and on the um um t.i. digits also?
no no no.
i think i just tried it on speechdat-car.
huh.
that'd be something to look at sometime.
because what um uh he was looking at was performance in this room.
uhhuh.
would that be more like
well you'd think that'd be more like speechdat-car.
i guess.
in terms of the noise.
the speechdat-car is more uh sort of roughly stationary a lot of it.
yeah.
and and t.i. digits maybe is not so much as
uhhuh.
yeah.
yeah.
uhhuh.
okay.
well maybe it's not a big deal.
but um
anyway that was just something we wondered about.
but um
uh certainly a lot of the noise uh is uh below a hundred hertz.
uh the signal to noise ratio you know looks a fair amount better if you if you high pass filter it from this room.
yeah.
but um
but it's still pretty noisy.
even even for a hundred hertz up it's it's still fairly noisy.
uhhuh.
the signal to noise ratio is is is actually still pretty bad.
huh?
so um
i mean the main the the
so that's on that's on the the far field ones though right?
yeah.
that's on the far field.
yeah.
yeah the near field's pretty good.
so what is uh what's causing that?
well we got a a video projector in here.
uh
and uh which we keep on during every every session we record.
yeah.
which you know i i we were aware of.
but but we thought it wasn't a bad thing.
uhhuh.
i mean that's a nice noise source.
yeah.
uh and there's also the uh uh air conditioning.
huh.
which uh you know is a pretty low frequency kind of thing.
uhhuh.
but but uh
so those are those are major components i think.
i see.
uh for the stationary kind of stuff.
huh.
um
but um
it uh
i guess i maybe i said this last week too.
but it it it really became apparent to us that we need to to take account of noise.
and uh
so i think when when he gets done with his prelim study i think one of the next things we'd want to do is to take this uh uh noise uh processing stuff and and uh uh synthesize some speech from it.
and then
when are his prelims?
um i think in about um a little less than two weeks.
oh.
wow.
yeah.
yeah.
so
uh it might even be sooner.
uh let's see this is the sixteenth?
seventeenth?
yeah i don't know if he's before
it might even be in a week.
so
a week.
huh.
week and a half.
i i guessed that they were going to do it some time during the semester.
but they'll do it any time huh?
they seem to be
well the semester actually is starting up.
is it already?
yeah.
the semester's late late august they start here.
yikes.
so they do it right at the beginning of the semester.
yeah.
yeah.
so uh
yep.
i mean that that was sort of one
i mean
the overall results seemed to be first place in in in the case of either um artificial reverberation or a modest sized training set.
uh either way uh uh it helped a lot.
and but if you had a a really big training set a recognizer uh system that was capable of taking advantage of a really large training set
i thought that one thing with the h.t.k. is that is has the as we're using the configuration we're using is is being bound by the terms of aurora.
we have all those parameters just set as they are.
so even if we had a hundred times as much data we wouldn't go out to you know ten or or a hundred times as many gaussians or anything.
so
um it's kind of hard to take advantage of of of big chunks of data.
uhhuh.
huh yeah.
uh whereas the other one does sort of expand as you have more training data.
it does it automatically actually.
and so
um
uh
that one really benefited from the larger set.
and it was also a diverse set with different noises and so forth.
uh so um
that uh that seemed to be
so if you have that that better recognizer that can that can build up more parameters and if you um have the natural room which in this case has a a pretty bad signal to noise ratio then in that case um the right thing to do is just do use speaker adaptation and and not bother with with this acoustic uh processing.
but i think that that would not be true if we did some explicit noise processing as well as uh the convolutional kind of things we were doing.
uhhuh.
so
that's sort of what we found.
huh.
i um uh started working on the uh mississippi state recognizer.
oh.
so i got in touch with joe and and uh from your email and things like that.
okay.
and uh they added me to the list.
uh the mailing list.
okay.
great.
and he gave me all of the pointers and everything that i needed.
and so i downloaded the um
there were two things uh that they had to download.
one was the uh i guess the software.
and another wad was a um sort of like a sample a sample run.
so i downloaded the software and compiled all of that.
and it compiled fine.
eight.
oh uh great.
no problems.
and um i grabbed the sample stuff.
but i haven't uh compiled it.
that sample was released only yesterday or the day before right?
no.
well i haven't grabbed that one yet.
so there's two.
oh there is another short sample set.
there was another short one.
sample.
yeah.
okay.
and so i haven't grabbed the latest one that he just uh put out yet.
oh okay.
yeah okay.
so
um but the software seemed to compile fine and everything.
so
and um
so
is there any word yet about the issues about um adjustments for different feature sets or anything?
no.
i i
you asked me to write to him.
and i think i forgot to ask him about that.
yeah.
or if i did ask him he didn't reply.
i i don't remember yet.
uh i'll i'll i'll double check that and ask him again.
yeah.
huh.
yeah.
it's like that that could turn out to be an important issue for us.
huh.
yeah.
yeah.
yeah.
because they have it.
maybe i'll send it to the list.
yeah.
because they have uh already frozen those in insertion penalties and all those stuff is what i feel.
because they have this document explaining the recognizer.
uhhuh.
and they have these tables with uh various language model weights insertion penalties.
okay i haven't seen that one yet.
uh it's it's there on that web.
so
okay.
and uh on that i mean they have run some experiments using various insertion penalties and all those
and so they've picked the values.
yeah i think they
yeah they picked the values from
oh okay.
okay.
for what test set?
uh the one that they have reported is a nist evaluation wall street journal.
but that has nothing to do with what we're testing on right?
you know.
uhhuh.
no.
so they're like
um
so they are actually trying to uh fix that those values using the clean uh training part of the wall street journal.
which is
i mean the aurora.
aurora has a clean subset.
i mean they want to train it.
right.
and then this they're going to run some evaluations.
so they're they're setting it based on that?
yeah.
okay.
so now we may come back to the situation where we may be looking for a modification of the features to account for the fact that we can't modify these parameters.
yeah.
but um
yeah.
uh
but it's still worth i think just since you know just chatting with joe about the issue.
yeah.
okay.
do you think that's something i should just send to him?
um
or do you think i should send it to this there's an a a mailing list?
well it's not a secret.
i mean we're you know certainly willing to talk about it with everybody.
but i think i think that um um it's probably best to start talking with him just to
okay.
uh you know it's a dialogue between two of you about what you know what does he think about this and what what you know what could be done about it.
yeah.
um
okay.
if you get ten people in involved in it there'll be a lot of perspectives based on you know how
yeah.
you know.
right.
uh
but i mean i think it all should come up eventually.
but if if if there is any uh uh way to move in a way that would that would you know be more open to different kinds of features.
okay.
but if if uh if there isn't and it's just kind of shut down and and then also there's probably not worthwhile bringing it into a larger forum where where political issues will come in.
yeah.
okay.
oh.
so this is now it's it's compiled under solaris?
yeah.
yeah okay.
because he there was some mail saying that it's may not be stable for linux and all those.
yep.
yeah.
yeah.
that was a particular version.
susi.
yeah.
yeah.
susi or whatever it was.
yeah yeah.
but we don't have that.
yeah okay.
so
okay.
that's fine.
should be okay.
yeah it compiled fine actually.
yeah.
no no errors.
nothing.
so
that's good.
uh this is slightly off topic.
but uh
i noticed just glancing at the uh hopkins workshop uh web site that uh um one of the i don't know well we'll see how much they accomplish but one of the things that they were trying to do in the graphical models thing was to put together a a uh tool kit for doing uh um arbitrary graphical models for uh speech recognition.
huh.
so and jeff uh the two jeffs were
who's the second jeff?
uh
oh uh do you know geoff zweig?
no.
oh.
uh he he uh he was here for a couple years.
and he uh got his p.h.d.
oh okay.
he
and he's uh been at i.b.m. for the last couple years.
oh okay.
so
wow.
uh so he did he did his p.h.d. on dynamic bayes-nets.
that would be neat.
uh for for speech recognition.
he had some continuity built into the model.
presumably to handle some um inertia in the in the production system.
and
um
huh.
so
huh.
um i've been playing with first the um v.a.d.
um so it's exactly the same approach.
but the features that the v.a.d. neural network use are uh m.f.c.c. after noise compensation.
oh i think i have the results.
what was it using before?
before it was just p.l.p.'s.
so
yeah it was actually
no.
not
i mean it was just the noisy features i guess.
yeah yeah yeah.
yeah.
noisy noisy features.
not compensated.
um
this is what we get after this.
so actually we yeah here the features are noise compensated.
and there is also the l.d.a. filter.
um and then it's a pretty small neural network which use um nine frames of of six features from c. zero to c. fives plus the first derivatives.
and it has one hundred hidden units.
is that nine frames uh centered around the current frame ? or?
yeah.
uhhuh.
so i'm i'm sorry.
there's there's there's how many how many inputs?
so it's twelve times nine.
twelve times nine inputs.
and a hundred uh hidden.
hidden.
and
two outputs.
two outputs.
two outputs.
okay.
so i guess about eleven thousand parameters.
uhhuh.
which actually shouldn't be a problem even in in small phones . yeah.
so i'm i'm
it should be okay.
so what is different between this and and what you
so the previous
it's based on the system that has a fifty three point sixty six percent improvement.
it's the same system.
the only thing that changed is the a uh a the estimation of the silence probabilities.
uh.
okay.
which now is based on uh cleaned features.
and it's a it's a lot better.
wow.
yeah.
um
that's great.
so it's it's not bad.
but the problem is still that the latency is too large.
what's the latency?
because
um
the the latency of the v.a.d. is two hundred and twenty milliseconds.
and uh the v.a.d. is used uh for online normalization.
and it's used before the delta computation.
so if you add these components it goes to a hundred and seventy.
right?
i i'm confused.
you started off with two twenty and you ended up with one seventy?
with two two hundred and seventy.
two seventy?
if
yeah.
if you add the delta delta computation.
oh.
which is done afterwards.
um
so it's two twenty.
is this are these twenty-millisecond frames?
is that why?
is it after downsampling?
the two twenty is one hundred milliseconds for the um
or
no it's forty milliseconds for for the uh uh cleaning of the speech.
um
then there is um the neural network which use nine frames.
so it adds forty milliseconds.
okay.
um
after that um you have the um filtering of the silence probabilities.
which is a million filter.
it
and it creates a one hundred milliseconds delay.
so um
plus there is a delta at the input.
yeah.
and there is the delta at the input.
which is
one hundred milliseconds for smoothing.
um
so it's
uh
it's like forty plus forty plus
median.
huh.
forty.
and then forty.
this forty plus twenty plus one hundred.
forty
uh
so it's two hundred actually.
yeah there are twenty that comes from
there is ten that comes from the l.d.a. filters also.
oh okay.
right?
uh so it's two hundred and ten.
yeah.
if you are using
uh
plus the frame.
if you are using three frames.
so it's two twenty.
if you are phrasing using three frames it is thirty here for delta.
yeah i think it's it's five frames.
so five frames that's twenty.
but
okay.
so it's who two hundred and ten.
uh wait a minute.
it's forty forty for the for the cleaning of the speech.
so
forty cleaning.
forty for the i.n. a.n.n.
a hundred for the smoothing.
yeah.
well but at ten
twenty for the delta.
at at the input.
twenty for delta.
i mean that's at the input to the net.
yeah.
delta at input to net?
and there
yeah.
yeah.
so it's like five six cepstrum plus delta.
at nine nine frames of
and then ten milliseconds for
there's an l.d.a. filter.
ten milliseconds for l.d.a. filter.
and and ten another ten milliseconds you said for the frame?
for the frame i guess.
i computed two twenty.
yeah well it's
i guess it's for the fr the
okay.
and then there's delta besides that?
so this is the features that are used by our network.
and then afterwards you have to compute the delta on the uh main feature stream.
which is um delta and double deltas.
okay.
which is fifty milliseconds.
yeah.
no i mean the
after the noise part the forty the the other hundred and eighty
well i mean
wait a minute.
some of this is uh is uh is in parallel isn't it?
i mean the l.d.a.
oh you have the l.d.a. as part of the v.d. uh v.a. d?
or
the v.a.d. use uh l.d.a. filtered features also.
oh it does?
uhhuh.
uh.
so in that case there isn't too much in parallel.
uh
no.
there is um just downsampling upsampling and the l.d.a.
um so the delta at the end is how much?
it's fifty.
it's
fifty.
all right.
so
but well we could probably put the delta um before online normalization.
it should not that make a big difference.
what if you used a smaller window for the delta?
because
could that help a little bit?
i mean i guess there's a lot of things you could do to
yeah.
yeah.
yeah.
but nnn
yeah.
so if you if you put the delta before the uh online if
uhhuh.
yeah.
uh then then it could go in parallel.
because
and then then you don't have that additive.
yep.
yeah.
because the time constant of the online normalization is pretty long compared to the delta window.
okay.
so
it should not make
okay.
and you ought to be able to shove uh uh pull off twenty milliseconds from somewhere else to get it under two hundred.
right?
i mean
uhhuh.
is two hundred the
the hundred
a hundred milliseconds for smoothing is sort of an arbitrary amount.
it could be eighty.
and and probably do.
yeah.
yeah.
a
uh
what's the baseline you need to be under?
two hundred.
well we don't know.
they're still arguing about it.
oh.
i mean if it's two if if it's uh if it's two fifty then we could keep the delta where it is if we shaved off twenty.
if it's two hundred if we shaved off twenty we could we could uh meet it by moving the delta back.
so how do you know that what you have is too much if they're still deciding?
uh we don't.
but it's just
i mean the main thing is that since that we got burned last time and you know by not worrying about it very much we're just staying conscious of it.
uhhuh.
oh okay.
i see.
and so
i mean if if if a week before we have to be done someone says well you have to have fifty milliseconds less than you have now it would be pretty frantic around here.
so
uh okay.
uh
but still that's that's a pretty big uh win.
and it doesn't seem like you're in terms of your delay you're uh that
he added a bit on.
huh.
i guess because before we were we were had were able to have the noise uh stuff uh and the l.v.a. be in parallel.
and now he's he's requiring it to be done first.
well i think the main thing maybe is the cleaning of the speech which takes forty milliseconds or so.
right.
well so you say
and
let's say ten milliseconds seconds for the l.d.a.
and but the l.d.a. is well pretty short right now.
well ten.
yeah.
and then forty for the other.
yeah the l.d.a.l.d.a. we don't know is like is it very crucial for the features right?
no.
i just
yeah.
this is the first try.
i mean i maybe the l.d.a.'s not very useful then.
right.
so you could start pulling back.
but
yeah.
but i think you have
i mean you have twenty for delta computation.
which now you're sort of doing twice.
right?
but were you doing that before?
huh.
on the in the
well in the proposal um the input of the v.a.d. network were just three frames i think.
uhhuh.
just
yeah just the static no delta.
uh static features.
right.
so what you have now is uh forty for the the noise twenty for the delta and ten for the l.d.a.
that's seventy milliseconds of stuff which was formerly in parallel.
right?
so i think
uhhuh.
you know that's that's the difference as far as the timing.
yeah.
right?
um
and you could experiment with cutting various pieces of these back a bit.
but
i mean we're we're not we're not in terrible shape.
yeah.
that's what it seems like to me.
uhhuh.
yeah.
it's pretty good.
it's it's not like it's adding up to four hundred milliseconds or something.
where where is this where is this fifty seven point o. two in in comparison to the last evaluation?
well it's i think it's better than anything uh anybody got.
yeah.
oh is that right?
the best was fifty four point five.
yeah.
point
oh.
yeah.
and our system was forty nine.
but with the neural network.
wow!
so this is almost ten percent.
with the with the neural net.
yeah.
yeah.
and and
it
so this is this is like the first proposal.
the proposal one.
it was forty four actually.
yeah.
yeah.
and we still don't have the neural net in.
so so it's
wow!
you know.
so it's
we're we're doing better.
i mean we're getting better recognition.
this is this is really good.
i mean i'm sure other people working on this are not sitting still either.
but
but
yeah.
but uh
uh
i mean the important thing is that we learn how to do this better.
and you know
so
um
yeah.
so our
um
yeah you can see the kind of kind of numbers that we're having say on speechdat-car.
which is a hard task.
because it's really um i think it's just sort of sort of reasonable numbers starting to be.
uhhuh.
i mean it's still
yeah even for a well matched case it's sixty percent error rate reduction.
which is
yeah.
yeah.
probably half.
good.
um
yeah.
so actually this is in between what we had with the previous v.a.d. and what sunil did with an i.d.l.v.a.d.
which gave sixty two percent improvement right?
yeah it's almost that.
so
it's almost an average.
somewhere around
yeah.
yeah.
what was that?
say that last part again.
so if you use like an i.d.l.v.a.d. uh for dropping the frames
or the best we can get.
the best that we can get that means that we estimate the silence probability on the clean version of the utterances.
then you can go up to sixty two percent error rate reduction globally.
huh.
huh.
yeah.
so that would be even that wouldn't change this number down here to sixty two?
yeah.
yeah.
so you you were
if you add a good very good v.a.d. that works as well as a v.a.d. working on clean speech.
yeah.
yeah.
then you you would go
so that's sort of the best you could hope for.
uhhuh.
probably yeah.
i see.
so fifty three is what you were getting with the old v.a.d.
yeah.
and uh
and sixty two with the the you know quote unquote cheating v.a.d.
and fifty seven is what you got with the real v.a.d.
uhhuh.
that's great.
uh yeah the next thing is i started to play
well i don't want to worry too much about the delay.
no.
maybe it's better to wait.
okay.
for the decision.
yeah.
from the committee.
uh but i started to play with the um uh tandem neural network.
huh
i just did the configuration that's very similar to what we did for the february proposal.
and
um
so there is a a first feature stream that use uh straight m.f.c.c. features.
uhhuh.
well these features actually.
and the other stream is the output of a neural network using as input also these um cleaned m.f.c.c.
um
i don't have the
those are those are what is going into the tandem net?
huh.
those two.
so there is just this feature stream the fifteen m.f.c.c. plus delta and double delta.
no.
yeah.
um so it's makes forty five features that are used as input to the h.t.k.
and then there is there are more inputs that comes from the tandem m.l.p.
oh oh.
okay.
yeah.
i see.
he likes to use them both.
because then it has one part that's discriminative.
uhhuh.
yeah.
um
one part that's not.
right.
okay.
so um
uh yeah.
right now it seems that i just tested on speechdat-car while the experiment are running on your on t.i. digits.
well it improves on the well matched and the mismatched conditions.
but it get worse on the highly mismatched.
um
compared to these numbers?
compared to these numbers yeah.
um
like on the well match and medium mismatch the gain is around five percent relative.
but it goes down a lot more like fifteen percent on the h.m. case.
you're just using the full ninety features?
the
you have ninety features?
i have
um
from the networks it's twenty eight.
so
and from the other side it's forty five.
so it's forty five.
so it's you have seventy three features.
yeah.
and you're just feeding them like that.
yeah.
uhhuh.
there isn't any k.l.t. or anything?
there's a k.l.t. after the neural network as as before.
that's how you get down to twenty eight?
yeah.
why twenty eight?
i don't know.
uh
oh.
it's it's because it's what we did for the first proposal.
we tested uh trying to go down.
uh.
it's a multiple of seven.
and
yeah.
yeah.
so
um
yeah.
i wanted to do something very similar to the proposal as a first first try.
yeah.
i see.
yeah.
yeah.
that makes sense.
but we have to for sure we have to go down.
because the limit is now sixty features.
so
yeah.
uh
we have to find a way to decrease the number of features.
um
so it seems funny that i don't know maybe i don't quite understand everything but that adding features
i guess i guess if you're keeping the back end fixed.
maybe that's it.
because it seems like just adding information shouldn't give worse results.
but i guess if you're keeping the number of gaussians fixed in the recognizer then
well yeah.
huh.
but i mean just in general adding information
suppose the information you added well was a really terrible feature and all it brought in was noise.
yeah.
right?
so so um
or or suppose it wasn't completely terrible.
but it was completely equivalent to another one feature that you had.
except it was noisier.
uhhuh.
right?
in that case you wouldn't necessarily expect it to be better at all.
oh yeah i wasn't necessarily saying it should be better.
i'm just surprised that you're getting fifteen percent relative worse on the
uhhuh.
but it's worse.
on the highly mismatched condition.
on the highly mismatch.
yeah i
yeah.
so highly mismatched condition means that in fact your training is a bad estimate of your test.
uhhuh.
so having having uh a a a greater number of features if they aren't maybe the right features that you use certainly can can easily uh make things worse.
i mean you're right.
if you have if you have uh lots and lots of data and you have and your your your training is representative of your test then getting more sources of information should just help.
but but it's it doesn't necessarily work that way.
huh.
uhhuh.
so i wonder
um
well what's your what's your thought about what to do next with it?
um
i don't know.
i'm surprised.
because i expected the neural net to help more when there is more mismatch as it was the case for the.
uhhuh.
so was the training set same as the the february proposal?
yeah it's the same training set.
so it's timit with the t.i. digits' uh noises uh added.
okay.
uhhuh.
um
well we might uh we might have to experiment with uh better training sets.
again.
uhhuh.
but
i the other thing is i mean before you found that was the best configuration but you might have to retest those things now that we have different the rest of it is different.
right?
so
um
uh
for instance what's the effect of just putting the neural net on without the other other path?
uhhuh.
i mean you know what the straight features do.
yeah.
that gives you this.
uhhuh.
you know what it does in combination.
you don't necessarily know what
what if you did the
would it make sense to do the k.l.t. on the full set of combined features?
instead of just on the
yeah.
i i guess.
um
the reason i did it this ways is that in february it we we tested different things like that.
so having two k.l.t. having just a k.l.t. for a network or having a global k.l.t.
oh i see.
and
so you tried the global k.l.t. before.
well
yeah.
and it didn't really
and uh
yeah.
the differences between these configurations were not huge.
i see.
but it was marginally better with this configuration.
uhhuh.
uhhuh.
but yeah that's obviously another thing to try.
um
since things are things are different.
uhhuh.
uhhuh.
and i guess if the
these are all
so all of these seventy three features are going into um the uh the h.m.m.
yeah.
and is are are are any deltas being computed of of them?
of the straight features yeah.
so
not of the
but the um
tandem features are used as they are.
are not.
so
yeah maybe we can add some context from these features also as dan did in in his last work.
could.
yeah.
but the other thing i was thinking was
um
uh now i lost track of what i was thinking.
but
what is the
you said there was a limit of sixty features or something?
uhhuh.
what's the relation between that limit and the um forty eight uh forty eight hundred bits per second?
oh.
i know what i was going to say.
um not no relation.
no relation.
so i i i don't understand.
the the forty eight hundred bits is for transmission of some features.
because
i mean if you're only using
and generally it allows you to transmit like fifteen uh cepstrum.
the issue was that um this is supposed to be a standard that's then going to be fed to somebody's recognizer somewhere.
which might be you know it it might be a concern how many parameters are use used and so forth.
and so
uh
they felt they wanted to set a limit.
so they chose sixty.
some people wanted to use hundreds of parameters.
and and that bothered some other people.
and so
uhhuh.
they just chose that.
i i i think it's kind of arbitrary too.
but but that's that's kind of what was chosen.
i i remembered what i was going to say.
what i was going to say is that um maybe maybe with the noise removal uh these things are now more correlated.
so you have two sets of things that are kind of uncorrelated uh within themselves.
but they're pretty correlated with one another.
uhhuh.
and um
they're being fed into these uh variants only gaussians and so forth.
and and uh
uhhuh.
so maybe it would be a better idea now than it was before to uh have uh one k.l.t. over everything.
uhhuh.
to de correlate it.
yeah i see.
maybe.
you know.
what are the s.n.r.'s in the training set timit?
it's uh ranging from zero to clean.
yeah.
uhhuh.
from zero to clean.
yeah.
so we found this this uh this macrophone data and so forth that we were using for these other experiments to be pretty good.
uhhuh.
so that's after you explore these other alternatives that might be another way to start looking is is just improving the training set.
uhhuh.
i mean we were getting uh lots better recognition using that than
of course you do have the problem that um we are not able to increase the number of gaussians uh or anything to uh uh to match anything.
so we're only improving the training of our feature set.
but that's still probably something.
so you're saying add the macrophone data to the training of the neural net ? the tandem net?
yeah.
that's the only place that we can train.
we can't train the other stuff with anything other than the standard amount.
yeah.
right.
so
um
um
what what was it trained on again?
the one that you used.
it's timit with noise.
uhhuh.
so yeah it's rather a small.
yeah.
um
how big is the net by the way?
uh it's uh five hundred hidden units.
and
and again you did experiments back then where you made it bigger.
and it and that was that was sort of the threshold point.
much less than that it was worse.
yeah.
and
yeah.
much more than that it wasn't much better.
huh.
so is it is it though the performance big relation in the high high mismatch has something to do with the uh cleaning up that you that is done on the timit after adding noise?
yeah.
so
it's all the noises are from the t.i. digits.
right?
yeah.
so you
um
well it's like the high mismatch of the speechdat-car.
they uh
after cleaning up maybe having more noise than the the training set of timit after clean after you do the noise cleanup.
huh.
i mean earlier you never had any compensation.
you just trained it straight away.
uhhuh.
so it had like all these different conditions of s.n.r.'s actually in their training set of neural net.
uhhuh.
uhhuh.
but after cleaning up you have now a different set of s.n.r.'s right?
yeah.
for the training of the neural net.
uhhuh.
and
is it something to do with the mismatch that that's created after the cleaning up like the high mismatch?
you mean the the most noisy occurrences on speechdat-car might be a lot more noisy than
uhhuh.
of that
i mean the s.n.r. after the noise compensation of the speechdat-car.
oh.
so right.
so the training the the neural net is being trained with noise compensated stuff.
maybe.
yeah.
yeah.
yeah.
yeah.
which makes sense.
but uh you're saying yeah the noisier ones are still going to be even after our noise compensation are still going to be pretty noisy.
yeah.
uhhuh.
yeah so now the after noise compensation the neural net is seeing a different set of s.n.r.'s than that was originally there in the training set of timit.
because in the timit it was zero to some clean.
right.
yes.
so the net saw all the s.n.r. conditions.
right.
now after cleaning up it's a different set of s.n.r.
right.
and that s.n.r. may not be like covering the whole set of s.n.r.'s that you're getting in the speechdat-car.
right.
but the speechdat-car data that you're seeing is also reduced in noise.
yeah yeah yeah.
yeah.
by the noise compensation.
yeah it is.
but i'm saying there could be some some issues of.
so
uhhuh.
yeah.
well if the initial range of s.n.r. is different we the problem was already there before.
and
yeah.
because
huh
yeah i mean it depends on whether you believe that the noise compensation is equally reducing the noise on the test set and the training set.
huh.
on the test set yeah.
uh
right?
i mean you're saying there's a mismatch in noise that wasn't there before.
huh.
uhhuh.
uhhuh.
but if they were both the same before then if they were both reduced equally then there would not be a mismatch.
so
i mean this may be
heaven forbid this noise compensation process may be imperfect.
but
uh
yeah uh
well i
so maybe it's treating some things differently.
i don't know.
i i just that could be seen from the t.i. digits uh testing condition.
because um the noises are from the t.i. digits right?
noise.
yeah.
so
so cleaning up the t.i. digits.
and if the performance goes down in the t.i. digits mismatch high mismatch like this
clean training yeah.
on a clean training or zero d.b. testing.
yeah we'll so we'll see.
yeah.
uh
then it's something to do.
maybe.
uhhuh.
yeah.
i mean one of the things about
i mean the macrophone data um i think you know it was recorded over many different telephones.
uhhuh.
and um
so there's lots of different kinds of acoustic conditions.
i mean it's not artificially added noise or anything.
so it's not the same.
i don't think there's anybody recording over a car from a car.
but i think it's it's varied enough that if if doing this adjustments uh and playing around with it doesn't uh make it better the most uh it seems like the most obvious thing to do is to improve the training set.
um
i mean what we were
uh the condition it it gave us an enormous amount of improvement in what we were doing with meeting recorder digits.
even though there again these macrophone digits were very very different from uh what we were going on here.
i mean we weren't talking over a telephone here.
but it was just i think just having a a nice variation in acoustic conditions was just a good thing.
uhhuh.
yep.
huh.
yeah actually to uh what i observed in the h.m. case is that the number of deletion dramatically increases.
it it doubles.
number of deletions.
when i added the the neural network it doubles the number of deletions.
yeah so i don't you know how to interpret that.
but huh
yeah.
me either.
and and did other numbers stay the same?
insertion substitutions stay the same?
they stayed the same.
they maybe they are a little bit uh lower.
roughly.
uhhuh.
they are a little bit better.
yeah but
uhhuh.
did they increase the number of deletions even for the cases that got better?
say for the i mean it
no it doesn't.
no.
so it's only the highly mismatched.
and it remind me again.
the highly mismatched means that the
clean training and
uh sorry?
it's clean training
well close microphone training and distant microphone um high speed i think.
close mike training.
well.
the most noisy cases are the distant microphone for testing.
right.
so
well maybe the noise subtraction is subtracting off speech.
separating.
yeah.
but
yeah.
i mean but without the neural network it's well it's better.
it's just when we add the neural networks.
the feature are the same except that
yeah right.
uh that's right that's right.
um
well that that says that you know the um the models in in uh the recognizer are really paying attention to the neural net features.
yeah.
uhhuh.
uh
but yeah.
actually the timit noises are sort of a range of noises.
and they're not so much the stationary driving kind of noises right?
it's it's pretty different.
isn't it?
uh there is a car noise.
so there are just four noises.
um
uh car i think.
babble.
babble.
subway right?
and
street or airport or something.
and street isn't
or train station.
train station yeah.
yeah.
so it's mostly well car is stationary.
uhhuh.
babble it's a stationary background plus some voices.
uhhuh.
some speech over it.
and the other two are rather stationary also.
well i i think that if you run it
actually you maybe you remember this.
when you in in the old experiments when you ran with the neural net only and didn't have this side path um uh with the the pure features as well did it make things better to have the neural net?
uhhuh.
was it about the same?
uh
it was a little bit worse.
than
than just the features.
yeah.
so
until you put the second path in with the pure features the neural net wasn't helping at all.
uhhuh.
well that's interesting.
it was helping uh if the features are were bad.
i mean
yeah.
just plain p.l.p.'s or m.f.c.c.'s.
yeah.
but
as soon as we added l.d.a. online normalization and all these things then
they were doing similar enough things.
well i still think it would be sort of interesting to see what would happen if you just had the neural net without the side thing.
yeah.
uhhuh.
and and the thing i i have in mind is uh maybe you'll see that the results are not just a little bit worse.
maybe that they're a lot worse.
you know?
and um
but if on the other hand uh it's say somewhere in between what you're seeing now and and and uh what you'd have with just the pure features then maybe there is some problem of a of a uh combination of these things or correlation between them somehow.
uhhuh.
if it really is that the net is hurting you at the moment then i think the issue is to focus on on uh improving the the net.
yeah.
uhhuh.
um
so what's the overall
i mean you haven't done all the experiments.
but you said it was somewhat better say five percent better for the first two conditions and fifteen percent worse for the other one?
but it's but of course that one's weighted lower.
yeah oh.
yeah.
so i wonder what the net effect is.
i i i think it's it was one or two percent.
that's not that bad.
but it was like two percent relative worse on speechdat-car.
i have to to check that.
well i have i will.
well it will overall it will be still better.
even if it is fifteen percent worse.
because the fifteen percent worse is given like twenty five point two five eight.
right.
uhhuh.
huh.
right.
so the so the worst it could be if the others were exactly the same is four.
is it like
yeah so it's four.
and and uh in fact since the others are somewhat better
is
so either it'll get cancelled out or you'll get like almost the same.
yeah it was it was slightly worse.
uh
slightly bad.
yeah.
um
yeah it should be pretty close to cancelled out.
yeah.
uhhuh.
you know i've been wondering about something.
in the um a lot of the um the hub five systems um recently have been using l.d.a.
and and they um they run l.d.a. on the features right before they train the models.
so there's the the l.d.a. is is right there before the h.m.m.'s.
yeah.
so you guys are using l.d.a.
but it seems like it's pretty far back in the process.
uh this l.d.a. is different from the l.d.a. that you are talking about.
the l.d.a. that you saying is like you take a block of features like nine frames or something and then do an l.d.a. on it.
yeah.
uhhuh.
and then reduce the dimensionality to something like twenty four or something like that.
yeah you you you can.
and then feed it to h.m.m.
i mean it's you know you're just basically
yeah so this is like a two two dimensional tile.
you're shifting the feature space.
yeah.
so this is a two dimensional tile.
and the l.d.a. that we are applying is only in time.
not in frequency.
high cost frequency.
so it's like more like a filtering in time.
rather than doing a
uh okay.
so what what about um what
i mean i don't know if this is a good idea or not.
but what if you put ran the other kind of l.d.a. uh on your features right before they go into the h.m. m?
uh it
uhhuh.
no actually i think
well.
what do we do with the a.n.n. is is something like that.
except that it's not linear.
but it's it's like a nonlinear discriminant analysis.
yeah.
right.
it's the it's
right.
the so
but
yeah so it's sort of like
the tandem stuff is kind of like nonlinear l.d.a.
yeah.
it's
i
yeah.
yeah.
yeah.
uh
but i mean but the other features that you have um the non tandem ones
uhhuh.
yeah i know.
that that yeah.
well in the proposal they were transformed using p.c.a.
but
uhhuh.
yeah it might be that l.d.a. could be better.
the the argument is kind of in
and it's not like we really know.
but the argument anyway is that um uh we always have the
i mean discriminative things are good.
l.d.a. neural nets they're good.
yeah.
uh they're good because you you you learn to distinguish between these categories that you want to be good at distinguishing between.
and p.c.a. doesn't do that.
it p.a.p.c.a. low order p.c.a. throws away pieces that are uh maybe not not going to be helpful just because they're small basically.
right.
but uh the problem is training sets aren't perfect and testing sets are different.
so you you you face the potential problem with discriminative stuff be it l.d.a. or neural nets that you are training to discriminate between categories in one space.
but what you're really going to be getting is is something else.
uhhuh.
and so uh stephane's idea was uh let's feed uh both this discriminatively trained thing and something that's not.
so you have a good set of features that everybody's worked really hard to make.
yeah.
and then uh you you discriminately train it.
but you also take the path that that doesn't have that.
uhhuh.
and putting those in together.
and that that
so it's kind of like a combination of the uh what uh dan has been calling you know a feature uh you know a feature combination versus posterior combination or something.
it's it's you know you have the posterior combination.
but then you get the features from that and use them as a feature combination with these these other things.
and that seemed at least in the last one as he was just saying he he when he only did discriminative stuff it actually was was it didn't help at all in this particular case.
yeah.
there was enough of a difference i guess between the testing and training.
but by having them both there
the fact is some of the time the discriminative stuff is going to help you.
uhhuh.
and some of the time it's going to hurt you.
and by combining two information sources if you know if if
right.
so you wouldn't necessarily then want to do l.d.a. on the non tandem features because now you're doing something to them that
that
i think that's counter to that idea.
yeah.
now again it's we're just trying these different things.
right.
we don't really know what's going to work best.
but if that's the hypothesis at least it would be counter to that hypothesis to do that.
right.
um
and in principle you would think that the neural net would do better at the discriminant part than l.d.a.
right.
yeah.
though maybe not.
well
yeah.
exactly.
i mean we uh we were getting ready to do the tandem uh stuff for the hub five system.
and um andreas and i talked about it.
and the idea the thought was well uh yeah that you know the neural net should be better.
but we should at least have uh a number you know to show that we did try the l.d.a. in place of the neural net.
so that we can you know show a clear path.
right.
you know that you have it without it.
then you have the l.d.a.
then you have the neural net.
and you can see theoretically.
so
i was just wondering
i i
well i think that's a good idea.
yeah.
did did you do that?
or that's a
um no.
that's what that's what we're going to do next.
as soon as i finish this other thing.
yeah.
so
yeah.
no well that's a good idea.
i i
yeah.
we just want to show.
i mean it everybody believes it.
oh no.
it's a
but you know we just
no no.
but it might not not even be true.
i mean it's it's it's it's it's a great idea.
yeah.
i mean one of the things that always disturbed me uh in the the resurgence of neural nets that happened in the eighties was that um a lot of people because neural nets were pretty easy to to use a lot of people were just using them for all sorts of things without uh looking at all into the linear uh uh versions of them.
yeah.
uhhuh.
and uh people were doing recurrent nets but not looking at i.i.r. filters.
yeah.
and you know i mean uh
so i think yeah it's definitely a good idea to try it.
yeah.
and everybody's putting that on their systems now.
and so
that's what made me wonder about this.
well they've been putting them in their systems off and on for ten years.
but
but but but uh
yeah what i mean is it's it's like in the hub five evaluations you know.
and you read the system descriptions and everybody's got you know l.d.a. on their features.
and now they all have that.
i see.
and so.
yeah.
uh
it's the transformation they're estimating on.
well they are trained on the same data as the final h.m.m. are.
yeah so it's different.
yeah.
exactly.
because they don't have these you know mismatches that that you guys have.
uhhuh.
so that's why i was wondering if maybe it's not even a good idea.
uhhuh.
i don't know.
i i don't know enough about it.
uhhuh.
but um
i mean part of why
i i think part of why you were getting into the k.l.t. you were describing to me at one point that you wanted to see if uh you know getting good orthogonal features was and combining the the different temporal ranges was the key thing that was happening or whether it was this discriminant thing right?
so you were just trying
i think you
i mean this is it doesn't have the l.d.a. aspect.
but as far as the orthogonalizing transformation you were trying that at one point right?
uhhuh.
uhhuh.
i think you were.
yeah.
does something.
it doesn't work as well.
yeah.
yeah.
so yeah.
i've been exploring a parallel v.a.d. without neural network.
with like less latency using s.n.r. and energy um after the cleaning up.
so what i'd been trying was um
uh
after the after the noise compensation i was trying to find a feature based on the ratio of the energies that is after clean and before clean.
so that if if they are like pretty close to one which means it's speech.
and if it is if it is close to zero which is so it's like a scale probability value.
so i was trying uh with full band and multiple bands.
uh separating them to different frequency bands.
and deriving separate decisions on each bands and trying to combine them.
uh
the advantage being like it doesn't have the latency of the neural net if it if it can.
uhhuh.
and it gave me like uh one point one more than one percent relative improvement.
so from fifty three point six it went to fifty four point eight.
so it's like only slightly more than a percent improvement.
uhhuh.
just like
which means that it's it's doing a slightly better job than the previous v.a.d.
uhhuh.
uh at a lower delay.
uhhuh.
um
so um
so
but
i'm sorry.
does it still have the median filter stuff?
it still has the median filter.
so it still has most of the delay.
so
it just doesn't
yeah.
so with the delay that's gone is the input which is the sixty millisecond.
the forty plus twenty.
at the input of the neural net you have this uh nine frames of context plus the delta.
well
uhhuh.
oh plus the delta.
right.
yeah.
okay.
so that delay plus the l.d.a.
uhhuh.
uh so the delay is only the forty millisecond of the noise cleaning plus the hundred millisecond smoothing at the output.
uhhuh.
uhhuh.
um
so yeah.
so the the the biggest
the problem for me was to find a consistent threshold that works well across the different databases.
because i i try to make it work on speechdat-car.
uhhuh.
and it fails on t.i. digits.
or if i try to make it work on that it's just the italian or something it doesn't work on the finnish.
uhhuh.
so
um
so there are there was like some problem in balancing the deletions and insertions when i try different thresholds.
uhhuh.
so
the
i'm still trying to make it better by using some other features from the after the clean up.
maybe some uh correlation auto correlation or some additional features of to mainly the improvement of the vad i've been trying.
now this this this uh before and after clean it sounds like you think that's a good feature.
that that it you think that the uh the it appears to be a good feature right?
uhhuh.
yeah.
what about using it in the neural net?
yeah.
eventually we could could just
yeah.
so yeah.
so that's the
yeah.
so we've been thinking about putting it into the neural net also.
yeah.
because they did that itself.
then you don't have to worry about the thresholds and
there's a threshold.
and yeah.
yeah.
yeah.
so that that's uh
but just
yeah.
so if we if we can live with the latency or cut the latencies elsewhere then then that would be a uh good thing.
yeah.
yeah.
um anybody has anybody you guys or or naren uh somebody tried the uh um second second stream thing?
oh i just i just put the second stream in place and uh ran one experiment.
uh
but just like just to know that everything is fine.
uhhuh.
so it was like uh forty five cepstrum plus twenty three mel log mel.
yeah.
and and just like it gave me the baseline performance of the aurora.
which is like zero improvement.
yeah.
yeah.
so i just tried it on italian just to know that everything is.
but i i didn't export anything out of it.
because it was like a weird feature set.
yeah.
so
yeah.
well what i think you know would be more what you'd want to do is is is uh put it into another neural net.
yeah yeah yeah yeah.
uhhuh.
right?
and then
but
yeah we're we're not quite there yet.
so we have to figure out the neural nets i guess.
yeah.
the uh other thing i was wondering was um if the neural net um has any because of the different noise unseen noise conditions for the neural net.
where like you train it on those four noise conditions while you are feeding it with like additional some four plus some few more conditions which it hasn't seen actually.
uhhuh.
from the while testing.
yeah yeah.
right.
um
instead of just having uh those cleaned up cepstrum should we feed some additional information like the the
we have the v.a.d. flag.
i mean should we feed the v.a.d. flag also at the input so that it it has some additional discriminating information at the input?
uhhuh.
um
uh the the v.a.d. what?
we have the v.a.d. information also available at the back end.
uhhuh.
so if it is something the neural net is not able to discriminate the classes
yeah.
i mean
because most of it is
i mean we have dropped some silence we have dropped silence frames.
uhhuh.
no we haven't dropped silence frames still.
uh still not.
yeah.
yeah.
so
the biggest classification would be the speech and silence.
so by having an additional uh feature which says this is speech and this is nonspeech i mean it certainly helps in some unseen noise conditions for the neural net.
what
do do you have that feature available for the test data?
well i mean we have we are transferring the v.a.d. to the back end.
feature to the back end.
because we are dropping it at the back end after everything all the features are computed.
oh oh i see.
so
i see.
so the neural
so that is coming from a separate neural net or some v.a.d.
okay.
okay.
which is which is certainly giving a
so you're saying feed that also into the neural net.
to
so it's an additional discriminating information.
yeah.
yeah.
right.
so that
you could feed it into the neural net.
the other thing you could do is just um modify the uh output probabilities of the of the uh uh um neural net tandem neural net based on the fact that you have a silence probability.
uhhuh.
right?
uhhuh.
so you have an independent estimator of what the silence probability is.
and you could multiply the two things and renormalize.
uh i mean you'd have to do the nonlinearity part and deal with that.
yeah.
uh i mean go backwards from what the nonlinearity would you know would be.
through to the soft max.
but but uh
yeah so
maybe yeah when
but in principle wouldn't it be better to feed it in?
and let the net do that.
well not sure.
i mean
huh?
let's put it this way.
i mean you you have this complicated system with thousands and thousand parameters.
yeah.
and you can tell it uh learn this thing.
or you can say it's silence.
go away.
i mean
i mean
doesn't
i think i think the second one sounds a lot more direct.
what
what if you
uh
right.
so what if you then uh since you know this what if you only use the neural net on the speech portions?
well uh
that's what
well i guess that's the same.
uh that's similar.
yeah.
i mean you'd have to actually run it continuously.
but it's.
but i mean i mean train the net only on
well no.
you want to train on on the nonspeech also.
because that's part of what you're learning in it.
to to to generate that it's it has to distinguish between.
speech.
but i mean if you're going to if you're going to multiply the output of the net by this other decision uh would then you don't care about whether the net makes that distinction right?
well yeah.
but this other thing isn't perfect.
uh.
so that you bring in some information from the net itself.
right.
okay.
that's a good point.
yeah.
now the only thing that that bothers me about all this is that i i i the the fact
it's sort of bothersome that you're getting more deletions.
yeah.
but
so i might maybe look at
is it due to the fact that um the probability of the silence at the output of the network is
uh
is too high.
too too high?
or
yeah.
so maybe
if it's the case then multiplying it again by by something
so
it may not be it
yeah.
yeah.
uhhuh.
it it may be too it's too high in a sense like everything is more like a um flat probability.
yeah.
so like it's not really doing any distinction between speech and nonspeech.
uh yeah.
or i mean different among classes.
yeah.
uhhuh.
be interesting to look at the
yeah.
for the
i wonder if you could do this.
but if you look at the um highly high the output of the net on the high mismatch case and just look at you know the distribution versus the the other ones do you do you see more peaks or something?
yeah.
yeah like the entropy of the the output.
yeah.
or
yeah for instance.
but i
it it seems that the v.a.d. network doesn't well it doesn't drop uh too many frames.
because the the number of deletion is reasonable.
but it's just when we add the tandem the final m.l.p. and then
yeah.
now the only problem is you don't want to i guess wait for the output of the v.a.d. before you can put something into the other system.
because that'll shoot up the latency a lot.
right?
am i missing something here?
but
uhhuh.
yeah.
right.
yeah.
so that's maybe a problem with what i was just saying.
but
but i guess
but if you were going to put it in as a feature it means you already have it by the time you get to the tandem net.
right?
um
well we we don't have it actually.
because it's it has a high rate energy.
no.
the v.a.d. has a
uh.
yeah.
okay.
it's kind of done in
i mean some of the things are not in parallel.
but certainly it would be in parallel with the with a tandem net.
right.
in time.
so maybe if that doesn't work
um
but it would be interesting to see if that was the problem anyway.
and and and then i guess another alternative would be to take the feature that you're feeding into the v.a.d. and feeding it into the other one as well.
uhhuh.
uhhuh.
and then maybe it would just learn learn it better.
um
but that's
yeah that's an interesting thing to try to see if what's going on is that in the highly mismatched condition it's um causing deletions by having this silence probability up up too high.
uhhuh.
at some point where the v.a.d. is saying it's actually speech.
yeah.
so
which is probably true.
because
well the v.a.
if the v.a.d. said
since the v.a.d. is is is right a lot
uh
yeah.
huh.
anyway.
might be.
uhhuh.
yeah.
well we just started working with it.
but these are these are some good ideas i think.
uhhuh.
yeah and the other thing
well there are other issues maybe for the tandem.
like uh well do we want to uh do we want to work on the targets.
or like instead of using phonemes using more context dependent units.
for the tandem net you mean?
well i'm
yeah.
huh.
i'm thinking also a about dan's work where he he trained a network not on phoneme targets but on the h.m.m. state targets.
and
it was giving slightly better results.
problem is if you are going to run this on different test sets including large vocabulary.
yeah.
yeah.
um
uh
huh.
i think
i was just thinking maybe about like generalized diphones.
and come up with a a reasonable not too large set of context dependent units.
and
and
yeah.
and then anyway we would have to reduce this with the k.l.t.
yeah.
so
but
i don't know.
yeah.
well maybe.
uhhuh.
but i i it it it's all worth looking at.
but it sounds to me like uh looking at the relationship between this and the speech noise stuff is is is probably a key thing.
uhhuh.
that and the correlation between stuff.
so if
uh
if the uh high mismatch case had been more like the uh the other two cases in terms of giving you just a better performance how would this number have changed?
uhhuh.
oh it would be
yeah.
around five percent better i guess.
if if
like sixty?
well we don't know what's it's going to be the t.i. digits yet.
he hasn't got the results back yet.
yeah.
if you extrapolate the speechdat-car well matched and medium mismatch it's around yeah maybe five.
uhhuh.
yeah.
so this would be sixty two?
sixty two.
sixty two.
yeah.
somewhere around sixty must be.
which is
yeah.
right?
yeah.
well it's around five percent because it's
yeah.
right?
if everything is five percent.
yeah.
uhhuh.
all the other ones were five percent.
the
i i i just have the speechdat-car right now.
yeah.
so
yeah.
it's running.
it we should have the results today during the afternoon.
huh.
but
well.
huh.
well.
um
so i won't be here for
when
when do you leave?
uh i'm leaving next wednesday.
may or may not be in in the morning.
i leave in the afternoon.
um
so i
but you're
are you
you're not going to be around this afternoon?
yeah.
oh.
oh well.
i'm talking about next week.
i'm leaving leaving next wednesday.
uhhuh.
this afternoon uh
oh right.
for the meeting meeting.
yeah that's just because of something on campus.
uh.
okay.
okay.
yeah.
but um
yeah.
so next week i won't.
and the week after i won't.
because i'll be in finland.
and the week after that i won't.
by that time you'll be uh you'll both be gone from here.
so there'll be no definitely no meeting on on september sixth.
uh
and
what's september sixth?
uh that's during eurospeech.
oh oh right.
okay.
so uh sunil will be in oregon.
uh stephane and i will be in denmark.
uh
right?
so it'll be a few weeks really before we have a meeting of the same cast of characters.
um
but uh
i guess just
i mean you guys should probably meet.
and maybe barry barry will be around.
and
and then uh uh we'll start up again with dave and dave and barry and stephane and us on the uh twentieth.
no.
thirteenth?
about a month?
so
uh you're going to be gone for the next three weeks or something?
i'm gone for two and a half weeks starting starting next late next wednesday.
so that's you won't be at the next three of these meetings.
is that right?
uh i won't.
it's probably four because of
is it three?
let's see.
twenty third.
thirtieth.
sixth.
that's right.
next three.
and the the third one won't probably won't be a meeting.
because because uh sunil stephane and i will all not be here.
oh right.
right.
um
huh so it's just uh the next two where there will be there you know may as well be meetings.
but i just won't be at them.
okay.
and then starting up on the thirteenth uh we'll have meetings again.
but we'll have to do without sunil here somehow.
when do you go back?
so
thirty first august.
yeah.
yeah.
so
cool.
when is the evaluation?
november?
or something?
yeah.
it was supposed to be november fifteenth.
has anybody heard anything different?
i don't know.
the meeting in is the five and six of december.
it's like yeah it's tentatively all full.
so
yeah.
uhhuh.
uh that's a proposed date i guess.
yeah.
um
so the evaluation should be on a week before.
or
yeah.
yep.
but no this is good progress.
so
uh
okay.
guess we're done.
should we do digits?
digits.
yep.
does anybody have an agenda?
no.
well i'm i sent a couple of items they're they're sort of practical.
i thought somebody had.
i don't know if you're
yeah that's right.
if if that's too practical for what we're focused on.
yeah we only want useless things.
i mean we don't want anything too practical.
yeah.
yeah that would be
no why don't we talk about practical things?
okay.
sure.
well um i can give you an update on the transcription effort.
great.
uh maybe raise the issue of microphone uh um procedures.
with reference to the cleanliness of the recordings.
okay transcription uh microphone issues
and then maybe ask uh these guys.
the we have great great uh steps forward in terms of the nonspeech speech pre-segmenting of the signal.
okay.
well we have steps forward.
well it's a it's a big improvement.
yeah.
yes.
i would prefer this.
yeah well
okay.
uh
we talk about the the results of
you have some
yeah.
okay.
i have a little bit of i ram stuff.
use
but i'm not sure if that's of general interest or not.
uh bigram?
i ram.
i ram.
i ram.
well maybe.
i ram bigram.
bigram.
yeah let's let's see where we are at three thirty.
you know.
huh.
um
since uh since i have to leave as usual at three thirty can we do the interesting stuff first?
i beg your pardon.
well
which is
i beg your pardon.
what's the interesting stuff?
yeah now you get to tell us what's the interesting part.
yeah.
please specify.
but
yeah.
well uh i guess the work that's been done on segmentation would be most.
i think that would be a good thing to start with.
yeah.
okay.
um and um the other thing uh
which i'll just say very briefly that maybe relates to that a little bit
which is that um
uh one of the suggestions that came up in a brief meeting i had the other day when i was in spain.
with uh manolo pardo and javier uh ferreiros who was here before.
was um why not start with what they had before but add in the non silence boundaries.
so in what javier did before when they were doing um
he was looking for uh speaker change points.
uhhuh.
um
as a simplification he originally did this only using silence as uh a putative uh speaker change point.
yeah.
and uh he did not say look at points where you were changing broad uh phonetic class for instance.
and for broadcast news that was fine.
here obviously it's not.
yeah.
and um so one of the things that they were pushing in in discussing with me is um
why are you spending so much time uh on the uh feature issue.
uh when perhaps if you sort of deal with what you were using before.
uhhuh.
and then just broadened it a bit instead of just using silence as putative change point also.
so then you've got you already have the super structure with gaussians and h you know simple h m ms and so forth.
nnn yeah.
and you you might
so there was a there was a little bit of a a a a difference of opinion.
because i i thought that it was it's interesting to look at what features are useful.
yeah.
but uh on the other hand i saw that the they had a good point.
that uh if we had something that worked for many cases before maybe starting from there a little bit.
because ultimately we're going to end up with some kind of structure like that.
yeah.
where you have some kind of simple h m m.
and you're testing the hypothesis that uh there is a change.
yeah.
so
so anyway i just reporting that.
but uh uh
okay.
so yeah why don't we do the speech nonspeech discussion?
yeah.
do i i hear you you didn't
speech nonspeech?
uhhuh.
okay.
yeah.
um so uh what we basically did so far was using the mixed file to to detect speech or nonspeech portions in that.
uhhuh.
and what i did so far is i just used our old munich system.
which is an h m based system with gaussian mixtures for speech and nonspeech.
and it was a system which used only one gaussian for silence and one gaussian for speech.
and now i added uh multi mixture possibility for for speech and nonspeech.
uhhuh.
uhhuh.
and i did some training on on one dialogue which was transcribed by
yeah we we did a speech nonspeech transcription.
jose.
adam dave and i we did.
for that dialogue.
and i trained it on that.
and i did some pre-segmentations for for jane.
and i'm not sure how good they are or what what the transcribers say.
they they can use it?
or
uh they they think it's a terrific improvement.
and um it it just makes a a world of difference.
huh.
and um you also did some something in addition.
which was um for those in which there was uh quiet speakers in the mix.
yeah.
uh yeah.
that that was one one one thing.
uh why i added more mixtures for for the speech.
so i saw that there were loud loudly speaking speakers and quietly speaking speakers.
uhhuh.
and so i did two mixtures one for the loud speakers and one for the quiet speakers.
and did you hand label who was loud and who was quiet?
i did that for for five minutes of one dialogue.
or did you just
right.
and that was enough to to train the system.
yeah.
what
and so it it adapts uh on while running.
so
what kind of uh front end processing did you do?
hopefully.
okay.
it's just our our old munich uh loudness based spectrum.
on mel scale twenty twenty critical bands and then loudness.
uhhuh.
and four additional features which is energy loudness modified loudness and zero crossing rate.
so it's twenty four twenty four features.
uhhuh.
huh.
and you also provided me with several different versions.
which i compared.
yeah.
yeah.
and so you change parameters.
what
do you want to say something about the parameters that you change?
yeah.
you can specify the minimum length of speech or and silence portions which you want.
and so i did some some modifications in those parameters.
basically changing the minimum minimum length for for silence.
to have
to have um
yeah
to have more or less uh silence portions inserted.
so
right so this would work well for uh pauses and utterance boundaries and things like that.
yeah.
yeah.
yeah.
but for overlap i imagine that doesn't work at all.
yeah.
yeah.
that you'll have plenty of sections that are
yeah.
yeah.
that's it.
uhhuh uhhuh.
yeah.
yeah.
that's true.
um
but it it saves so much time the the transcribers.
but
yep.
just enormous enormous savings.
that's great.
fantastic.
um just one quickly uh still on the features.
so you have these twenty four features.
yeah.
uh a lot of them are spectral features.
is there a a transformation uh like principal components transformation or something?
no.
no we originally we did that.
yeah it was i s two.
just
but we saw uh
when we used it uh for our close talking microphone which
yeah for our for our recognizer in munich
we saw that it's it's not it's not so necessary.
it it works as well with with without uh a l d a or something.
okay.
okay no i was curious.
yeah.
yeah i don't think it's a big deal for this application.
uhhuh.
but but yeah it's a
yeah.
right.
uhhuh.
okay but then there's another thing that also thilo's involved with
which is um
okay and and also dave gelbart.
so there's this this problem of
and and so we had this meeting.
the also adam before the the before you went away.
uh we um regarding the representation of overlaps.
because at present um because of the limitations of the interface we're using.
overlaps are uh not being encoded by the transcribers in as complete and uh detailed a way as it might be.
and as might be desired i think would be desired in the corpus ultimately.
uhhuh.
so we don't have start and end points at each point where there's an overlap.
we just have the the overlaps encoded in a simple bin.
well okay so the limits of the of of the interface are such that we were at this meeting we were entertaining how we might either expand the the interface or find other tools which already do what would be useful.
because what would ultimately be um ideal in my my view
and i think i mean i had the sense that it was consensus
is that um a thorough going musical score notation would be the best way to go.
because you can have multiple channels.
there's a single timeline.
it's very clear flexible and all those nice things.
uhhuh.
okay so um
um i spoke i had a meeting with dave gelbart on on and he had uh excellent ideas on how the interface could be modified to to do this kind of representation.
but um he in the meantime you were checking into the existence of already um existing interfaces which might already have these properties.
so do you want to say something about that?
yes.
um i talked with uh munich guys from from ludwig maximilians university who do a lot of transcribing and transliterations.
uhhuh.
and they basically said they have they have uh a tool they developed themselves.
and they can't give away uh it's too error prone.
and had it's not supported.
and
yeah.
but um
susanne burger who is at c m u he who was formally at in munich and and is now at with c m u.
she said she has something.
which she uses to do eight channels uh transliterations.
eight channels simultaneously.
excuse me.
but it's running under windows.
under windows.
uhhuh.
so i'm not sure if if if we can use it.
uhhuh.
she said she would give it to us.
it wouldn't be a problem.
and i've got some some kind of manual down in my office.
well maybe we should get it.
and if it's good enough we'll arrange windows machines to be available.
yeah.
uhhuh we could uh potentially so.
so
i also wanted to be sure
i mean i've i've seen the
this this is called praat p r a a t which i guess means speech in dutch or something.
yep.
yeah but then i'm not sure that's the right thing for us.
but
yeah.
in terms of it being windows versus
but i'm just wondering is
no no praat isn't praat's multi platform.
no no praat
yeah yeah.
oh i see.
yeah.
oh i see.
so praat may not be
that's not praat.
it's called transedit i think.
it's a different one.
i see.
the the uh the tool from from susanne.
oh i see.
okay okay all right.
the other thing uh to keep in mind
uh i mean
we've been very concerned to get all this rolling.
so that we would actually have data.
huh yeah.
but um i think our outside sponsor is actually going to kick in.
uhhuh.
and ultimately that path will be smoothed out.
so i don't know if we have a long-term need to do lots and lots of transcribing.
i think we had a very quick need to get something out.
and we'd like to be able to do some later.
because just it's it's interesting.
but as far you know uh with with any luck we'll be able to wind down the larger project.
oh.
but you
what our decision was is that we'll go ahead with what we have with a not very fine time scale on the overlaps.
yeah.
right.
yeah.
and and do what we can later to clean that up if we need to.
uhhuh.
right.
and and i was just thinking that um if it were possible to bring that in like you know this week
uhhuh.
then when they're encoding the overlaps it would be nice for them to be able to specify when you know the start points and end points of overlaps.
yeah.
uh they're making really quick progress.
that's great.
and um so my my goal was my charge was to get eleven hours by the end of the month.
and it'll be i'm i'm i'm clear that we'll be able to do that.
that's great.
yeah.
and did you uh forward morgan brian's thing?
i sent it to
um
who did i send that to?
i sent it to a list.
and i thought i sent it to the to the local list.
meeting recorder.
oh you did?
you saw that?
okay so you probably did get that.
so brian did tell me that in fact what you said that uh that our
that they are making progress.
and that he's going that they're going he's going to check the the output of the first transcription.
and
i mean basically it's it's all the difference in the world.
and
i mean basically he's he's on it now.
yeah.
oh that's
so so so this is so it'll happen.
this is a new development.
okay.
super.
super.
okay great.
yeah i mean basically it's just saying that one of our one of our best people is on it.
you know who just doesn't happen to be here anymore.
yeah.
someone else pays him.
so
isn't that great?
but about the need for transcription.
i mean don't we didn't we previously decide that the i b m transcripts would have to be checked anyway and possibly augmented?
yeah.
yes that's true.
so i think having a good tool is worth something no matter what.
uhhuh.
yeah.
okay that's that's a good point.
yeah and dave gelbart did volunteer.
good.
and since he's not here i'll repeat it.
to at least modify transcriber.
which if we don't have something else that works i think that's a pretty good way of going.
huh.
uhhuh.
and we discussed on some methods to do it.
my approach originally
and i've already hacked on it a little bit
it was too slow because i was trying to display all the waveforms.
but he pointed out that you don't really have to.
uhhuh.
uhhuh.
i think that's a good point.
huh.
that if you just display the mix waveform and then have a user interface for editing the different channels that's perfectly sufficient.
yeah exactly.
and just keep those things separate.
and and um dan ellis's hack already allows them to be able to display different waveforms to clarify overlaps and things.
so that's already
no they can only display one.
but they can listen to different ones.
oh yes but
well uh yes but what i mean is that uh from the transcriber's perspective uh those two functions are separate.
and dan ellis's hack handles the um choice the ability to choose different wave forms from moment to moment.
but only to listen to.
not to look at.
yeah.
um
the waveform you're looking at doesn't change.
yeah.
that's true.
yeah.
but that's that's okay.
yeah.
because they're they're
you know they're focused on the ear anyway.
right.
huh.
and then and then
the hack to preserve the overlaps better would be one which creates different output files for each channel.
which then would also serve liz's request of having.
right.
you know a single channel.
separable uh cleanly easily separable.
uhhuh.
uh transcript tied to a single channel uh audio.
uhhuh.
have uh folks from nist been in contact with you?
not directly.
i'm trying to think if if i could have gotten it over a list.
okay.
i don't i don't think so.
okay well holidays may have interrupted things.
because in in in
they seem to want to get absolutely clear on standards for transcription standards and so forth with with us.
oh this was from before december.
right because they're they're presumably going to start recording next month.
yeah.
okay okay.
so
oh we should definitely get with them then.
and
agree upon a format.
though i don't remember email on that.
so was i not in the loop on that?
um yeah i don't think i mailed anybody.
i just think i told them to contact jane that uh if they had a
oh okay.
that's right.
if uh that that uh as the point person on it.
but
yeah i think that's right.
just uh
so yeah maybe i'll uh ping them a little bit about it to get that straight.
okay.
i'm keeping the conventions absolutely as simple as possible.
yeah so is it because with any luck there'll actually be a a there'll be collections at columbia.
collections at at u w.
i mean dan dan is very interested in doing some other things.
right.
yeah yeah.
well i think it's important both for the notation and the machine representation to be the same.
and collections at nist.
so
yeah.
so
there was also this uh email from dan regarding the speech non nonspeech segmentation thing.
yep.
yeah.
yeah.
i don't know if
uh
uh we want to
uh
and dan and dave gelbart is interested in pursuing the aspect of using amplitude as a a a as a basis for the separation.
oh yeah he was talking he was talking i mean uh we he had
cross correlation.
yeah cross correlation.
i had mentioned this a couple times before the the commercial devices that do uh uh voice uh you know active miking.
uhhuh.
basically look at the at the energy at each of the mikes.
and and you basically compare the energy here to some function of all of the mikes.
yeah.
so
okay.
yeah.
by doing that you know rather than setting any uh absolute threshold you actually can do pretty good uh selection of who who's talking.
okay.
uh
and those those systems work very well by the way.
i mean so people use them in panel discussions and so forth with sound reinforcement differing in in sort of
uhhuh.
uh
and uh those if
boy the guy i knew who built them built them like twenty twenty years ago.
so they're it's the the techniques work pretty well.
huh.
fantastic.
so
because there is one thing that we don't have right now and that is the automatic um channel identifier.
uhhuh.
that that you know that would help in terms of encoding of overlaps.
the the transcribers would have less uh disentangling to do if that were available.
yeah so i think you know basically you can look at some you have to play around a little bit uh to figure out what the right statistic is.
but
uhhuh.
but you compare each microphone to some statistic based on the on the overall.
uhhuh.
yeah.
uhhuh.
okay.
uh and we also have these we have the advantage of having distant mikes too.
so that you
yeah although the the
using the close talking i think would be much better.
wouldn't it?
yeah.
um i i don't know.
i just it'd be
yeah.
if i was actually working on it i'd sit there and and play around with it.
and and get a feeling for it.
i mean the the the
uh
but
uh you certainly want to use the close talking as a at least.
right.
i don't know if the other would would add some other helpful dimension or not.
uhhuh.
uhhuh.
okay what what are the different uh classes to to code uh the the overlap you will use?
um to code
so types of overlap?
what you you
yeah.
um so at a meeting that wasn't transcribed we worked up a a typology.
yeah.
and um
look like uh you you explaining in the blackboard?
the
yes exactly.
yeah yeah.
that hasn't changed.
so
it the it's basically a two tiered structure where the first one is whether the person who's interrupted continues or not.
and then below that there're subcategories uh that have more to do with you know is it uh simply backchannel.
uhhuh.
or is it um someone completing someone else's thought.
or is it someone introducing a new thought.
right.
and i hope that if we do a forced alignment with the close talking mike that will be enough to recover at least some of the time the time information of when the overlap occurred.
huh.
yeah.
uhhuh well one would
yeah.
we hope.
yeah who knows?
that'd be that'd be nice.
i mean i i i i've
so who's going to do that?
who's going to do forced alignment?
well uh i b m was going to.
um
oh okay.
oh.
and i imagine they still plan to.
but but you know i haven't spoken with them about that recently.
okay.
uhhuh.
well uh my suggestion now is is on all of these things to uh contact brian.
okay i'll do that.
this is wonderful to have a direct contact like that.
yeah.
yeah.
uh well let me ask you this.
yeah.
it occurs to me one of my transcribers told me today that she'll be finished with one meeting um by
uhhuh.
well she said tomorrow.
but then she said you know
but the you know let's let's just uh say
uhhuh.
maybe the day after just to be on the safe side.
i could send brian the um the transcript.
i know these are
uh uh i could send him that if it would be possible or a good idea or not to try to do a forced alignment on what we're on the way we're encoding overlaps now.
well just talk to him about it.
yep.
good.
i mean you know basically he's he just studies he's a colleague.
a friend.
and
yeah.
uh they and and you know the the organization always did want to help us.
super super.
it was just a question of getting you know the right people connected in who had the time.
yeah yeah.
right.
so
um uh
is he on the mailing list?
the meeting recorder mailing
oh!
we should add him.
yeah i i i don't know for sure.
yeah.
did something happen morgan that he got put on this or was he already on it?
add him.
or
no i uh uh
it it i it's
yeah something happened.
i don't know what.
he asked for more work.
huh.
but he's on it now.
that would be like that'd be like him.
he's great.
right.
so uh
where are we?
maybe uh uh brief
well let's why don't we talk about microphone issues?
yeah that'd be great.
that was that was a
um
so one thing is that i did look on sony's for a replacement for the mikes for the head head worn ones.
because they're so uncomfortable.
but i think i need someone who knows more about mikes than i do.
because i couldn't find a single other model that seemed like it would fit the connector.
which seems really unlikely to me.
does anyone like know stores?
or know about mikes who who would know the right questions to ask?
oh i probably would.
i mean my knowledge is twenty years out of date.
but some of it's still the same.
so
uhhuh.
uh so maybe we we can take a look at that.
you couldn't you couldn't find the right connector to go into these things?
yep.
huh.
when i looked they listed one microphone and that's it.
as having that type of connector.
but my guess is that sony maybe uses a different number for their connector than everyone else does.
and and so
uhhuh.
well let's look at it together.
it seems it seems really unlikely to me that there's only one.
and
and there's no adaptor for it?
seems like there'd be a
yeah.
okay.
as i said who knows.
uhhuh.
who who are we buying these from?
um
that'd be
i have it downstairs.
i don't remember off the top of my head.
yeah.
okay.
yeah we we can try and look at that together.
and then uh
just in terms of how you wear them
i mean i had thought about this before.
i mean when when when you use a product like dragon dictate they have a very extensive description about how to wear the microphone and so on.
oh.
but i felt that in a real situation we were very seldom going to get people to really do it.
and maybe it wasn't worth concentrating on.
but
well i think that that's that's a good back off position.
that's what i was saying earlier.
that you know we are going to get some recordings that are imperfect.
and hey that's life.
but i i think that it it doesn't hurt uh the naturalness of the situation to try to have people wear the microphones properly if possible.
uhhuh.
because um the natural situation is really what we have with the microphones on the table.
i mean i think you know in the target applications that we're talking about people aren't going to be wearing head mounted mikes anyway.
oh that's true.
yeah.
yeah.
so this is just for these head mounted mikes are just for use with research.
uhhuh.
and uh it's going to make
yeah.
you know if if andreas plays around with language modeling he's not going to be want to be messed up by people breathing into the microphone.
right.
so it's it's uh uh
well i'll dig through the documentation to dragon dictate and see if they still have the little form.
but it does happen.
right?
i mean and any
yeah.
it's interesting.
uh
i talked to some i b m guys uh last january i think i was there.
and
so people who were working on the on their viavoice dictation product.
yeah.
and they said uh the breathing is really a a terrible problem for them.
to to not recognize breathing as speech.
wow.
so anything to reduce breathing is is is a good thing.
yeah.
well that's the
it seemed to me when i was using dragon that it was really microphone placement helped an in uh an enormous amount.
uhhuh.
so you want it enough to the side so that when you exhale through your nose it doesn't the wind doesn't hit the mike.
right.
uhhuh.
and then uh
everyone's adjusting their microphones of course.
and then just close enough so that you get good volume.
so you know wearing it right about here seems to be about the right way to do it.
yeah.
yeah.
i remember when i was when i i i i used uh um a prominent laboratory's uh uh speech recognizer.
is
uhhuh.
about uh this was boy this was a while ago.
this was about twelve twelve years ago or something.
and um
they were they were perturbed with me.
because i was breathing in instead of breathing out.
and they had models for they they had markov models for breathing out.
but they didn't have them for breathing in.
uh
yeah.
that's interesting.
well what i wondered is whether it's possible to have to maybe use the display at the beginning.
to be able to to judge how how correctly
yeah.
i mean have someone do some routine whatever.
and and then see if when they're breathing it's showing.
i don't know if the if it's
i mean when when it's on you can see it.
i
you can definitely see it.
can you see the breathing?
because i
absolutely.
oh.
absolutely.
yeah.
and so you know i've i've sat here and watched sometimes the breathing.
and the bar going up and down.
and i'm thinking i could say something.
but
i mean i think
i don't want to make people self conscious.
stop breathing.
it it's going to be imperfect.
you're not going to get it perfect.
yeah uhhuh.
and you can do some uh you know first order thing about it.
which is to have people move it uh uh away from being just directly in front of the middle.
yeah.
good.
but not too far away.
yeah
and then you know i think there's not much
because you can't you know interfere
you can't fine tune the meeting that much i think.
right.
it's sort of
yeah.
that's true.
it just seems like if something simple like that can be tweaked and the quality goes you know uh dramatically up then it might be worth doing.
yep.
and then also the position of the mike also.
if it's more directly you'll get better volume.
yeah.
so so like yours is pretty far down below your mouth yeah.
but
uhhuh.
my my feedback from the transcribers is he is always close to crystal clear and and just fantastic to
yeah.
huh yeah.
uhhuh.
i don't know why that is.
well i mean you yeah of course you're you're also
uh your volume is is greater.
but but still i mean they they say
i've been eating a lot.
it makes their their job extremely easy.
uh.
yeah.
and then there's mass.
uhhuh.
anyway.
i could say something about about the
well i don't know what you want to do yeah.
about what?
about the transcribers or anything or
i don't know.
well the other
why don't we do that?
but uh just to to um
one more remark uh concerning the s r i recognizer.
um
it is useful to transcribe and then ultimately train models for things like breath.
and also laughter is very very frequent and important to to model.
uhhuh.
so
if you can in your transcripts mark
so
mark them.
mark very audible breaths and laughter especially.
huh.
they are.
um
they're putting uh so in curly brackets they put inhale or breath.
okay.
uhhuh.
oh great.
it they and then in curly brackets they say laughter.
now they're they're not being awfully precise.
uh so they're two types of laughter that are not being distinguished.
uhhuh.
one is when sometimes someone will start laughing when they're in the middle of a sentence.
uhhuh.
and and then the other one is when they finish the sentence and then they laugh.
so um i i did i did some double checking to look through.
i mean you'd need to have extra
okay.
extra complications like time tags indicating the beginning and ending of of the laughing through the utterance.
it's not so i don't think it's um
and that and what they're doing is in both cases just saying curly brackets laughing after the unit.
as as long as there is an indication that there was laughter somewhere between two words i think that's sufficient.
yeah.
good.
oh.
okay.
against they could do forced alignment.
because
yeah.
actually the recognition of laughter once you um you know is pretty good.
so as long as you can stick a you know a a tag in there that that indicates that there was laughter.
oh i didn't know that.
okay.
that would probably be uh sufficient to train models.
that would be a really interesting prosodic feature.
then
and let me ask
yeah.
and i got to ask you one thing about that.
when
so um
huh.
if they laugh between two words you you'd get it in between the two words.
uhhuh.
but if they laugh across three or four words you you get it after those four words.
right.
does that matter?
yeah.
well the thing that you is hard to deal with is when they speak while laughing.
um and that's uh i don't think that we can do very well with that.
right.
so
yeah.
but um that's not as frequent as just laughing between speaking.
okay.
so
so are do you treat breath and laughter as phonetically or as word models or what?
is it?
huh.
i think he's right.
i i think it's frequent in in the meeting.
yeah.
we tried both.
uh currently um we use special words.
there was a there's actually a word for uh it's not just breathing but all kinds of mouth
uhhuh.
mouth stuff.
mouth mouth stuff.
and then laughter is a is a special word.
how would we do that with the hybrid system?
same thing.
same thing?
so train a phone in the neural net?
yeah.
yeah you oh and each of these words has a dedicated phone.
no.
oh it does?
so the so the the mouth noise uh word has just a single phone.
um that is for that.
right so in the hybrid system we could train the net with a laughter phone and a breath sound phone.
yeah.
yeah.
i mean it's it's it's always the same thing.
yeah.
right?
uhhuh.
i mean you could you could say well let
we now think that laughter should have three sub sub sub units in the the three states uh different states.
yeah.
and then you would have three
i mean you know uh uh it's
and the the the pronunciations the pronunciations are are somewhat nonstandard.
i mean you know you you do whatever you want.
yeah yeah.
yeah.
no.
they actually are
uh it's just a single uh you know a single phone in the pronunciation.
but it has a self loop on it.
so it can
to go on forever?
can go on forever.
and how do you handle it in the language model?
it's just a it's just a word.
we train it like any other word.
it's just a word in the language model.
cool.
yeah.
we also tried um absorbing these uh both laughter and and actually also noise.
and um
yeah.
yes.
okay.
anyway we also tried absorbing that into the pause model.
i mean the the the model that that matches the stuff between words.
uhhuh.
and um
it didn't work as well so.
huh.
okay.
uhhuh.
can you hand me your digit form?
sorry.
i just want to mark that you did not read digits.
okay.
say hi for me.
good.
you you did get me to thinking about
i i'm not really sure which is more frequent.
whether laughing.
i think it may be an individual thing some people are more prone to laughing when they're speaking.
yeah.
yeah i think
i was noticing that with dan in the one that we uh we hand hand segmented.
but i can't
yeah.
that he has these little chuckles as he talks.
yeah okay.
i'm sure it's very individual.
and and
one thing that that we're not doing of course is we're not claiming to uh get be getting a representation of mankind.
in these recordings we have this very very tiny sample of of
yeah.
yeah.
yeah.
speech researchers?
uh yeah.
and yeah right.
speech research.
so uh who knows?
uh
yeah why why don't we just since we're on this vein why don't we just continue with uh what you were going to say about the transcriptions?
and
okay.
um um the i i'm really very i'm extremely fortunate with the people who uh applied and who are transcribing for us.
they are um um uh really perceptive and very um
and i'm not just saying that because they might be hearing this.
because they're going to be transcribing it in a few days.
no they're super they're they very quick.
okay turn the mikes off and let's talk.
yeah i know i am i'm serious.
they're just super.
so i um you know i i brought them in and um trained them in pairs.
because i think people can raise questions.
you know they think about different things.
that's a good idea.
and they think of different
and um
i trained them to uh on about a minute or two of the one that was already transcribed.
this also gives me a sense of
you know i can i can use that later with reference to inter coder reliability kind of issues.
but the main thing was to get them used to the conventions.
and you know the idea of the the size of the unit versus how long it takes to play it back.
so these sort of calibration issues.
and then um i just set them loose.
and they're
they all have already background in using computers.
they're um they're trained in linguistics.
good.
they got
uhhuh.
oh no!
is that good or bad?
well they're very
they'll so one of them said well you know he really said n.
not really and.
yeah.
so what what should i do with that.
yeah.
and i said well for our purposes
yeah.
yeah.
i do have a convention if it's an a noncanonical
that one i think we you know with eric's work i sort of figure we we can just treat that as a variant.
okay.
but i told them if if there's an obvious speech error uh like i said in one thing
yes.
and i gave my my example like i said microfon instead of microphone.
didn't bother
i knew it when i said it.
i remember thinking oh that's not correctly pronounced.
but it but i thought it's not worth fixing.
because often when you're speaking everybody knows what what you mean.
you'll self repair.
yeah.
yeah.
but i have a convention that if it's obviously a noncanonical pronunciation a speech error with you know within the realm of resolution that you can tell in this native english american english speaker you know that i didn't mean to say microfon.
then you'd put a little tick at the beginning of the word.
yeah.
and that just signals that um this is not standard.
and then in curly brackets pron error.
and um and other than that it's word level.
but you know the fact that they noticed you know the nnn he said nnn not and.
what shall i do with that.
i mean they're very perceptive.
and and several of them are trained in i p a.
they really could do phonetic transcription if if we wanted them to.
uhhuh.
well well you know it might be something we'd want to do with some uh small subset of the whole thing.
huh.
where were they when we needed them?
i think
we certainly wouldn't want to do it with everything.
and i'm also thinking these people are a terrific pool.
i mean if uh so i i told them that um we don't know if this will continue past the end of the month.
uhhuh.
and i also
i
i think they know that the data source is limited.
and i may not be able to keep them employed till the end of the month.
even although i hope to.
the other thing we could do actually uh is uh use them for a more detailed analysis of the overlaps.
and
oh that'd be so super.
they would be so so terrific.
right?
i mean this was something that we were talking about.
we could get a very detailed overlap if they were willing to transcribe each meeting four or five times.
right?
one for each participant.
so they could by hand
well that's one way to do it.
yeah.
but i've been saying the other thing is just go through it for the overlaps.
yeah.
uhhuh that's right.
right?
and with the right interface.
given that and and do so instead of doing phonetic uh uh transcription for the whole thing
which we know from the steve's experience with the switchboard transcription is you know very very time consuming.
yeah.
and and you know it took them i don't know how many months to do to get four hours.
and so that hasn't been really our focus.
uh we can consider it.
but i mean the other thing is since we've been spending so much time thinking about overlaps is is maybe get a much more detailed analysis of the overlaps.
yeah.
uhhuh.
but anyway i'm i'm open to our consideration.
that'd be great.
i i don't want to say that by fiat.
huh.
i'm open to every consideration of what are some other kinds of detailed analysis that would be most useful.
yeah.
and uh uh
uhhuh.
huh.
i i i think this year we we actually uh can do it.
oh wonderful.
it's a we have we have due to variations in funding we have we seem to be doing uh very well on money for this this year.
and next year we may have have much less.
so i don't want to hire a
is you mean two thousand one?
calendar year?
or
uh i mean calendar year two thousand one.
okay.
yeah so it's uh it's we don't want to hire a bunch of people a long term staff.
full time.
uhhuh yeah.
yeah.
because the the funding that we've gotten is sort of a big chunk for this year.
but having temporary people doing some specific thing that we need is actually a perfect match to that kind of uh funding.
wonderful.
and then school will start in in the on the sixteenth.
yeah.
so
some of them will have to cut back their hours at that point.
yeah.
are they working full time now?
or
some of them are.
yeah.
wow!
well why i wouldn't say forty hour weeks.
no.
but what i mean is
oh i shouldn't say it that way.
because that does sound like forty hour weeks.
no.
i i i would say they're probably they don't have they don't have other things that are taking away their time.
i don't see how someone could do forty hours a week on transcription.
huh.
but it's
you can't.
yeah yeah.
no you're right.
it's it would be too taxing.
but um they're putting in a lot of
yeah.
and and i checked them over.
i
i i i haven't checked them all.
but just spot checking.
they're fantastic.
i remember when we were transcribing berp uh uh uh ron kay uh volunteered to to do some of that.
i think it would be
and he was the first first stuff he did was transcribing chuck.
and he's saying you you know i always thought chuck spoke really well.
yeah yeah.
well you know and i also thought liz has this uh you know and i do also this this interest in the types of overlaps that are involved.
these people would be great choices for doing coding of that type if we wanted.
we'd have to mark them.
or
uhhuh.
whatever so um
uhhuh.
yeah.
i think it would also be interesting to have uh a couple of the meetings have more than one transcriber do.
uhhuh.
because i'm curious about inter annotator agreement.
yeah.
okay.
yeah.
that'd be i think that's a a good idea.
yeah.
you know there's also the in my mind
i think andreas was leading to this topic.
the idea that um we haven't yet seen the the type of transcript that we get from i b m.
and it may just be you know pristine.
but on the other hand given the lesser interface
because this is you know we've got a good interface.
we've got great headphones
um
it could be that they will uh theirs will end up being a kind of first pass or something.
something like that.
maybe an elaborate one.
because again they probably are going to do these alignments.
which will also clear things up.
that's that's true.
although you have to don't you have to start with a close enough approximation of the of the verbal part to be able to
well that's that's debatable.
right?
i mean so the so the argument is that if your statistical system is good it will in fact uh clean things up.
okay.
right?
okay.
so it's got its own objective criterion.
yeah.
and uh so in principle you could start up with something that was kind of rough
i mean to give an example of um something we used to do uh at one point uh back back when chuck was here
in early times
is we would take
take a word and uh have a canonical pronunciation
and uh if there was five phones in a word you'd break up the word uh into five equal length pieces.
which is completely gross.
wrong.
right?
yeah.
i mean the timing is off all over the place in just about any word.
uhhuh.
okay.
but it's okay.
you start off with that and the statistical system.
then aligns things.
and eventually you get something that doesn't really look too bad.
oh excellent.
okay.
so so i think using a a good aligner um actually can can help a lot.
um but uh
you know they both help each other.
if you have a if you have a better starting point then it helps the aligner.
if you have a good alignment it helps the uh the human in in taking less time to correct things.
so so
excellent.
i guess there's another aspect too.
and i don't know.
uh this this is very possibly a different uh topic.
but uh just let me say with reference to this idea of um higher order organization within meetings.
so like in a
you know the topics that are covered during a meeting with reference to the other uh uses of the data.
uhhuh.
so being able to find where so-and-so talked about such and such.
then um um
i mean i i i did sort of a a rough pass on encoding like episode like level things on the uh transcribed meeting.
uhhuh.
already transcribed meeting.
uhhuh.
and i don't know if um
where that if that's something that we want to do with each meeting.
sort of like a um
it's like a manifest when you get a box full of stuff.
uhhuh.
or or if that's um
i mean i i don't know what uh level of detail would be most useful.
i don't know if that's something that i should do when i look over it or if we want someone else to do or whatever.
uhhuh.
but this issue of the contents of the meeting.
in an outline form.
okay.
yeah.
meaning really isn't my thing.
um
i think it just whoever is interested can do that.
i mean so if someone wants to use that data
okay.
we're running a little short here.
we uh uh trying to
that's fine.
i'm finished.
uh was well you know the thing i'm concerned about is we wanted to do these digits.
and and i haven't heard uh from jose yet.
oh yeah.
oh yes!
so
uhhuh.
okay what do you want?
uh
we could skip the digits.
we don't have to read digits each time.
uh
i i i think it you know another another bunch of digits
more data is good.
okay.
so so i'd like to do that.
yeah sure.
but i think do you maybe
uh
did you prepare some whole thing you wanted us just to see?
or what was that?
yeah it's it's prepared.
yeah.
uh how long a
oh sorry.
i i think it's it's fast.
because uh i have the results uh of the study of different energy without the law length.
uh um uh
in the in the measurement uh the average uh dividing by the by the um variance.
yeah.
um i
the other uh the the last uh meeting
uh i don't know if you remain.
we have problem to with the with with the parameter.
with the representations of parameter.
yes.
because the the valleys and the peaks in the signal uh look like uh it doesn't follow to the to the energy in the signal.
right.
and it was a problem uh with the scale.
uh the scale.
with what?
scale.
scale.
uh and i i change the scale and we can see the the variance.
okay.
but the bottom line is it's still not uh separating out very well.
yeah.
right?
yeah.
okay.
the distribution the distribution is is similar.
so that's
that's that's enough then.
okay.
yeah.
no i mean that there's no point in going through all of that if that's the bottom line really.
yeah.
uhhuh.
so i i think we have to start
yeah.
uh i mean there there's two suggestions really.
which is uh what we said before is that
huh.
yeah.
um
it looks like
at least that you haven't found an obvious way to normalize.
so that the energy is anything like a reliable uh indicator of the overlap.
yeah.
um i i'm i'm still a little think that's a little funny these things seems like there should be.
yeah.
yeah.
but but you don't want to keep uh keep knocking at it if it's if you're not getting any any result with that.
but i mean the other things that we talked about is uh pitch related things and harmonicity related things.
yeah.
so which we thought also should be some kind of a reasonable indicator.
um
but uh
a completely different tack on it is the one that was suggested uh by your colleagues in spain.
yeah.
which is to say don't worry so much about the uh features.
yeah.
that is to say use you know as as you're doing with the speech uh nonspeech use some very general features.
yeah.
and uh then uh look at it more from the aspect of modeling.
yeah.
you know have a have a couple markov models.
yeah.
and
and uh try to try to determine you know when is when are you in an overlap when are you not in an overlap.
huh.
and let the uh uh statistical system determine what's the right way to look at the data.
i i um
yeah.
i think it would be interesting to find individual features and put them together.
i think that you'd end up with a better system overall.
but given the limitation in time and given the fact that javier's system already exists doing this sort of thing.
yeah.
yeah.
uh but uh
its main limitation is that again it's only looking at silences which would
yeah.
maybe that's a better place to go.
yeah.
yeah.
so
uhhuh.
i i i think that uh the possibility uh can be that uh
thilo uh working uh with a new class.
uhhuh.
not only uh nonspeech and speech but uh in in in the speech class.
uhhuh.
dividing uh speech uh of from a speaker and overlapping.
to try to to do uh uh a fast a fast uh experiment to to prove that nnn this uh general feature uh can solve the the the problem.
yeah.
and what
nnn how far is
maybe.
yeah.
and i i have prepared the the pitch tracker now.
uhhuh.
and i hope the the next week i will have uh some results and we we will show we will see uh the the parameter the pitch uh tracking in with the program.
i see.
have you ever looked at the uh uh javier's uh speech segmenter?
and nnn nnn
no.
oh maybe you could you uh show thilo that.
no.
no.
yeah.
yeah.
yeah.
sure.
because again the idea is there the limitation there again was that he was he was only using it to look at silence as a as a as a as a putative split point between speakers.
yeah.
but if you included uh broadened classes then in principle maybe you can cover the overlap cases.
okay.
yeah.
yeah.
yeah but i'm not too sure if if we can really represent overlap with with the detector i i i used up to now.
uh
uhhuh.
the to speech nonspeech as
i think with
uh
it's only speech or it's it's it's nonspeech.
that's right but i think javier's
yeah.
uhhuh.
i think javier's might be able to.
so
it doesn't have the same uh h m m modeling.
yeah.
which is i think a drawback.
okay.
but uh
well it's sort of has a simple one.
huh.
yeah.
right?
it's
does it?
it's just it's just a isn't it just a gaussian?
for each
yeah.
yeah.
huh.
uhhuh.
yeah.
and then he you choose optimal splitting.
oh it doesn't have it doesn't have any temporal uh
i thought it
maybe i'm misremembering but i did not think it had a markov.
yeah.
i i guess i don't remember either.
uh it's been a while.
javier
yeah uh i could have a look at it.
uh
you mean uh uh javier program.
so
uhhuh.
no javier doesn't worked with uh a markov.
yeah i didn't think so.
he only train
oh okay so he's just he just computes a gaussian over potential
yep.
yeah it was only gaussian.
oh i see i see.
and and
this is the idea.
and so i i think it would work fine for detecting overlap.
it's just uh that it
he has the two pass issue that
what he does is as a first pass he he he does um a guess at where the divisions might be and he overestimates.
and that's just a data reduction step.
so that you're not trying at every time interval.
okay.
and so those are the putative places where he tries.
yeah.
yeah.
okay.
and right now he's doing that with silence.
and that doesn't work.
with the meeting recorder.
yeah.
so if we used another method to get the first pass i think it would probably work.
yeah sure.
yeah.
yeah okay.
it's a good method.
as long as the as long the segments are long enough.
yeah.
that's the other problem.
o okay so let me go back to what you had though.
so
yeah.
um
the other thing one could do is couldn't
uhhuh.
i mean it's so you have two categories.
and you have markov models for each.
yeah.
couldn't you have a third category?
so you have uh you have uh nonspeech single person speech and multiple person speech?
he has this on his board actually.
don't you have like those those several different categories on the board?
right?
and then you have a markov model for each?
um
i'm not sure.
i i thought about uh adding uh uh another class too.
but it's not too easy i think the the transition between the different class to model them in in the system i have now.
but it it it could be possible i think.
i see.
i see.
in principle.
yeah i mean i
this is all pretty gross.
i mean the the reason why uh i was suggesting originally that we look at features is because i thought well we're doing something we haven't done before.
yeah.
yeah.
we should at least look at the space and understand
yeah.
yeah.
it seems like if two people two or more people talk at once it should get louder.
yeah.
uh and uh uh there should be some discontinuity in pitch contours.
i had the impression.
yeah.
and uh there should overall be a um smaller proportion of the total energy that is explained by any particular harmonic sequence in the spectrum.
yeah.
yeah.
right.
so those are all things that should be there.
so far um uh jose has has been
uhhuh.
yeah.
by the way i was told i should be calling you pepe.
but
by your friends.
yeah.
but
anyway.
um
yeah.
uh the has has uh been exploring uh largely the energy issue.
and
um
as with a lot of things it is not uh like this it's not as simple as it sounds.
and then there's you know is it energy is it log energy is it l p c residual energy is it is it is it uh delta of those things.
yeah.
uh what is it
obviously just a simple number absolute number isn't going to work.
so it should be with compared to what.
should there be a long window for the normalizing factor and a short window for what you're looking at?
yeah.
or you know how short should they be.
so
he's been playing around with a lot of these different things.
huh.
and and so far at least has not come up with any combination that really gave you an indicator.
yeah.
so
i i still have a hunch that there's it's in there some place.
but it may be given that you have a limited time here it it just may not be the best thing to to to focus on for the remaining of it.
yeah.
to overrule yeah.
so pitch related and harmonic related i'm i'm somewhat more hopeful for it.
yeah.
yeah.
but it seems like if we just want to get something to work
yeah.
yeah.
that uh their suggestion of of
they were suggesting going to markov models.
uh but in addition there's an expansion of what javier did.
and one of those things looking at the statistical component.
one.
yeah.
even if the features that you give it are maybe not ideal for it it's just sort of this general filter bank.
or or cepstrum or something um
yeah.
eee it's in there somewhere probably.
but uh what did you think about the possibility of using the javier software?
uh i mean the uh the uh the bic criterion.
the the to train the the gaussian.
uh using the the mark uh by hand uh uh to distinguish huh to train overlapping zone and speech zone i mean.
uh i i i think that an interesting uh experiment uh could be uh to prove that huh if
we suppose that uh the the first step i mean the the classifier
what worked?
the classifier from javier or classifier from thilo?
what happen with the second step?
i i mean what what happen with the uh the uh the uh the the clustering process?
uhhuh.
using the the gaussian.
you mean javier's?
yeah.
what do you mean?
i i mean that is is enough is enough uh to work well uh to uh separate or to distinguish uh between overlapping zone and uh speaker zone?
because if if we if we uh nnn develop an classifier and the second step doesn't work well uh we have another problem.
i
yeah i had tried doing it by hand at one point with a very short sample.
and it worked pretty well.
but i haven't worked with it a lot.
so what i i i took a hand segmented sample.
nnn yeah.
and i added ten times the amount of numbers at random.
yeah.
oh.
and it did pick out pretty good boundaries.
yeah but is is if
but this was just very anecdotal sort of thing.
but it's possible with my segmentation by hand that we have information about the the overlapping.
right so if we if we fed the hand segmentation to javier's and it doesn't work then we know something's wrong.
uh yeah
the
yeah.
no.
the demonstration by hand segmentation by hand i i i think is the fast experiment.
yeah i think that's probably worthwhile doing.
uhhuh.
uh we can prove that the
whether it'll work or not.
this kind emphasizes parameter and gaussian.
yeah.
yeah.
yep.
do you know where his software is?
have you used it at all?
i i have.
i have.
okay.
so
i i have as well.
so if you need need help let me know.
okay.
let's read some digits.
okay.
uhhuh.
uh
and we are
so i guess this is more or less now just to get you up to date johno.
this is what uh
this is a meeting for me.
um
eva bhaskara and i did.
did you add more stuff to it later?
um why?
um i don't know.
there were like the you know and all that stuff.
but i thought you you said you were adding stuff.
but i don't know.
uh no.
this is
um
ha!
very nice.
um so we thought that we can write up uh an element.
and for each of the situation nodes that we observed in the bayes net.
so
what's the situation like at the entity that is mentioned?
if we know anything about it.
is it under construction?
or is it on fire or something happening to it?
or is it stable?
and so forth.
going all the way um through parking location hotel car restroom riots fairs strikes or disasters.
so is this is a situation are is all the things which can be happening right now?
or what is the situation type?
that's basically just specifying the the input for the what's
oh i see.
why are you specifying it in x m l?
um just because it forces us to be specific about the values here.
okay.
and also i mean this is a what the input is going to be.
right?
so we will uh
this is a schema this is
well yeah i just don't know if this is what the does this is what java bayes takes as a bayes net spec?
no because i mean if we i mean we're sure going to interface to we're going to get an x m l document from somewhere.
right?
and that x m l document will say we are able to we were able to observe that the element um of the location that the car is near.
so that's going to be um
so this is the situational context everything in it.
is that what situation is short for?
situational context?
yep.
okay.
so this is just again an x m l schemata which defines a set of possible uh permissible x m l structures.
which we view as input into the bayes net.
right?
and then we can uh possibly run one of them uh transformations?
that put it into the format that the bayes or java bayes or whatever wants.
are you talking are you talking about the the structure?
well it
i mean when you observe a node.
when you when you say the input to the java bayes it takes a certain format.
uhhuh.
right?
which i don't think is this.
although i don't know.
no it's certainly not this.
nuh.
so you could just couldn't you just run a
x s l yeah.
yeah.
to convert it into the java bayes format?
yep.
okay.
that's that's no problem.
but i even think that um
i mean once once you have this sort of as running as a module
right?
what you want is you want to say okay give me the posterior probabilities of the go there node when this is happening.
right?
when the person said this the car is there it's raining and this is happening.
and with this you can specify the what's happening in the situation and what's happening with the user.
so we get after we are done through the situation we get the user vector.
so this is a
so this is just a specification of all the possible inputs?
yep.
and
all the possible outputs too.
okay.
so we have um for example the uh go there decision node.
which has two elements.
going there and its posterior probability.
and not going there and its posterior probability.
because the output is always going to be all the decision nodes and all the the all the posterior probabilities for all the values.
and then we would just look at the uh struct that we want to look at?
in terms of if if we're only asking about one of the
so like if i'm just interested in the going there node i would just pull that information out of the struct that gets that would that java bayes would output?
um pretty much yes.
but i think it's a little bit more complex.
as if i understand it correctly it always gives you all the posterior probabilities for all the values of all decision nodes.
so when we input something we always get the uh posterior probabilities for all of these.
right?
okay.
so there is no way of telling it not to tell us about the eva values.
yeah wait i agree.
that's yeah use oh uh yeah okay.
so so we get this whole list of of um things.
and the question is what to do with it.
what to hand on.
how to interpret it.
in a sense.
so you said if you i'm only interested in whether he wants to go there or not then i just look at that node.
look which one
look at that struct in the output?
right?
yep.
look at that struct in the the output.
even though i wouldn't call it a struct.
but
well well it's an x m l structure that's being returned.
oh uhhuh.
right?
so every part of a structure is a struct.
yeah.
yeah i just uh i just was abbreviated it to struct in my head and started going with that.
that element or object i would say.
not a c struct that's not what i was trying to
yeah.
though yeah.
okay.
and um
the reason is why i think it's a little bit more complex or why why we can even think about it as an interesting problem in and of itself is
um
so the uh
let's look at an example.
well wouldn't we just take the structure that's outputted and then run another transformation on it that would just dump the one that we wanted out?
yeah we'd need to prune.
right?
throw things away.
well actually you don't even need to do that with x m l.
can't you just look at one specific
yeah exactly the xerxes allows you to say just give me the value of that and that and that.
but we don't really know what we're interested in before we look at the complete at at the overall result.
so the person said um where is x.
and so
we want to know um is does he want info.
on this.
or know the location.
or does he want to go there.
let's assume this is our our question.
sure.
nuh?
so
um
do this in perl.
so we get
okay.
let's assume this is the output.
so
we should be able to conclude from that that
i mean it's always going to give us a value of how likely we think it is that he wants to go there and doesn't want to go there.
or how likely it is that he wants to get information.
but maybe we should just reverse this to make it a little bit more delicate.
so does he want to know where it is or does he want to go there?
he wants to know where it is.
right.
i i i tend to agree.
and if it's if
well now i mean you could
and if there's sort of a clear winner here
and um and this is pretty uh indifferent
then we then we might conclude that he actually wants to just know where uh uh he does want to go there.
uh out of curiosity is there a reason why we wouldn't combine these three nodes into one smaller subnet?
that would just basically be the question for
we have where is x is the question.
right?
that would just be info on or location?
based upon
or go there.
a lot of people ask that if they actually just want to go there.
people come up to you on campus and say where's the library.
you're going to say you're going to say go down that way.
you're not going to say it's it's five hundred yards away from you or it's north of you or it's located
well i mean but the there's so you just have three decisions for the final node that would link these three nodes in the net together.
um
i don't know whether i understand what you mean.
but again in this given this input we also in some situations may want to postulate an opinion whether that person wants to go there now.
the nicest way.
use a cab.
or so
wants to know it wants to know where it is because he wants something fixed there because he wants to visit it or whatever.
so it i mean all i'm saying is whatever our input is we're always going to get the full output.
and some some things will always be sort of too not significant enough.
or or it'll be tight.
you won't it'll be hard to decide.
yep.
but i mean i guess i guess the thing is uh this is another smaller case of reasoning in the case of an uncertainty.
which makes me think bayes net should be the way to solve these things.
so if you had if for every construction
right?
oh!
you could say well there here's the where is construction.
and for the where is construction we know we need to look at this node that merges these three things together.
uhhuh.
as for to decide the response.
and since we have a finite number of constructions that we can deal with we could have a finite number of nodes.
okay.
say if we had to deal with arbitrary language it wouldn't make any sense to do that.
uhhuh.
because there'd be no way to generate the nodes for every possible sentence.
uhhuh.
but since we can only deal with a finite amount of stuff
so basically the idea is to to feed the output of that belief net into another belief net.
yeah so basically take these three things and then put them into another belief net.
but why why why only those three?
why not the
well i mean for the where is question.
so we'd have a node for the where is question.
yeah but we believe that all the decision nodes are can be relevant for the where is
and the where how do i get to.
or the tell me something about.
you can come in if you want.
yes it is allowed.
as long as you're not wearing your your headphones.
well i i see i don't know if this is a good idea or not.
i'm just throwing it out.
but uh it seems like we could have i or uh we could put all of all of the information that could also be relevant into the where is node answer.
uhhuh.
yep.
node.
thing.
stuff.
and uh
okay.
i mean
let's not forget we're going to get some very strong input from these from these discourse things.
right?
so
tell me the location of x.
nuh?
or where is x located at.
we
nuh?
yeah i know but the bayes net would be able to the weights on the on the nodes in the bayes net would be able to do all that.
wouldn't it?
uhhuh.
here's a
oh!
oh i'll wait until you're plugged in.
oh don't sit there.
sit here.
you know how you don't like that one.
it's okay.
that's the weird one.
that's the one that's painful that hurts it hurts so bad.
i'm i'm happy that they're recording that.
that headphone the headphone that you have to put on backwards with the little little thing and the little little foam block on it.
it's a painful painful microphone.
i think it's called the crown.
the crown.
what?
yeah.
versus the sony.
the crown?
is that the actual name?
uhhuh.
okay.
the manufacturer.
i don't see a manufacturer on it.
oh wait.
you
here it is.
this thingy.
yeah it's the crown.
the crown of pain.
yes.
you're online?
are you are your mike is your mike on?
indeed.
okay.
so you've been working with these guys.
you know what's going on?
yes.
i have.
and i do.
yeah all right.
excellent!
so where are we?
we're discussing this.
i don't think it can handle french.
but anyway.
so
assume we have something coming in.
a person says where is x.
and we get a certain we have a situation vector and a user vector and everything is fine.
and and our and our
did you just did you just stick the the the the microphone actually in the tea?
no.
and um
i'm not drinking tea.
what are you talking about?
oh yeah.
sorry.
let's just assume our bayes net just has three decision nodes for the time being.
these three he wants to know something about it he wants to know where it is he wants to go there.
in terms of these would be how we would answer the question where is.
right?
we
this is that's what you it seemed like explained it to me earlier.
we we're we want to know how to answer the question where is x.
yeah but huh
yeah.
no i can i can do the timing node in here too.
and say okay
well yeah but in the uh let's just deal with the the simple case of we're not worrying about timing or anything.
we just want to know how we should answer where is x.
okay.
and um
okay and
go there has two values.
right?
go there and not go there.
let's assume those are the posterior probabilities of that.
uhhuh.
info on has true or false and location.
so he wants to know something about it.
and he wants to know something he wants to know where it is.
excuse me.
has these values.
and um
oh i see why we can't do that.
and um in this case we would probably all agree that he wants to go there.
our belief net thinks he wants to go there.
right?
yeah.
uhhuh.
in the uh whatever.
if we have something like this here.
and
this like that ..
and maybe here also some
you should probably make them out of
yeah.
well it
something like that.
then we would guess aha he our belief net has stronger beliefs that he wants to know where it is than actually wants to go there.
right?
that it doesn't this assume though that they're evenly weighted?
true.
like
i guess they are evenly weighted.
the different decision nodes you mean?
yeah the go there the info on and the location.
well yeah this is making the assumption.
yes.
what do you mean by differently weighted?
like
they don't feed into anything really anymore.
or i
but i mean why do we
if we trusted the go there node more much more than we trusted the other ones then we would conclude even in this situation that he wanted to go there.
so in that sense we weight them equally.
okay.
right now.
makes sense.
yeah.
so but i guess the question that i was wondering or maybe robert was proposing to me is
but
how do we make the decision on as to which one to listen to?
yeah so the final decision is the combination of these three.
so again it's it's some kind of uh
bayes net.
yeah sure.
okay so then the question so then my question is to you then would be
so is the only reason we can make all these smaller bayes nets because we know we can only deal with a finite set of constructions?
because if we're just taking arbitrary language in we couldn't have a node for every possible question.
you know?
a decision node for every possible question you mean.
well i like in the case of yeah.
in the any piece of language we wouldn't be able to answer it with this system if we just
because we wouldn't have the correct node.
basically what you're proposing is a where is node.
right?
yeah.
and and if we and if someone says you know uh something in mandarin to the system wouldn't know which node to look at to answer that question.
so is
yeah.
right?
yeah.
huh
so but but if we have a finite
what?
i don't see your point.
what what what i am thinking or what we're about to propose here is we're always going to get the whole list of values and their posterior probabilities.
and now we need an expert system or belief net or something that interprets that ..
that looks at all the values and says the winner is timing.
now go there.
uh go there timing now.
or the winner is info on function off.
so he wants to know something about it and what it does.
nuh?
uh regardless of of of the input
yeah but but how does the expert but how does the expert system know which one to declare the winner if it doesn't know the question it is and how that question should be answered?
based on the what the question was so what the discourse the ontology the situation and the user model gave us we came up with these values for these decisions.
yeah i know but how do we weight what we get out?
as which one which ones are important.
so my so if we were to it with a bayes net we'd have to have a node for every question that we knew how to deal with.
that would take all of the inputs and weight them appropriately for that question.
uhhuh.
does that make sense?
yay?
nay?
um i mean are you saying that what happens if you try to scale this up to the situation or are we just dealing with arbitrary language?
we
is that your point?
well no i i guess my question is is the reason that we can make a node
or okay so let me see if i'm confused.
are we going to make a node for every question?
does that make sense?
or not?
for every question?
like
every construction.
huh
i don't not necessarily i would think.
i mean it's not based on constructions.
it's based on things like uh there's going to be a node for go there or not and there's going to be a node for enter view approach.
okay.
so someone asked a question.
yeah.
how do we decide how to answer it?
well look at look face yourself with this question.
you get this you'll have this is what you get.
and now you have to make a decision what do we think.
what does this tell us?
and not knowing what was asked and what happened and whether the person was a tourist or a local.
because all of these factors have presumably already gone into making these posterior probabilities.
yeah.
what what we need is a just a mechanism that says aha there is
i just don't think a winner take all type of thing is the
i mean in general like we won't just have those three.
right?
we'll have uh like many many nodes.
yep.
so we have to like so that it's no longer possible to just look at the nodes themselves and figure out what the person is trying to say.
because there are interdependencies.
right?
the uh
uh no so if if for example the go there posterior possibility is so high um
uh if it's if it has reached reached a certain height then all of this becomes irrelevant.
so if even if if the function or the history or something is scoring pretty good on the true node true value
i don't know about that.
because that would suggest that i mean
he wants to go there and know something about it?
do they have to be
yeah do they have to be mutually exclusive?
i think to some extent they are.
or maybe they're not.
because i uh the way you describe what they meant they weren't uh they didn't seem mutually exclusive to me.
well if he doesn't want to go there even if the enter posterior
so
go there is no.
enter is high.
and info on is high.
well yeah just out of the other three though that you had in the
huh?
those three nodes.
they didn't seem like they were mutually exclusive.
no there's no.
but
it's through the
so so yeah but some so some things would drop out and some things would still be important.
uhhuh.
but i guess what's confusing me is if we have a bayes net to deal another bayes net to deal with this stuff.
uhhuh.
you know.
uh
is the only reason okay so i guess if we have a another bayes net to deal with this stuff the only reason we can design it is because we know what each question is asking?
yeah.
i think that's true.
and then so the only reason way we would know what question he's asking is based upon
oh so if let's say i had a construction parser and i plug this in i would know what each construction the communicative intent of the construction was.
uhhuh.
and so then i would know how to weight the nodes appropriately in response.
so no matter what they said if i could map it onto a where is construction i could say uh.
uhhuh.
well the intent here was where is.
okay right.
and i could look at those.
yeah.
yes i mean
sure.
you do need to know i mean to have that kind of information.
huh
yeah i'm also agreeing that a simple
take the ones where we have a clear winner.
forget about the ones where it's all sort of middle ground.
prune those out.
and just hand over the ones where we have a winner.
yeah because that would be the easiest way.
we just compose as an output an x m l message that says go there now enter historical information.
and not care whether that's consistent with anything.
right?
but in this case if we say definitely he doesn't want to go there he just wants to know where it is.
or let's call this this look at
he wants to know something about the history of.
so he said tell me something about the history of that.
now the but for some reason the endpoint approach gets a really high score too.
we can't expect this to be sort of at o point three three three o point three three three o point three three three.
right?
somebody needs to zap that.
you know?
or know there needs to be some knowledge that
we yeah but the bayes net that would merge
i just realized that i had my hand in between my mouth and my uh my and my microphone.
so then the bayes net that would merge there that would make the decision between go there info on and location would have a node to tell you which one of those three you wanted.
and based upon that node then you would look at the other stuff.
yep.
i mean
yep.
does that make sense?
yep it's sort of one of those that's it's more like a decision tree if if you want.
you first look at the lowball ones.
yeah
and then
yeah i didn't intend to say that every possible
okay.
there was a confusion there.
i didn't intend to say every possible thing should go into the bayes net.
because some of the things aren't relevant in the bayes net for a specific question.
like the endpoint is not necessarily relevant in the bayes net for where is until after you've decided whether you want to go there or not.
uhhuh.
right.
show us the way bhaskara.
i guess the other thing is that um yeah
i mean when you're asked a specific question and you don't even
like if you're asked a where is question you may not even look like ask for the posterior probability of the uh e v a node.
right?
because that's what i mean in the bayes net you always ask for the posterior probability of a specific node.
so i mean
you may not even bother to compute things you don't need.
um
aren't we always computing all?
no.
you can compute uh the posterior probability of one subset of the nodes given some other nodes.
but totally ignore some other nodes also.
basically things you ignore get marginalized over.
yeah but that's that's just shifting the problem.
then you would have to make a decision.
okay if it's a where is question which decision nodes do i query.
yeah so you have to make
yeah.
yes.
that's
but i would think that's what you want to do.
right?
huh.
well eventually you still have to pick out which ones you look at.
yeah.
so it's pretty much the same problem.
yeah it's it's it's apples and oranges.
isn't it?
nuh?
i mean maybe it does make a difference in terms of performance computational time.
so either you always have it compute all the posterior possibilities for all the values for all nodes and then prune the ones you think that are irrelevant
uhhuh.
huh
or you just make a a priori estimate of what you think might be relevant and query those.
yeah.
so basically you'd have a decision tree query go there?
if if that's false query this one if that's true query that one.
and just basically do a binary search through the
i don't know if it would necessarily be that uh complicated.
but uh
well in the case of go there it would be.
i mean it
in the case
because if you needed if if go there was true you'd want to know what endpoint was.
and if it was false you'd want to look at either income info on or history.
yeah.
that's true i guess.
yeah so in a way you would have that.
also i'm somewhat boggled by that hugin software.
okay why's that?
i can't figure out how to get the probabilities into it.
like i'd look at
uhhuh.
it's it's boggling me.
okay.
all right.
well hopefully it's fixable.
oh yeah yeah i i just think i haven't figured out what the terms in hugin mean versus what java bayes terms are.
it's there's a
okay.
um by the way are do we know whether jerry and nancy are coming?
so we can figure this out.
or
they should come when they're done their stuff basically whenever that is.
so
what what do they need to do left?
um
i guess jerry needs to enter marks.
but i don't know if he's going to do that now or later.
but uh if he's going to enter marks it's going to take him awhile i guess.
and he won't be here.
and what's nancy doing?
nancy?
um she was sort of finishing up the uh calculation of marks and assigning of grades.
but i don't know if she should be here.
well or she should be free after that.
so
assuming she's coming to this meeting.
i don't know if she knows about it.
she's on the email list.
right?
is she?
okay.
uhhuh.
okay.
because
basically what where we also have decided prior to this meeting is that we would have a rerun of the three of us sitting together.
okay.
sometime this week again.
okay.
and finish up the uh values of this.
so we have uh believe it or not we have all the bottom ones here.
well i
you added a bunch of nodes for
yep.
okay.
we we we have actually what we have is this line.
right?
uh what do the uh structures do?
huh
so the the the for instance this location node's got two inputs.
that one you
four inputs.
huh
four.
those are the bottom things are inputs also.
oh i see.
yeah.
okay that was
okay that makes a lot more sense to me now.
yep.
because i thought it was like that one in stuart's book about you know the
alarm in the dog?
yeah.
yeah.
or the earthquake and the alarm.
sorry yeah i'm confusing two.
yeah there's a dog one too.
but that's in java bayes.
isn't it?
right.
maybe.
but there's something about bowel problems or something with the dog.
yeah.
and we have all the top ones.
all the ones to which no arrows are pointing.
what we're missing are the these.
where arrows are pointing.
where we're combining top ones.
so we have to come up with values for this and this this this and so forth.
and maybe just fiddle around with it a little bit more.
and um
and then it's just uh edges.
many of
edges.
and um we won't meet next monday.
so
because of memorial day?
yep.
we'll meet next tuesday i guess.
yeah.
when's jerry leaving for italia?
on on friday.
which friday?
this this friday.
okay.
oh this friday?
ugh.
this friday.
as in four days?
yep.
or three days.
is he how long is he gone for?
two weeks.
italy huh?
what's uh what's there?
well it's a country.
buildings.
people.
but it's not a conference or anything.
pasta.
he's just visiting.
huh
right.
just visiting.
vacation.
it's a pretty nice place.
in my brief uh encounter with it.
do you guys
oh yeah so part of what we actually want to do is sort of schedule out what we want to surprise him with when when he comes back.
um so
oh i think we should disappoint him.
yeah?
you or have a finished construction parser and a working belief net.
and uh
that wouldn't be disappointing.
i think we should do absolutely no work for the two weeks that he's gone.
well that's actually what i had planned personally.
i had i i had sort of scheduled out in my mind that you guys do a lot of work and i do nothing.
and then i sort of
oh yeah that sounds good too.
sort of bask in in your glory.
but uh do you guys have any vacation plans?
because i myself am going to be um gone.
but this is actually not really important.
just this weekend we're going camping.
yeah i'm want to be this gone this weekend too.
uh
but we're all going to be here on tuesday again?
looks like it.
yeah.
okay then let's meet meet again next tuesday.
and um finish up this bayes net.
and once we have finished it i guess we can um
and that's going to be more just you and me.
because bhaskara is doing probabilistic recursive structured object oriented uh
killing machines.
reasoning machines.
yes.
and um
killing reasoning what's the difference?
wait so you're saying next tuesday is it the whole group meeting?
or just us three working on it or or
uh
the whole group.
and we present our results.
our final.
okay.
definite.
so when you were saying we need to do a re run of like
what?
what like just working out the rest of the
yeah we should do this the upcoming days.
this week?
so this week yeah.
okay.
when you say the whole group you mean the four of us and keith?
and ami might
ami might be here.
and it's possible that nancy will be here.
yep.
so
yeah.
because you know
once we have the belief net done
you're just going to have to explain it to me then on tuesday.
how it's all going to work out.
you know.
we will.
okay because then once we have it sort of up and running then we can start you know defining the interfaces.
and then feed stuff into it.
and get stuff out of it.
and then hook it up to some fake construction parser.
and
that you will have in about nine months or so.
yeah.
yeah.
and um
the first bad version will be done in nine months.
yeah i can worry about the ontology interface.
and you can keith can worry about the discourse.
i mean this is pretty um i mean i i i hope everybody uh knows that these are just going to be uh dummy values.
right?
which
where the
which ones?
so so if the endpoint if the go there is yes and no then go there discourse will just be fifty fifty.
right?
um what do you mean if the go there says no then the go there is
i don't get it.
i don't understand.
um
like the go there depends on all those four things.
yep.
yeah.
but what are the values of the go there discourse?
well it depends on the situation.
if the discourse is strongly indicating that
yeah but uh we have no discourse input.
oh i see the see uh specifically in our situation d and o are going to be uh
yeah.
sure.
so whatever.
so so far we have
is that what the keith node is?
yep.
okay and you're taking it out for now?
or
well this is d
okay this i can i can get it in here.
all the d s are
i can get it in here.
so we have the uh um
let's let's call it keith johno.
node.
johno.
there is an h somewhere printed.
there you go.
yeah people have the same problem with my name.
yeah.
oops!
and um
does does the h go before the a or after the a?
oh in my name?
yeah.
before the a.
okay good.
because you when you said people have the same problem i thought
because my h goes after the uh the
people have the inverse problem with my name.
okay.
i always have to check every time i send you an email a past email of yours to make sure i'm spelling your name correctly.
yeah.
that's good.
i worry about you.
i appreciate that.
but when you abbreviate yourself as the basman you don't use any h s.
basman?
yeah it's because of the chessplayer named michael basman.
who is my hero.
okay.
you're a geek!
it's okay.
how do you
okay.
how do you pronounce your name?
eva.
eva?
yeah.
not eva?
what if i were what if i were to call you eva?
i'd probably still respond to it.
i've had people call me eva.
but i don't know.
no not just eva eva.
like if i take the v and pronounce it like it was a german v.
which is f.
yeah.
um no idea then.
voiced.
what?
it sounds like an f.
i
there's also an f in german.
okay.
which is why i
well it's just the difference between voiced and unvoiced.
yeah.
okay.
as long as that's okay.
um
i mean i might slip out and say it accidentally.
that's all i'm saying.
that's fine.
yeah it doesn't matter what those nodes are anyway.
because we'll just make the weights zero for now.
yep.
we'll make them zero for now.
because it who who knows what they come up with.
what's going to come in there.
okay.
and um
then
should we start on thursday?
okay.
and not meet tomorrow?
sure.
okay.
i'll send an email.
make a time suggestion.
wait.
maybe it's okay so that that that we can that we have one node per construction.
because even in people like they don't know what you're talking about if you're using some sort of strange construction.
yeah they would still sort of get the closest best fit.
well yeah but i mean the uh i mean that's what the construction parser would do.
uh i mean if you said something completely arbitrary it would find the closest construction.
uhhuh.
okay.
right?
but if you said something that was uh
theoretically the construction parser would do that.
but if you said something for which there was no construction whatsoever people wouldn't have any idea what you were talking about.
uhhuh.
like bus dog fried egg i mean.
you know.
or if even something chinese for example.
or something in mandarin yeah.
or cantonese as the case may be.
what do you think about that bhaskara?
i mean
well
but how many constructions do could we possibly have nodes for?
in this system or in
no we.
like when people do this kind of thing.
oh when how many constructions do people have?
yeah.
i have not the slightest idea.
is it considered to be like in are they considered to be like very uh sort of abstract things?
every noun is a construction.
okay so it's like in the thousands.
the
yeah.
any any form meaning pair to my understanding is a construction.
okay.
so
and form starts at the level of noun or actually maybe even sounds.
phoneme.
yep.
yeah.
and goes upwards.
until you get the ditransitive construction.
and then of course the i guess maybe there can be the
can there be combinations of the
discourse level constructions.
yeah.
the giving a speech construction.
rhetorical constructions.
yeah.
yes.
but i mean you know you can probably count count the ways.
i mean
it's yeah i would definitely say it's finite.
yeah.
and at least in compilers that's all that really matters.
as long as your analysis is finite.
how's that how it can be finite again?
nah i can't think of a way it would be infinite.
well you can come up with new constructions.
yeah if the if your if your brain was totally non deterministic then perhaps there's a way to get uh an infinite number of constructions that you'd have to worry about.
but i mean in the practical sense it's impossible.
right because if we have a fixed number of neurons.
yeah.
so the best case scenario would be the number of constructions
or the worst case scenario is the number of constructions equals the number of neurons.
well two to the power of the number of neurons.
right.
but still finite.
okay.
no wait.
not necessarily is it?
we can end the meeting.
i just
can't you use different different levels of activation?
across uh lots of different neurons to specify different values.
uhhuh.
um yeah but there's like a certain level of
there's a bandwidth issue.
right?
yeah so you can't do better than something.
yeah.
okay.
some some introductions are in order.
oh okay.
sorry.
okay.
getting ahead of myself.
so
um for those who don't know
everyone knows me.
this is great.
yay!
um apart from that sort of the old gang johno and bhaskara have been with us from from day one.
hi.
and um they're engaged in in various activities some of which you will hear about today.
ami is um our counselor and spiritual guidance.
and um also interested in problems concerning reference of the more complex type.
oh wow.
well
and um he sits in as a interested participant and helper.
is that a good characterization?
that's pretty good i think.
i don't know.
yeah.
thanks.
okay.
keith is not technically one of us yet.
not yet.
ha ha.
one of us.
but um it's too late for him now.
yeah right.
so
i've got the headset on after all.
um
officially i guess he will be joining us in the summer.
yes.
and um hopefully it is by by means of keith that we will be able to get a a better formal and a better semantic um idea of what a construction is.
and um how we can make it work for us.
additionally his interest um surpasses um english.
because it also entails german.
an extra capability of speaking and writing and understanding and reading that language.
and um is there anyone who doesn't know nancy?
do you do you know nancy?
me?
uhhuh.
i know nancy.
okay.
i made that joke already nancy sadly.
what?
the i don't know myself joke.
you did?
when?
uh before you came in.
oh.
about me or you?
man!
about me.
okay okay.
you could do it about you.
well i didn't know.
yeah.
i didn't mean to be humor copying.
but okay.
sorry.
yes i know myself.
it's okay.
it's a
okay.
and um fey is with us as of six days ago officially.
officially.
yeah.
officially.
but in reality already um much much longer.
and um um next to some some more or less bureaucratic uh stuff with the the data collection she's also the wizard in the data collection.
of oz.
um
it's very exciting.
we're sticking with the term wizard.
yes.
yes.
okay.
and um
not witch like.
wizardette.
wizard.
wizardess.
sorceress i think.
okay.
wizard.
wizard.
okay.
uh by by popular vote.
um
didn't take a vote.
okay.
okay um why don't we get started on that subject anyways?
um so we're about to collect data.
and um the uh the following things have happened since we last met.
when will we three meet again?
and um
more than three of us.
what happened is that um a there was some confusion between you and jerry with the that leading to your talking to catherine snow.
and he was uh he he agreed completely that something confusing happened.
um his idea was to get sort of the the lists of mayors of the department.
the students.
it it's exactly how you interpreted it.
the list of majors in the department?
sort of
majors?
majors majors.
majors?
okay mayor.
mayors.
something i don't know about these.
majors.
the department has many mayors.
majors.
okay.
and um just sending the the little write up that we did on to those email lists.
okay.
yeah yeah yeah.
but
yeah.
uh
so it was really carol snow who was confused not me and not jerry.
yep yep yep.
okay.
so
that's good.
so that is uh
so i should still do that?
yep.
okay.
and using the thing that you wrote up.
and
yep.
okay.
wonderful.
and um we have a little description of asking subjects to contact fey for you know recruiting them for our thing.
and um there was some confusion as to the consent form which is basically that that what what you just signed.
right.
and since we have one already um
did jerry talk to you about maybe using our class?
the students in the undergrad class that he's teaching?
um well he said um we definitely yes.
however there is always more people in a in a uh in a department than are just taking his class or anybody else's class at the moment.
yeah.
and one should sort of reach out and try and get them all.
okay but i guess it's that um people in his class cover a different set.
so
than the
is the cogsci department that you were talking about?
i guess.
uh reaching out to.
see
because we have you know people from other areas.
that's what i suggested to him that people like like jerry and george and et cetera just
yeah.
advertise in their classes as well.
yeah.
or even i could you know i could do the actual
because i mean i i know how to contact our students.
uhhuh.
that's generally the way it's done.
so if there's something that you're sending out you can also um send me a copy.
yeah.
me or bhaskara could either of us could post it to
uh
is it
if it's a general solicitation that you know is just contact you then we can totally post it to the news group.
a mailing list.
uhhuh.
yeah.
yeah.
so
do it.
that's
yeah.
okay.
so you'll send it?
or something?
so
as a matter of fact if you
i can send it.
i'll send it.
you can send it to me.
if
okay.
yeah.
now
don't worry.
we this doesn't concern you anymore robert.
it's fine.
how however i suggest that if you if you look at your email carefully you may think you may find that you already have it.
oops!
already?
really?
oops!
maybe.
okay.
i don't remember getting anything.
we'll see.
anyhow.
um the uh
yeah not only cogsci.
also we will talk about linguistics and of course computer science.
uhhuh.
um and then secondly we had you may remember um the problem with the re phrasing.
that subject always rephrase sort of the task that uh we gave them.
right.
and so we had a meeting on friday talking about how to avoid that.
and it proved finally fruitful in the sense that we came up with a new scenario for how to get the the subject to really have intentions.
and sort of to act upon those.
and um
there the idea is now that next actually we we need to hire one more person to actually do that job.
because it it's getting more complicated.
so if you know anyone interested in in what i'm about to describe tell that person to to write a mail to me or jerry soon.
fast.
um the idea now is to sort of come up with a high level of sort of abstract tasks.
go shopping.
um take in uh a batch of art.
um
visit
do some sightseeing.
blah blah blah blah blah.
sort of analogous to what fey has started in in in compiling compiling here.
and already she has already gone to the trouble of of anchoring it with specific um um entities and real world places you will find in heidelberg.
and um
so out of these these high level categories the subject can pick a couple.
such as if if there is a uh a category in emptying your roll of film the person can then decide okay i want to do that at this place.
sort of make up their own itinerary and and tasks.
and the person is not allowed to take sort of this high level category list with them.
but uh
the person is able to take notes on a map that we will give him and the map will be a tourist's sort of schematic representation with with symbols for the objects.
and so the person can maybe make a mental note that uh yeah i wanted to go shopping here.
and i wanted to maybe take a picture of that.
and maybe um eat here.
and then goes in and solves the task with the system.
i e fey.
and um
and we're going to try out that.
any questions?
so um you'll have those say somewhere what their intention was?
so you still have the the nice thing about having data where you know what the actual intention was.
uhhuh yeah.
but they will um
there's nothing that says you know these are the things you want to do .
so they'll say well these are the things i want to do.
and
right.
so they'll have a little bit more natural interaction.
okay.
hopefully.
uhhuh.
so they'll be given this map which means that they won't have to like ask the system for for like high level information about where things are?
yeah it's a schematic tourist map.
so it'll be uh it'll still require the that information.
and
it it doesn't have like streets on it that would allow them to figure out their way
not not not really the street network.
okay.
nuh.
so you're just saying like what part of town the things are in or whatever?
yeah
and um
the map is more a means for them to have the buildings and their names.
and maybe some major streets and their names.
uhhuh.
and we want to maybe ask them if you have get it sort of isolated street.
the the whatever.
river street.
and they know that they have decided that yes that's where they want to do this kind of action um that they have it with them and they can actually read them.
or sort of have the label for the object.
because it's too hard to memorize all these strange german names.
and then we're going to have another we're going to have another trial run.
i e the first with that new setup tomorrow at two.
and we have a real interesting subject which is ron kay.
for who those who know him he's the founder of i c i.
so he'll he's around seventy years old or something.
i didn't know he was the founder.
that's
okay.
and he also approached me and he offered to help um our project.
and he was more thinking about some high level thinking tasks.
and i said sure we need help.
you can come in as a subject.
and he said okay.
so that's what's going to happen tomorrow.
using this new new um plan.
data.
new new set up.
okay.
yeah.
which i'll hopefully sort of scrape together
but thanks to fey we already have sort of a nice blueprint and i can work with that.
questions?
comments on that?
if not we can move on.
no?
no more questions?
so what's the this is what you made fey?
i'm not sure i totally understand this.
but
huh?
i'm not sure i totally understand everything that's being talked about.
like so so it's just based on like the materials you had about heidelberg?
but i i imagine i'll just catch on.
um are you familiar with with the with the very rough setup of the data?
based on the web site.
yeah at the
oh okay.
there's a web site.
and then you could like um figure out what the
experiment.
right.
uh this is where they're supposed to
it's a tourist information web site.
okay.
so
okay.
talk to a machine and it breaks down and then the human comes on.
yeah.
yeah.
the question is just sort of how do we get the tasks in their head that they have an intention of doing something and have a need to ask the system for something without giving them sort of a clear wording or phrasing of the task.
okay.
okay.
okay.
okay.
because what will happen then is that people repeat repeat or as much as they can of that phrasing.
huh.
um are you worried about being able to identify
okay.
um
the the goals that we've you guys have been talking about are this these you know identifying which of three modes um their question uh concerns.
uhhuh.
so it's like the enter versus view
yeah we we we will sort of get a protocol of the prior interaction.
uhhuh.
right?
that's where the instructor the person we are going to hire um and the subjects sit down together with these high level things.
uhhuh.
uhhuh.
and so
the first question for the subject is so these are things you know we thought a tourist can do.
is there anything that interests you?
uhhuh.
and the person can say yeah sure
this is something i would do.
i would go shopping.
yeah?
uhhuh.
and then we can sort of this instructor can say well uh then you you may want to find out how to get over here.
because this is where the shopping district is.
so the interaction beforehand will give them hints about how specific or how whatever though the kinds of questions that are going to ask during the actual session?
no.
just sort of okay what what what would you like to buy.
and then um okay there you want to buy a whatever cuckoos clocks.
yeah.
okay and there is a store there.
uhhuh.
so the task then for that person is finding out how to get there.
uhhuh.
right?
that's sort of what's left.
uhhuh.
and we know that the intention is to enter because we know that the person wants to buy a cuckoos clock.
okay that's what i mean.
so like those tasks are all going to be um unambiguous about which of the three modes.
hopefully.
right okay.
so
well so the idea is to try to get the actual phrasing that they might use and try to interfere as little as possible with their choice of words.
hopefully.
that they'll be here?
yes in a sense that's exactly the the the idea.
uh uh
which is never possible in a in a in a lab situation.
well the one experiment that that that i've read somewhere it was they used pictures.
nuh?
yep.
uhhuh.
so to to uh actually um uh specify the the tasks.
uh but you know
yeah.
we had exactly that on our list of possible things.
so we uh i even made a sort of a silly thing how that could work how you control you are here.
you you want to know how to get someplace.
and this is the place.
and it's a museum.
and you want to do
and and and there's a person looking at pictures.
so you know this is exactly getting someplace with the intention of entering and looking at pictures.
right.
however not only was the common census were among all participants of friday's meeting was it's going to be very laborious to to make these drawings for each different things.
all the different actions if at all possible.
right.
and also people will get caught up in the pictures.
so all of a sudden we'll get descriptions of pictures in there.
right.
and people talking about pictures and pictorial representations.
huh.
and um
right.
i would i would still be willing to try it.
i mean i'm i'm not saying it's necessary.
but but uh uh uh you might be able to combine you know text uh and and some sort of picture.
and also uh i think it it will be a good idea to show them the text and kind of chew the task.
and then take the test away the the the the the text away.
uhhuh.
yeah.
so that they are not uh guided by by by what you wrote.
we will
but can come up with their with their own
yeah.
they will have no more linguistic matter in front of them when they enter this room.
right.
okay.
then i suggest we move on to the to
we have um uh the e d u project.
let me make one more general remark has sort of two two side uh um actions its um action items that we're dealing with.
one is modifying the smartkom parser and the other one is modifying the smartkom natural language generation module.
and um this is not too complicated.
but i'm just mentioning it.
put it in the framework.
because this is something we will talk about now.
um i have some news from the generation.
do you have news from the parser?
um not
by that look i
yes uh i would really it would be better if i talked about it on friday.
okay.
if that's okay.
yeah.
wonderful.
um did you run into problems or did you run into not having time?
yeah.
but not not any time part.
okay so that's good.
that's better than running into problems.
okay.
and um i i do have some good news for the natural language generation however.
and the good news is i guess it's done.
uh meaning that tilman becker who does the german one actually took out some time and already did it in english for us.
and so the version he's sending us is already producing the english that's needed to get by in version one point one.
so i take it that was similar to the what what we did for the parsing.
yeah.
i i it even though the generator is a little bit more complex.
and it would have been not changing one hundred words but maybe four hundred words.
okay.
but it would have been.
okay.
but this this is i guess good news.
and the uh the time
and especially bhaskara
and uh and um
oh do i have it here?
no.
the time is now pretty much fixed.
it's the last week of april until the fourth of may.
so it's twenty sixth through fourth that they'll be here.
so it's it's extremely important that the two of you are also present in this town during that time.
wait what what are the days?
april twenty sixth to the may fourth?
yeah something like that.
i'll probably be here.
yeah.
it's
you will be here.
there is a
isn't finals coming up then pretty much after that?
finals was that.
yeah it doesn't really have much meaning to grad students.
but final projects might.
okay.
yeah actually that's true.
that
anyway so this is
well i'll be here working on something.
guaranteed.
it's just uh will i be here you know in
uh
i'll be here too actually.
but
huh.
no it's just um you know they're coming for us so that we can bug them.
and ask them more questions.
and sit down together.
and write sensible code.
and they can give some nice talks and stuff.
but uh
just make a
but it's not like we need to be with them twenty four hours a day for the seven days that they're here.
not not unless you really really want to.
they're very dependent.
not unless you really want to.
and they're both nice guys.
so you may may want to.
okay that much from the parser and generator side.
unless there are more questions on that.
so no sample generator output yet?
no.
okay.
it just a mail that you know he's sending me the the the stuff soon.
this is being sent.
uhhuh.
okay.
uhhuh.
and i was completely flabbergasted here.
and i and that's also it's it's going to produce the concept to speech uh blah blah blah information for necessary for one point one in english.
based on the english.
you know in english.
so
i was like okay.
we're done.
we're done!
so that was like one of the first you know the first task was getting it working for english.
so that's basically over now.
is that right?
yeah.
so the basic requirement fulfilled.
um the basic requirement is fulfilled almost.
when andreas stolcke and and his gang
uhhuh.
when they have um changed the language model of the recognizer and the dictionary then we can actually put it all together.
uhhuh.
so the speech recognizer also works.
uhhuh.
uhhuh.
and
you can speak into it and ask for t v and movie information.
toll.
and then when
if if something actually happens and some answers come out then we're done.
uhhuh.
if and they're kind of correct.
so it's not done basically.
huh?
and they kind of are are correct.
right.
it's not just like anything.
perhaps if the answers have something to do with the questions for example.
and they're mostly in english.
so
then um
are they is it using the database?
the german t v movie.
yeah.
okay.
so all the actual data might be german names.
um well actually um
or are they all like american t v programs?
um well
i want to see die dukes von hazard.
the
okay so you don't know how the german dialogue uh the german the demo dialogue actually works.
it works
the first thing is what's you know showing on t v.
and then the person is presented with what's running on t v.
in germany on that day on that evening.
uhhuh.
uhhuh.
and so you take one look at it and then you say well that's really nothing there's nothing for me there.
what's running in the cinemas?
uhhuh.
so maybe there's something better happening there.
and then you get you're shown what movies play which films.
and it's going to be of course all the heidelberg movies and what films they are actually showing.
uhhuh.
and most of them are going to be hollywood movies.
huh.
so american beauty is american beauty.
right?
yeah.
right.
and um
but they're shown like on a screen.
it's a i mean so would the generator
like the english language sentence of it is these are the you know the following films are being shown or something like that.
yeah but it in that sense it doesn't make in that case uh it doesn't really make sense to read them out loud.
right.
so it'll just display
if you're displaying them.
okay.
so we don't have to worry about um
but uh it'll tell you that this is what's showing in heidelberg and there you go.
yeah.
and the presentation agent will go nuh.
okay.
like that the avatar.
okay.
and um
and then you pick pick a movie and and and it shows you the times.
and you pick a time.
and you pick seats and all of this.
so
okay.
pretty straightforward.
okay.
but it's so this time we we are at an advantage.
because it was a problem for the german system to incorporate all these english movie titles.
yeah.
nuh?
right.
but in english that's not really a problem.
uhhuh.
unless we get some some topical german movies that have just come out and that are in their database.
right.
so the person may select huehner rennen or whatever.
chicken run.
okay.
then uh on to the modeling.
right?
yeah yeah i guess.
um then modeling.
there it is.
okay what's the next thing?
yep.
this is very rough but this is sort of what um johno and i managed to come up with.
the idea here is that
this is the uh the schema of the x m l here.
not an example or something like that.
yeah this is not an x m l.
okay.
this is sort of towards an a schema.
nuh?
right.
definition.
the idea is so imagine we have a library of schema.
such as the source path goal.
and then we have forced uh motion.
we have cost action.
uhhuh.
uhhuh.
we have a whole library of schemas.
and they're going to be you know fleshed out in in their real ugly detail.
source path goal.
and there's going to be a lot of stuff on the goal and blah blah blah that a goal can be and so forth.
what we think is
and all the names could should be taken cum grano salis.
so
this is a
the fact that we're calling this action schema right now should not entail that we are going to continue calling this action schema.
but what that means is we have here first of all
on the in the in the first iteration a stupid list of source path goal actions.
actions that can be categorized with
or that are related to source path goal.
okay.
to that schema.
uhhuh.
and we will have you know forced motion and cost action actions.
and then those actions can be in multiple categories at the same time if necessary.
so a push may be in in in both you know push uh in this or this.
forced motion and caused action for instance.
uh
exactly.
okay.
yeah.
also these things may or may not get their own structure in the future.
so this is something that you know may also be a as a result of your work.
in the future we may find out that you know there're really these subtle differences between
um
even within the domain of entering in the light of a source path goal schema that we need to put in fill in additional structure up there.
but it gives us a nice handle.
so with this we can basically um you know slaughter the cow anyway we want.
uh
it it is it was sort of a it gave us some headache.
how do we avoid writing down that we have sort of the enter source path goal?
that this but this sort of gets the job done in that respect.
and maybe it is even conceptually somewhat adequate in a sense that um we're talking about two different things.
we're talking more on the sort of intention level up there.
and more on the this is the your basic bone um schema down there.
uh one question robert.
when you point at the screen is it your shadow that i'm supposed to look at?
yeah it's the shadow.
okay.
whereas i keep looking where your hand is.
and it doesn't
well that wouldn't have helped you at all.
right.
yeah.
basically what this is is that there's an interface between what we are doing and the action planner.
spit right here.
and right now the way the interface is action go and then they have the what the person claimed was the source and the person claimed as the goal passed on.
uhhuh.
and the problem is is that the current system does not distinguish between goes of type going into goes of type want to go to a place where i can take a picture of et cetera.
so this is sort of what it looks like
now some simple go action from it from an object named peter's kirche of the type church to an object named powder tower of the type tower.
this is the uh what the action planner uses?
right?
this is
right currently.
okay.
currently.
and is that and that's changeable?
or not?
yeah well
like are we adapting to it?
or
no.
we this is the output sort of of the natural language understanding.
oh yeah.
uhhuh.
right?
the input into the action planning as it is now.
uhhuh.
uhhuh.
and what we are going to do
we going to
and you can see here
and again for johno please please focus the shadow.
okay.
um we're
uh uh
here you have the action and the domain object.
and and on on
what did you think he was doing?
okay sorry!
i just
a laser pointer would be most appropriate here i think.
eee
yeah i i um have i have no
robert likes to be abstract.
and that's what i just thought he was doing.
you look up here.
okay.
sort of between here and here.
so as you can see this is on one level and we are going to add another um struct if you want i e a rich action description on that level.
uhhuh.
so in the future
so it's just an additional information
exactly.
in the future though the content of a hypothesis will not only be an object and an an action and a domain object but an action a domain object and a rich action description.
right.
that doesn't hurt the current way.
uhhuh.
uhhuh.
which is
which which we're abbreviating as rad.
good.
huh.
rad!
so um you had like an action schema and a source path goal schema.
huh.
huh.
uhhuh.
right?
so how does this source path goal schema fit into the uh action schema?
like is it one of the tags there?
yeah can you go back to that one?
so the source path goal schema in this case
i've if i understand how we described we set this up
um
because we've been arguing about it all week.
but uh we'll hold the the
well in this case it will hold the i mean the the features i guess.
i'm not it's hard for me to exactly
so basically that will store the the object that is
the source will store the object that we're going from.
uhhuh.
the goal will store the the
so the fillers of the role source.
we'll fill those in fill those roles in.
okay.
right?
yeah.
the action schemas basically have extra
see we so those are schemas exist
because in case we need extra information
instead of just making it an attribute and which which is just one thing we we decided to make it's own entity so that we could explode it out later on in case there is some structure that that we need to exploit.
okay so
sorry i just don't
um um um
this is just uh x m l notational.
but um the fact that it's action schema and then sort of slash action schema.
that's a whole
that's a block.
that's a block.
yeah.
whereas source is just an attribute?
is that
no no no.
source is just not spelled out here.
oh okay okay.
source meaning source will be uh will have a name a type maybe a dimensionality.
uhhuh uhhuh.
maybe canonical uh orientation
okay could it it could also be blocked out then as
yeah.
the so
okay.
yeah.
source it will be
you know we'll
we know a lot about sources.
so we'll put all of that in source.
okay.
but it's independent whether we are using the s p g schema in an enter view or approach mode.
uhhuh.
right?
this is just properties of the s p g schema.
we can talk about paths being the fastest the quickest the nicest and so forth.
uh or or and the trajector should be coming in there as well.
okay.
and then the same about goals.
okay.
so i guess the question is when you actually fill one of these out it'll be under action schema?
those are
it's going to be one you'll pick one of those for
right.
okay these are this is just a layout of the possible that could go play that role.
right so the the the roles will be filled in with the schema.
huh?
okay go it.
uhhuh.
and then what actual action is chosen is will be in the in the action schema section.
okay.
okay.
okay so one question.
this was in this case it's all um clear sort of obvious.
but you can think of the enter view and approach as each having their roles.
right?
the i mean it's it's implicit that the person that's moving is doing entering viewing and approaching.
but you know the usual thing is we have bindings between sort of
they're sort of like action specific roles and the more general source path goal specific roles.
so are we worrying about that or not for now?
yes yes.
okay.
since you bring it up now we will worry about it.
okay.
tell us more about it.
what do you what do you
what's that?
oh i guess it
i i may be just um reading this and interpreting it into my head in the way that i've always viewed things.
huh.
huh.
and that that may or may not be what you guys intended.
but if it is then the top block is sort of like
um you know you have to list exactly what x schema
or in this action schema
there'll be a certain one that has its own structure.
and maybe it has stuff about that specific to entering or viewing or approaching.
but those could include roles like the thing that you're viewing the thing that you're entering the thing that you're
whatever.
you know that which are
so very specific role names are viewed thing entered thing.
think think of enter view and approach as frames.
and they have frame specific parameters and and roles.
yeah.
uhhuh.
yeah.
and you can also describe them in a general way as source path goal schema.
and maybe there's other image schemas that you could you know add after this that you know how do they work in terms of you know a force dynamics?
uhhuh.
or how do they work in terms of other things?
uhhuh uhhuh uhhuh.
so all of those have um basically either specific frame specific roles or more general frame specific roles that might have binding.
so the question is are um how to represent when things are linked in a certain way?
so we know for enter that there's container potentially involved.
uhhuh.
and it's not
uh i don't know if you want to have in the same level as the action schema s p g schema.
it it's somewhere in there that you need to represent that there is some container.
and the interior of it corresponds to some part of the source path goal um you know goal uh goal i guess in this case.
uhhuh.
so uh is there an easy way in this notation to show when there's identity basically between things?
yeah.
and i don't know if that's something we need to invent.
or you know just
the
wasn't there supposed to be a link in the
right.
i don't know if this answers your question.
i was just staring at this while you were talking.
sorry.
it's okay.
uh a link between the action schema a field in the in the schema for the image schemas that would link us to which action schema we were supposed to use.
so we could
yeah.
um well that's that's one one thing is that we can link up.
think also that um we can have one or as many as we want links from from the schema up to the action um description of it.
huh.
but the notion i got from nancy's idea was that we may find sort of concepts floating around in the action description of the action enter frame up there that are when you talk about the real world actually identical to the goal of the the source path goal schema.
exactly.
right right.
and do we have means of of telling it within that.
right.
and the answer is absolutely.
yeah.
the way we absolutely have those means that are even part of the m three l a a p i.
oh great.
uhhuh.
meaning we can reference.
so meaning
great.
that's exactly what is necessary.
yeah.
and um
this referencing thing however is of temporary nature.
because sooner or later the w three c will be finished with their x path uh um specification.
and then it's going to be even much nicer.
then we have real means of pointing at an individual instantiation of one of our elements here.
uhhuh.
uhhuh.
and link it to another one and this not only within a document but also via documents.
uhhuh.
okay.
and and all in a very easy homogenous framework.
so you know happen to know how what what sooner or later means like in practice?
that's.
or estimated.
but it's soon.
okay okay.
so it's it's the spec is there.
and it's going to part of the m three l a p a p i filed by the end of this year.
so that this means we can start using it basically now.
uhhuh.
but this is a technical detail.
so a pointer a way to really say pointers.
basically references from the roles in the schema the bottom schemas to the action schemas is uh i'm assuming.
yeah.
okay yeah.
yeah.
yeah.
okay.
i mean personally i'm looking even more forward to the day when we're going to have x forms which is a form of notation where it allows you to say that if the s p g action up there is enter then the goal type can never be a statue.
uhhuh.
right.
uhhuh.
so you have constraints that are dependent on the actual specific filler uh of some attribute.
uhhuh yeah.
yeah exactly.
um you know this of course does not make sense in light of the statue of liberty.
uhhuh.
right.
however it is uh you know sort of these sort of things are imaginable.
tsk
yeah.
or the gateway arch in st.
yeah?
so um like are you going to have similar schemas for f m?
louis.
yeah.
so
like forced motion and caused action and stuff.
like you have for s p g.
yeah.
and if so like can are you able to enforce that you know if if it's if it's s p g action then you have that schema.
if it's a forced motion then you have the other schema present in the
um we have absolute
no.
we have absolutely no means of enforcing that.
so it would be considered valid if we have an s p g action enter and no s p g schema but a forced action schema.
could happen.
which is not bad.
because i mean that there's multiple
i mean that particular case there's there there's a forced side of of that verb as well.
huh.
it maybe it means we had nothing to say about the source path goal.
okay.
what's also nice and for for me in my mind.
it's it's crucially necessary is that we can have multiple schemas and multiple action schemas in parallel.
right.
and um we started thinking about going through our bakery questions.
so when i say is there a bakery here.
you know i do ultimately want our module to be able to first of all tell the rest of the system hey this person actually wants to go there.
and b that person actually wants to buy something to eat there.
nuh?
and if these are two different schemas
i e the source path goal schema of getting there and then the buying snacks schema
nuh?
would they both be listed here in
yes.
okay.
under so under action schema there's a list that can include both both things.
yeah they they would both schemas would appear.
right.
so what is the
snack action.
uh is is there a buying snacks schema?
that's interesting.
what is the uh have
what?
the buying snack schema.
see.
buying buying his food
i'm sure there's a commercial event schema in there somewhere.
oop.
yeah.
a commercial event or something.
yeah i i
yeah?
so uh so we would we would instantiate the s p g schema with a source path goal blah blah blah.
i see.
and the buying event you know at which however that looks like the place thing to buy.
uhhuh.
uhhuh.
interesting.
would you say that the like
i mean you could have a flat structure and just say these are two independent things.
but there's also this sort of like causal.
well so one is really facilitating the other and it's part of a compound action of some kind which has structure.
yeah.
now it's technically possible that you can fit schema within schema.
uh i i think that's nicer for a lot of reasons.
and schema within schemata
but might be a pain.
so uh
um
well for me it seems that uh
i mean there are truly times when you have two totally independent goals that they might express at once.
yes.
but in this case it's really like there's a means that you know for achieving some other purpose.
well if i'm if i'm recipient of such a message.
and i get a source path goal where the goal is a bakery.
and then i get a commercial action which takes place in a bakery.
right?
uhhuh.
and and and they they are obviously via identifiers identified to be the same thing here.
yeah.
see that that bothers me that they're the same thing.
no no just the
yeah?
yeah.
because they're two different things.
one of which is
you could think of one a sub you know whatever pre condition for the second.
yeah yeah.
right.
yeah yeah.
so so okay.
so there's like levels of granularity.
so uh there's there's um a single event of which they are both a part.
and they're independently they they are events which have very different characters as far as source path goal whatever.
uhhuh yeah.
so when you identify source path goal and whatever there's going to to be a desire whatever eating hunger.
whatever other frames you have involved they have to match up in in nice ways.
so it seems like each of them has its own internal structure and mapping to these schemas.
uhhuh.
you know from the other
but you know that's just that's just me.
like i i
well i think we're going to hit a lot of interesting problems.
and as i prefaced it this is the result of one week of arguing about it.
uhhuh.
between you guys?
uh
yeah.
okay.
and um and so
yeah i mean i i still am not entirely sure that i really fully grasp the syntax of this.
you know like what
well it's not it's not actually a very actually it doesn't actually
um it occur it occurs to me that i mean
right.
or the intended interpretation of this.
yeah.
um well i should have we should have added an an x m l example.
or some x m l examples.
yeah.
yeah that would be that would be nice.
and and this is on on a on on my list of things until next next week.
okay.
it's also a question of the recursiveness and and a hierarchy um in there.
yeah.
yeah.
do we want the schemas just blump blump blump blump?
i mean it's if we can actually you know get it so that we can out of one utterance activate more than one schema i mean then we're already pretty good.
uhhuh.
right?
well well you have to be careful with that uh uh thing.
because uh i mean many actions presuppose some um almost infinitely many other actions.
so if you go to a bakery you have a general intention of uh not being hungry.
yeah.
yeah.
you have a specific intentions to cross the traffic light to get there.
uhhuh.
yeah.
you have a further specific intentions to left to lift your right foot.
huh.
and so uh uh i mean you really have to focus on on on
right.
and decide the level of of abstraction that that you aim at it kind of zero in on that.
right.
yeah.
and more or less ignore the rest unless there is some implications that that you want to constant draw from from sub tasks um that are relevant.
uh i mean but very difficult.
the other thing that i just thought of is that you could want to go to the bakery because you're supposed to meet your friend there or
yeah.
uhhuh.
you know so you like being able to infer the second thing is very useful and probably often right.
well the the the utterance was is there a bakery around here.
but having them separate
not i want to go to a bakery.
well maybe their friend said they were going to meet them in a bakery around the area.
right.
and i'm yeah i'm i'm inventing contexts which are maybe unlikely.
right.
sure it
okay.
but yeah i mean like
yeah.
but it's still the case that um you could you could override that default by giving extra information.
uhhuh yeah.
which is to me a reason why you would keep the inference of that separate from the knowledge of okay they really want to know if there's a bakery around here.
yeah.
which is direct.
well there there there should never be a hard coded uh shortcut from the bakery question to the uh double schema thing.
right.
how uh
and as a matter of fact when i have traveled with my friends we make these exactly these kinds of appointments.
uhhuh.
yeah.
uhhuh.
exactly.
we
it's i met someone at the bakery you know in the victoria station you know train station london before.
uhhuh.
right.
yep.
well
yeah.
it's like
i have a question about the slot of the s p g action.
so the enter view approach the the the eva um those are fixed slots in this particular action.
every action of this kind will have a choice.
or or or or will it just
um uh
every s p g every s p g action either is an enter or a view or an approach.
is it change
right right.
uhhuh.
right?
okay.
so so i i mean for for each particular action that you may want to characterize you would have some number of slots that define uh uh uh you know in some way what this action is all about.
it can be either a b or c.
um so is it a fixed number or or do you leave it open it could be between one and fifteen?
uh
it's it's it's flexible.
um the uh
well it sort of depends on on if you actually write down the the schema.
then you have to say it's either one of them.
or it can be none.
or it can be any of them.
however the uh it seems to be sensible to me to to view them as mutually exclusive.
um
maybe even not.
do you mean within the source path goal actions?
uh uh uh uh i i understand.
yeah.
those three?
uh
but
and um how how where is the end?
so that's
no no.
there actually by i think my question is simpler than that.
um is
okay so you have an s p g action.
and and it has three different um uh aspects.
um
because you can either enter a building or view it or or approach it and touch it or something.
um now you define uh another action.
forced action or forced motion.
it's it's called um uh s p g one action that has to do with writing a letter let's say.
yeah.
i mean not even within this context but a different action.
um and this uh action two would have various variable possibilities of interpreting what you would like to do.
and in in a way similar to either enter view approach you may want to send a letter read a letter or dictate a letter let's say.
oh the okay.
so
uh maybe i'd
the uh
these actions
i don't know if i'm going to answer your question or not with this.
but the categories inside of action schemas
so s p g action is a category.
although i think what we're specifying here is this is a category where the actions enter view and approach would fall into.
because they have a related source path goal schema in our tourist domain.
because viewing in a tourist domain is going up to it and or actually going from one place to another to take a picture in this in a
right.
oh so it's sort of automatic derived from the structure that that is built elsewhere.
derived
i don't know if i
this is a this a category structure here.
right?
action schema.
right.
what are some types of action schemas?
well one of the types of action schemas is source path goal action.
and what are some types of that?
and an enter a view an approach.
right.
huh.
those are all source path goal actions.
inside of enter there will be roles that can be filled basically.
so if i want to go from outside to inside then you'd have the roles that need to filled where you'd have a source path goal set of roles.
so you'd the source would be outside and path is to the door or whatever.
right?
right.
so if you wanted to have a new type of action you'd create a new type of category.
then this category would we would put it
or not necessarily.
we would put a new action in the uh in the categories that in which it has the
um
well every action has a set of related schemas.
like source path goal or force whatever.
uhhuh.
right?
right.
so we would put write a letter in the categories uh that in which it had it had uh schemas
there could be a communication event action or something like that.
uhhuh.
exactly.
and you could write it.
schemas uh that of that type.
and then later you know there the we have a communication event action where we'd define it down there as
huh.
so there's a bit a redundancy.
right?
in in which the things that go into a particular you have categories at the top under action schema.
and the things that go under a particular category are um supposed to have a corresponding schema definition for that type.
so i guess what's the function of having it up there too?
i mean i guess i'm wondering whether you could just have.
under action schema you could just sort of say whatever you know it's going to be enter view or approach or whatever number of things.
uhhuh.
and partly because you need to know somewhere that those things fall into some categories.
and it may be multiple categories as you say which is um the reason why it gets a little messy.
yeah.
um but if it has if it's supposed to be categorized in category x then the corresponding schema x will be among the structures that that follow.
right.
yeah.
well this is one of things we were arguing about.
that's like
this is this
okay.
sorry.
this is this is more this is probably the way that that's the way that seemed more intuitive to johno i guess.
you didn't tell me to
also for a while for
uhhuh.
but now you guys have seen the light.
no no no uh we have not we have not seen the light.
so
no.
it's easy to go back and forth.
the the reason one reason we're doing it this way is in case there's extra structure that's in the enter action that's not captured by the schemas.
isn't it?
uhhuh.
i agree.
right.
right.
right?
which is why i would think you would say enter.
and then just say all the things that are relevant specifically to enter.
and then the things that are abstract will be in the abstract things as well.
and that's why the bindings become useful.
right but
you'd like so you're saying you could practically turn this structure inside out?
or something?
or
um i see what you mean by that.
no basically
but i i don't if i would i would need to have have that.
get get rid of the sort of s p g slash something.
right.
uh or the sub actions category.
because what does that tell us?
uhhuh.
yeah.
um and i agree that you know this is something we need to discuss.
in fact what you could say is for enter
yeah.
you could say here list all the kinds of schemas that on the category that
list all the parent categories.
you know
list all the parent categories.
it's just like a frame hierarchy.
right?
yeah.
like you have these blended frames.
uhhuh.
so you would say enter and you'd say my parent frames are such and such.
and then those are the ones that actually you then actually define and say how the roles bind to your specific roles.
which will probably be richer and fuller and have other stuff in there.
yeah.
this sounds like a paper i've read around here recently in terms of
yeah it could be not a coincidence.
like i said i'm sure i'm just hitting everything with a hammer that i developed.
yeah.
but i mean you know uh it's
i'm just telling you what i think.
you just hit the button and it's like
and i guess uh
yeah i mean but there's a good question here.
like i mean uh do you when do you need
damn this headset!
when you this uh
metacomment.
uh
yeah that's all recorded.
um
damn this project!
why do you
no just kidding.
i don't know like
how do i how do i come at this question?
um
i just don't see why you would
i mean does
who uses this uh this data structure?
you know like do you say all right i'm going to uh do an s p g action?
and then you know somebody either the computer or the user says all right well i know i want to do a source path goal action so what are my choices among that.
and oh okay so i can do an enter view approach.
it's not like that.
right?
it's more like you say i want to uh i want to do an enter.
well only one of
and then you're more interested in knowing what the parent categories are of that.
right?
so that the um the uh sort of representation that you were just talking about seems more relevant to the kinds of things you would have to do?
huh.
i'd
i think i'd
i'm not sure if i understand your question.
only one of those things are going to be lit up when we pass this on.
okay.
so only enter will be
if we if our if our module decided that enter is the case view and approach will not be there.
okay.
okay.
well uh it's it sort of came into my mind that sometimes even two could be on and would be interesting.
yeah.
um nevertheless um
well maybe i'm not understanding where this comes from and where this goes to.
well in that case we can't we can't if if
okay.
let's let's not
well the thing is if that's the case we our i don't think our system can handle that currently.
what are we doing with this?
no not at all.
but so
in principle.
approach and then enter.
the i think the in some sense we we get the task done extremely well.
run like this.
uh
because this is exactly the discussion we need need.
uhhuh.
period.
no more qualifiers than that.
no this is the useful.
so
you know don't worry.
and um and and i i hope
um uh
let's make a a a a sharper claim.
we will not end this discussion anytime soon.
yeah.
i can guarantee that.
and it's going to get more and more complex the the complexer and larger our domains get.
sigh.
and i think um we will have all of our points in writing pretty soon.
so this is nice about being being recorded also.
right.
the um
that's true.
the uh the in terms of why is it's laid out like this versus some other
the people
yeah.
yeah.
um that's kind of a contentious point between the two of us.
but this is one so this is a way to link uh the way these roles are filled out to the action.
in my view.
uhhuh.
because if we know that enter is a is an s p g action
uhhuh.
right?
we know to look for an s p g schema and put the appropriate fill in the appropriate roles later on.
uhhuh.
and you could have also indicated that by saying enter what are the kinds of action i am.
yeah.
uhhuh yeah.
right.
yeah.
right?
so there's just like sort of reverse organization.
right?
so like unless are there reasons why one is better than the other i mean that come from other sources?
again
yes because the modules don't
yeah.
uh
this is this is a schema that defines x m l messages that are passed from one module to another.
uhhuh.
mainly meaning from the natural language understanding or from the deep language understanding to the action planner.
uhhuh.
now the the reason for for not using this approach is because you always will have to go back.
each module will try have to go back to look up which uh you know entity can have which uh you know entity can have which parents.
and then
so you always need the whole body of of your model um to figure out what belongs to what.
or you always send it along with it.
uhhuh.
uhhuh.
nuh?
so you always send up here i am i am this person and i can have these parents.
uhhuh.
in every message.
okay so it's just like a pain to have to send it.
which
it may or may not be a just a pain.
it's it's
i'm completely willing to to to throw all of this away.
okay i understand.
and completely redo it.
well
you know and and and it after some iterations we may just do that.
uhhuh.
i i would just like to ask um like if it could happen for next time.
i mean just because i'm new.
and i don't really just i just don't know what to make of this.
uhhuh.
and what this is for and stuff like that you know.
so if someone could make an example of what would actually be in it
yeah.
like first of all what modules are talking to each other using this.
yeah we i will promise for the next time to have fleshed out n x m l examples for a a run through and and see how this this then translates.
right?
and
okay.
be great.
and how this can come about.
nuh?
including the sort of miracle occurs here um part.
right.
and um
is there more to be said?
i think
um
in principle what i i think that this approach does
and whether or not we take the enter view and we all throw up up the ladder
um how do how does professor peter call that?
yeah.
the uh silence sublimination?
throwing somebody up the stairs?
have you never read the peter's principle?
nope.
anyone here?
oh uh
people reach their level of uh their level of at which they're incompetent or whatever.
yeah.
maximum incompetence.
and then you can throw them up the stairs.
yeah.
all right.
right right.
oh!
um yeah.
promote them.
yeah.
okay so we can promote enter view all all up a bit.
and and get rid of the uh blah blah x blah uh asterisk sub action item altogether.
okay.
no no problem with that.
and we we we will play around with all of them.
but the principal distinction between having the the pure schema and their instantiations on the one hand and adding some whatever more intention oriented specification um on parallel to that that this approach seems to be uh workable to me.
i don't know.
if you all share that opinion then that made my day much happier.
uh yeah wait
this is a simple way to basically link uh roles to actions.
yeah yeah that's fine.
sure.
that's the that was the intent of of it basically.
uh that's true.
sure.
although um roles
yeah.
so i i i'm i'm not
yeah i i
i'm i'm never happy when he uses the word roles.
yeah.
i was going to
i'm
i i mean r o l l s.
so
bread rolls?
oh you meant pastries then?
yeah pastries is what i'm talking about.
pastry
oh oh the bakery example.
bakery.
this is the bakery example.
bakery.
i see.
got it all right.
right okay.
help!
i guess i'll agree to that then.
okay.
that's all i have for today.
oh no!
there's one more issue.
bhaskara brought that one up.
meeting time rescheduling.
i didn't you say something about friday?
or
yeah.
huh.
so it looks like
you have not been partaking
the monday at three o'clock time has turned out to be not good anymore.
so people have been thinking about an alternative time.
and the one we came up with is friday two thirty.
three?
what was it?
you have class until two.
right?
so if we don't want him if we don't want him to run over here.
uhhuh.
two two-thirty-ish or three or friday at three or something around that time.
so do i.
yeah.
uhhuh.
two thirtyish or three is
yeah.
yeah.
um how how are your
that would be good.
uh friday
uh yeah that's fine.
uh
and i know that you have until three you're busy.
yeah.
so three is sounds good.
yeah.
yeah.
i'll be free by then.
i could do that.
yeah i mean earlier on friday is better but three you know i mean if it were a three or a three thirty time then i would take the three or whatever.
uhhuh.
but yeah sure three is fine.
yeah.
and you can always make it shortly after three probably.
i mean
yeah and i don't need to be here particularly deeply.
often.
no but uh
yeah.
whenever.
but yeah.
you are more than welcome if you think that this kind of discussion gets you anywhere in in your life then uh you're free to
it's fascinating.
that's the right answer.
i'm just glad that i don't have to work it out.
because
yeah.
huh?
i'm just glad that don't have to work it out myself.
that i'm not involved at all in the working out of it.
because
yeah.
uh but you're a linguist.
you should
oh yeah that's why i'm glad that i'm not involved in working it out.
okay.
so it's at friday at three?
there that's
and um
so already again this week.
huh?
how diligent do we feel?
yeah.
do feel that we have done our chores for this week?
or
yeah.
so i mean clearly there's i can talk about the um the parser changes on friday at least.
okay bhaskara will do the big show on friday.
so
and you guys will argue some more?
yeah between now and then.
and between now and then yeah.
and have some
probably.
promise?
we will
yeah.
we will.
don't worry.
yeah.
and we'll get the summary.
yeah.
like this the you know short version.
like
and i would like to second keith's request.
an example would be nice.
yes.
yeah.
to have kind of a detailed example.
yes i've i've i've i guess i'm on record for promising that now.
okay.
so um
like have it we'll have it in writing.
so
or better speech.
so
this is it.
and um
the other good thing about it is jerry can be on here on friday.
and he can weigh in as well.
yeah.
and um if you can get that binding point also maybe with a nice example that would be helpful for johno and me.
oh yeah.
uh
okay.
let's uh yeah they're
give us
no problem.
i think you've got one on hand.
huh?
yeah.
i have several in my head.
yeah.
always thinking about binding.
well the the the binding is technically no problem.
but it's it for me it seems to be conceptually important that we find out if we can if if there if there are things in there that are sort of a general nature.
we should distill them out.
uhhuh.
and put them where the schemas are.
if there are things that you know are intention specific then we should put them up somewhere.
so in general they'll be bindings across both intentions and the actions.
yep.
so
that's wonderful.
yeah.
so it's it's general across all of these things.
it's like i mean shastri would say you know binding is like an essential cognitive uh process.
yeah.
so um
okay.
so i don't think it will be isolated to one or the two.
but you can definitely figure out where
yeah sometimes things belong and
so actually i'm not sure
i would be curious to see how separate the intention part and the action part are in the system.
like i know the whole thing is like intention lattice or something like that.
uhhuh.
right?
so is the right now are the ideas the rich rich the r a d or whatever is one you know potential block inside intention.
it's still it's still mainly intention hypothesis.
yeah.
yeah.
and then that's just one way to describe the the action part of it.
yeah.
okay.
it's
it's an attempt to refine it basically.
and yeah.
okay great uhhuh.
it's an an it's it's sort of
not just that you want to go from here to here.
it's that the action is what you intend.
yeah.
and this action consists of all complicated modules and image schemas and whatever.
so
yeah and and there will be a a a relatively high level of redundancy.
in the sense that um ultimately one
uhhuh.
which is
yeah it's fine.
so so that if we want to get really cocky we we will say well if you really look at it you just need our rad.
you can throw the rest away.
uhhuh.
right?
right.
because you're not going to get anymore information out of the action as you find it there in the domain object.
right.
uhhuh.
but then again um in this case the domain object may contain information that we don't really care about either.
uhhuh.
so
but we'll see that then and how how it sort of evolves.
uhhuh.
i mean if if people really like our our rad
i mean what might happen is that they will get rid of that action thing completely.
you know and leave it up for us to get the parser input um
huh we know the things that make use of this thing so that we can just change them so that they make use of rad.
yeah.
yeah.
i can't believe we're using this term.
you don't have to use the acronym.
so i'm like rad.
like every time i say it it's horrible.
okay.
i see what you mean.
uhhuh.
rad's a great term.
is the
but what is the why?
it's rad even.
why?
why?
it happened to be what it stands for.
well
it just happened to be the acronym.
yeah that's doesn't make it a great term.
it's just like those jokes where you have to work on both levels.
no but
just think of it as as wheel in german.
do you see what i mean?
but if you if you if you work in in that x m l community it is a great acronym.
like
because it evokes whatever r d f
oh.
r d f is the biggest thing.
right?
that's the rich sort of resource description framework.
oh rich
oh.
and um and also
so description having the word term description in there is wonderful.
uhhuh.
uh rich is also great.
rwww
huh.
who doesn't like to be
everybody likes action.
oh.
yeah okay.
yeah.
plus it's hip.
but what if it's not an action?
the kids will like it.
yeah all the kids will love it.
huh.
it's it's rad.
yeah.
and intentions will be rid?
like
okay.
um are the are the sample data that you guys showed sometime ago
like the things
maybe maybe you're going to run a trial tomorrow.
i mean i'm just wondering whether the some the actual sentences from this domain will be available.
because it'd be nice for me to like look if i'm thinking about examples.
i'm mostly looking at child language which you know will have some overlap but not total with the kinds of things that you guys are getting.
so you showed some in this here before.
uhhuh.
and maybe you've posted it before.
but where would i look if i want to see?
oh i you want audio?
you know
or do you want transcript?
no just just transcript.
yeah well just transcript is just not available.
because nobody has transcribed it yet.
sorry.
oh okay.
um i can i can uh i'll transcribe it though.
i take that back then.
it's no problem.
okay well don't don't make it a high priority.
i in fact if you just tell me like you know like two examples.
yeah.
i mean
uhhuh.
the the the representational problems are i'm sure will be there.
okay.
like enough for me to think about.
so
okay so friday.
whoever wants and comes and can.
okay.
okay.
here?
this friday.
okay.
the big parser show.
now you can all turn off your
are we to just to make sure i know what's going on we're talking about robert's thesis proposal today?
is that
we could.
true?
we are?
we might.
okay.
well you you had you said there were two things that you might want to do.
is
one was rehearse your talk
oh yes and that too.
not not rehearse.
i mean i have just not spent any time on it.
so i can show you what i've got get your input on it and maybe some suggestions.
that would be great.
and the same is true for the proposal.
i will have time to do some revision and some additional stuff on various airplanes and trains.
so um
i don't know how much of a chance you had to actually read it.
i haven't looked at it.
because
yet.
but you could always send me comments per electronic mail.
but i will.
and they will be incorporated.
okay.
um the
it basically says well this is construal.
and then it continues to say that one could potentially build a probabilistic relational model that has some general domain general rules
how things are construed.
and then the idea is to use ontology situation user and discourse model to instantiate elements in the classes of the probabilistic relational model to do some inferences in terms of what is being construed as what.
huh.
in our beloved tourism domain.
but with a focus on
can i
i think i need a copy of this yes.
sorry.
huh?
is there an extra copy around?
okay we can we can we can pass pass my uh we can pass my extra copy around.
uh he sent it.
okay you can keep it.
uh actually my only copy now that i think about it.
okay.
but i already read half of it.
um i don't i uh i don't need it.
so it's okay.
okay.
um actually this is the the newest version after your comments.
and
yeah no i i i see this has got the castle in it and stuff like that.
yeah.
yep.
oh maybe the version i didn't have
that i mine the
did the one you sent on the email have the
yeah.
that was the most recent one?
uh yeah i think so.
okay.
yep.
because i read halfway but i didn't see a castle thing.
i'm changing this.
just so you know.
yeah.
but anyway
um if you would have checked your email you may have received a note from yees asking you to send me the uh
oh oh sorry.
okay.
sorry.
current formalism thing that you presented.
okay i will.
okay okay.
okay.
but for this it doesn't matter.
but uh
we can talk about it later.
that's not even ready.
so um
okay go on to uh whatever.
and
i'm making changes.
don't worry about that.
okay uhhuh
oh okay sorry.
go on.
and any type of comment whether it's a spelling or a syntax or
uhhuh.
there's only one s in interesting.
readability
huh?
there's only one s in interesting on page five.
interesting.
anyway.
and uh email any time.
but most usefully before
the twenty first i'm assuming.
the twenty first?
twenty ninth.
no this is the twenty first.
that's
what?
today's the twenty first?
well better hurry up then!
oh man!
the twenty ninth.
before the twenty ninth.
okay.
okay.
that's when i'm meeting with wolfgang wahlster to sell him this idea.
uhhuh.
okay.
okay.
then i'm also going to present a little talk at e m l about what we have done here.
and so
of course i'm i'm going to start out with this slide.
so the most relevant aspects of our stay here.
and um
then i'm asking them to imagine that they're standing somewhere in heidelberg.
and someone asks them in the morning the cave forty five is a is a well known discotheque.
which is certainly not open at that that time.
okay.
and so
they're supposed to imagine that you know do they think the person wants to go there or just know where it is.
uh
which is probably not uh the case in that discotheque example.
or in the bavaria example.
you just want to know where it is.
and so forth.
so basically we can make a point that here is ontological knowledge.
but if it's nine nine p m in the evening then the discotheque question would be for example one that might ask for directions instead of just location.
um and so forth and so forth.
that's sort of motivating it.
then what have we done so far?
we had our little bit of um um smartkom stuff that we did.
um
oh you've got the parser done.
sorry.
that's the not the construction parser.
okay.
that's the uh tablet based parser.
easy parser.
okay.
and
the generation outputter.
halfway done.
yeah.
that's done.
huh.
you have to change those strategies.
okay.
right?
yeah.
that's ten words?
well it you know
maybe twelve.
twelve okay.
and um and fey is doing the synthesis stuff as we speak.
that's all about that.
then i'm going to talk about the data.
you know these things about
uh actually i have an example
probably.
two
can you hear that?
uhhuh.
or should i turn the volume on?
i could hear it.
i can hear it.
i heard it.
they might not hear it in the
well maybe they will.
i don't know.
this was an actual um subject?
uh.
uhhuh.
sounds like fey.
yeah.
but they're they're mimicking the synthesis when they speak to the computer.
oh okay.
the you can observe that all the time.
oh really?
they're trying to match their prosody onto the machine.
interesting.
oh it's pretty slow.
yeah you have to
the system breaking.
what is the
oh!
okay and so forth and so forth.
um i will talk about our problems with the rephrasing.
and how we solved it.
and some preliminary observations.
also um
i'm not going to put in the figures from liz.
but i thought it would interesting to uh um point out that it's basically the same.
um as in every human human telephone conversation and the human computer telephone conversation is of course quite quite different from uh some first uh observations.
then
sort of feed you back to our original problem.
because uh
how to get there.
what actually is happening there today?
and then maybe talk about the big picture here.
tell a little bit as much as i can about the n t l story.
i i i do want to um
i'm not quite sure about this whether i should put this in.
um that you know you have these two sort of different ideas that are or two different camps of people envisioning how language understanding works.
and then talk a bit about the embodied and simulation approach favored here.
and as a prelude i'll talk about monkeys in italy.
and um srini was going to send me some slides.
but he didn't do it.
so from but i have the paper.
i can make a resume of that.
and then i stole an x schema from one of your talks i think.
oh.
i was like where'd you get that.
yeah that looks familiar.
okay.
looks familiar.
i think that's bergen chang something or the other.
uh
whatever.
okay.
um and that's
now i'm not going to bring that.
so that's basically what i have so far.
and the rest is for airplanes.
so x schemas then i would like to do
talk about the construction aspect.
and then at the end about our bayes net.
uhhuh.
end of story.
anything i forgot that we should mention?
oh maybe the f m r i stuff.
should i mention the fact that um we're also actually started going to start to look at people's brains in a more direct way?
you certainly can.
i mean you know i don't know
you might just want to like tack that on as a comment to something.
right.
um.
future activities something?
well the time to mention it if you mention it is when you talk about mirror neurons.
then you should talk about the more recent stuff about the kicking.
and you know the
yeah.
yeah yeah.
and that the plan is to see to what extent the you'll get the same phenomena with stories about this.
so that
uhhuh.
and that we're planning to do this.
um
which we are.
so that's one thing.
um
depends i mean there is a um whole language learning story.
okay?
yeah.
which
uh actually
even on your five layer slide you you've got an old one that that leaves that off.
yeah i i i do have it here.
huh.
yeah.
um
and of course you know the the big picture is this bit.
right.
but you know it would
but i don't think i i am capable of of pulling this off and doing justice to the matter.
i mean there is interesting stuff in terms of how language works.
so the emergentism story would be nice to be you know it would be nice to tell people.
how what's happening there.
plus how the uh language learning stuff works.
okay so so anyway i i agree that's not central.
but
what you might want to do is
uhhuh.
um
and may not.
but you might want to this is rip off a bunch of the slides on the
there the there we've got various generations of slides that show language analysis and matching to the underlying image schemas.
and
um how the construction and simulation
that that whole
yeah that that's that comes up to the x schema slide.
okay right.
so basically i'm going to steal that from nancy.
one of nancy's
okay i can give you a more recent if you want.
well that might have enough.
uh i
yeah but i also have stuff you trash you left over.
okay.
your quals and your triple a i.
the quals the the the quals slides would be fine.
yeah.
you could get it out of there or
which i can even email you then.
you know like there probably was a little few changes.
not a big deal.
yeah you could steal anything you want.
i don't care.
which you've already done obviously.
so
well i i don't feel bad about it at all.
sorry.
no you shouldn't.
because because you are on the uh title.
i mean on the the you're that's see that's you.
oh that's great that's great.
yeah.
i'm glad to see propagation.
yeah.
huh!
huh?
propagated?
yes.
i mean i might even mention that this work you're doing is sort of also with the m p i in leipzig.
so
it's it's certainly related.
um
because um e m l is building up a huge thing in leipzig.
might want to say.
is it?
so it
it's on biocomputation would
yeah it's different.
this is the uh d n a building or the double helix building.
yeah.
yeah.
kind of a different level of analysis.
the yeah it was it turns out that if if you have multiple billions of dollars you can do all sorts of weird things.
and
wait they're building building in the shape of d n a?
what?
is that what you said?
roughly yeah.
oh oh boy.
including cross bridges.
and
what?
that's brilliant!
oh my god!
hhh.
you you really now i spent the last time i was there i spent maybe two hours hearing this story.
which is um
of what?
you definitely want to don't want to waste that money on research.
the building.
you know?
right.
that's horrible.
right.
well no no there's infinite money.
see.
you you you then fill it with researchers.
and give them more money.
they just want a fun place for them to to work.
right.
right.
and everybody gets a trampoline in their office.
well the the offices are actually a little
the
think of um ramps coming out of the double helix.
and then you have these half domes glass half domes.
and the offices are in in the glass half dome.
really?
all right let's stop talking about this.
yeah.
does it exist yet?
yeah.
uh as a model.
they are now building it?
huh.
but i
so yeah i think that's that's a good point.
that the date the uh
a lot of the this is interacting with uh people in italy.
but also definitely the people in leipzig.
and
the
the the combination of the biology and the leipzig connection might be interesting to these guys.
yeah.
okay.
okay.
anyway enough of that.
let's talk about your thesis proposal.
yeah if somebody has something to say.
yep.
you might want to uh double check the spellings of the authors' names on your references.
you had a few uh misspells in your slides there.
like i believe you had jackendorf.
um
uh unless there's a person called jackendorf.
no no no.
yeah.
on that one?
but that's the only thing i noticed in there.
in the presentation?
in the presentation.
i'll probably i might have i'll probably have comments for you separately.
not important.
anyway
oh in the presentation here.
yeah that's what he was talking about.
yeah.
i was actually worried about bibtex.
uh no that's quite possible.
that's copy and paste from something.
so i did
it looks like the uh metaphor didn't get in yet.
uh it did.
there is a reference to srini
well reference is one thing.
the question is is there any place
oh did you put in something about
metonymy and metaphor here.
right?
uh the individual
we'd talked about putting in something about people had
uh
oh yeah okay.
good i see where you have it.
so the top of the second of page two you have a sentence.
uhhuh.
but what i meant is i think even before you give this to wahlster uh you should
unless you put it in the text.
and i don't think it's there yet.
about we talked about is the um scalability that you get by um combining the constructions with the general construal mechanism.
is that in there?
yeah.
huh um
uh okay so where where is it?
because i'll have to take a look.
um but i i did not focus on that aspect.
but um uh
um it's just underneath uh um that reference to metaphor.
so it's the last paragraph.
before two.
so on page two
um the main focus
uh okay.
yeah.
but that's really
that's not about that.
yeah.
is it?
no it it it says it.
but it doesn't say it doesn't it it
why.
yeah it doesn't give the punch line.
uhhuh.
because let me tell the gang what i think the punch line is.
because it's actually important.
which is that the constructions that uh nancy and keith and friends are doing uh are in a way quite general.
but cover only base cases.
and to make them apply to metaphorical cases and metonymic cases and all those things requires this additional mechanism of construal.
and the punch line is he claimed that if you do this right you can get essentially orthogonality.
that if you introduce a new construction at at the base level it should uh interact with all the metonymies and metaphors.
so that all of the projections of it also should work.
uhhuh.
and similarly if you introduce a new metaphor it should then uh compose with all of the constructions.
uhhuh.
yeah.
and it to the extent that that's true then then it's a big win over anything that exists.
so does that mean instead of having tons and tons of rules in your context free grammar you just have these base constructs and then a general mechanism for coercing them?
yeah.
uhhuh.
so that you know for example uh in the metaphor case that you have a kind of direct idea of a source path and goal.
and any metaphorical one and abstract goals and all that sort of stuff you can do the same grammar.
and it is the same grammar.
huh.
but um
the trick is that the
the way the construction's written it requires that the object of the preposition for example be a container.
well trouble isn't a container.
but it gets construed as a container.
right.
et cetera.
so that's that's where this um
so with construal you don't have to have a construction for every possible thing that can fill the rule.
right.
so's it's it it's a very big deal in this framework.
and the thesis proposal as it stands doesn't
um i don't think say that as clearly as it could.
no it doesn't say it at all no.
even though one could argue what if there are basic cases even.
i mean it seems like nothing is context free.
oh nothing is context free.
but there are basic cases.
that is um there are physical containers.
there are physical paths.
there
you know.
et cetera.
but walked into the cafe and ordered a drink and walked into the cafe and broke his nose.
that's sort of
oh it doesn't mean that they're unambiguous.
i mean a cafe can be construed as a container or it can be construed you know as as a obstacle.
huh yeah.
uhhuh.
or as some physical object.
so there are multiple construals.
and in fact that's part of what has to be done.
this is why there's this interaction between the analysis and the construal.
uhhuh.
yep.
the the the double arrow.
yep.
so uh yeah i mean it doesn't magically make ambiguity go away.
no.
but it does say that uh if you walked into the cafe and broke your nose then you are construing the cafe as an obstacle.
uhhuh.
and if that's not consistent with other things then you've got to reject that reading.
yep.
you con you conditioned me with your first sentence.
and so i thought why would he walk into the cafe and then somehow break his nose.
he slipped on the wet floor.
uh oh uh
right.
you don't find that usage.
uh uh i checked for it in the brown national corpus.
yeah.
the walk into it never really means as in walked smack
but run into does.
yeah but if you find walked smacked into the cafe or slammed into the wall
yeah no but run into does.
uhhuh.
because you will find run into
uh
cars run into telephone poles all the time.
well or into the cafe for that
you know.
right.
his car ran into the cafe.
yeah or you can run into an old friend or run.
well you can run into in that sense too.
but uh
yeah run into might even be more impact sense than you know container sense.
right.
depends.
but like run into an old friend it probably needs its own construction.
i mean uh you know george would have i'm sure some complicated reason why it really was an instance of something else.
uhhuh uhhuh.
and maybe it is.
but um
there are idioms.
and my guess is that's one of them.
but um i don't know.
all contact.
i mean there's contact that doesn't social contact.
whatever.
i mean
uh
sudden surprising contact.
right?
yeah but it's it's it's it's right yeah it's more
forceful.
but of course no.
i mean it has a life of its own.
it's sort of partially inspired by the spatial
well this is this motivated
but yeah.
yeah.
oh yeah for sure.
motivated.
but then you can't parse on motivated.
yeah.
yeah right.
uh
too bad.
you should get a t shirt that says that.
okay.
there's there's lots of things you could make t shirts out of.
but uh
this has gotten
i mean
we don't need the words to that.
probably not your marks in the kitchen today.
what?
not not your marks.
oh no no no no no no no no no we're not going there.
okay.
in other news.
okay so um
anything else you want to ask us about?
the thesis proposal you got
well
we could look at a particular thing and give you feedback on it.
well there actually
the what would have been really nice is to find an example for all of this uh from our domain.
so maybe if we if we can make one up now that would be incredibly helpful.
so where it should illustrate
okay.
how
uh when you say all this do you mean like i don't know the related work stuff?
as well as mappings?
right right
well we have for example a canonical use of something.
and it's you know we have some constructions.
and then it's construed as something.
and then we we may get the same constructions with a metaphorical use that's also relevant to the to the domain.
okay let's let's suppose you use in and on.
i mean that's what you started with.
uhhuh.
so in the bus and on the bus.
um that's actually a little tricky in english.
because to some extent they're synonyms.
okay.
i had two hours with george on this.
okay what did he say?
so it
did you?
um um
join the club.
right oh that's
on the bus is a is a metaphorical metonymy.
that relates some path metaphorically.
and you're on on that path.
and i mean it's he there's a platform notion.
yeah i i believe all that.
it's just
right?
he's on the standing on the bus waving to me.
yeah.
but the regular as we speak johno was on the bus to new york.
yeah.
yeah.
um uh
he's
that's uh
what did i call it here?
the transportation schema something.
yeah.
where you can be on the first flight on the second flight.
yeah.
and you can be you know on the wagon.
right so so that that may or may not be what you what you want to do.
i mean you could do something much simpler.
yeah.
like under the bus or something.
where
but it's it's unfortunately this is not really something a tourist would ever say.
so
well unless he was repairing it or something.
but yeah.
yeah.
but um
uh but okay.
so in terms of the this
i see.
we had we had initially we'd started discussing the out of film.
right.
and there's a lot of out of analysis.
so um
right.
could we capture that?
with a different construal of
yeah it's a little it's
uh we've thought about it before uh uh to use the examples in other papers.
and it's it's a little complicated.
out of out of film in particular.
because you're like it's a state of there's resource.
yeah.
right?
and like what is film?
the state you know you're out of the state of having film right and somehow film is standing for the the the state of having some resource is just labeled as that resource.
it's
yeah i mean
i mean
but
and plus the fact that there's also
it's a little bit
i mean can you say like the film ran out?
you know.
or maybe you could say something like the film is out.
yeah is film the trajector?
so like the the film went away from where it should be.
namely with you or something.
right?
you know.
the the film the film is gone.
right?
um
i never really knew what was going on.
i mean i i find it sort of a little bit farfetched to say that that i'm out of film means that i have left the state of having film or something like that.
it's weird.
that
but
uh.
or having is also um associated with location.
yeah yeah.
right?
so if the film left
you know state is being near film.
so running running out of something is different from being out of somewhere.
or being out of something as uh as well.
so running out of it definitely has a process aspect to it.
uhhuh.
but that's from run yeah.
uhhuh.
so
that's okay.
i mean but the difference
yeah.
is the the final state of running out of something is being out of it.
is
yeah.
right.
yeah so
you got there.
that part is fine.
you got to out of it.
yeah.
yeah.
but uh
huh!
yeah so so so no one has in in of the
uh
professional linguists.
they haven't
uh
there was this whole thesis on out of.
there was?
well there i thought or there was a paper on it.
who?
out.
huh?
there was one on on out or out of?
there was
well it may be just out.
okay.
yeah.
i think there was over.
but there was also a paper on out.
yeah susan lindner.
right?
or something.
oh yeah you're right yeah.
the the the syrup spread out.
that kind of thing?
yeah and all that sort of stuff.
yeah.
and undoubtably there's been reams of work about it in cognitive linguistics.
okay but anyway we're not going to do that between now and next week.
but
yeah.
yeah.
okay.
so um
it's not one of the it's more straightforward ones forward ones to defend.
so you probably don't want to use it for the purposes.
uhhuh.
right.
okay.
these are you're addressing like computational linguists.
right?
or are you?
there's going to be four computational linguists.
okay but more emphasis on the computational or emphasis on the linguist?
it's
more there's going to be the just four computational linguists by coincidence.
but the rest is whatever biocomputing people and physicists.
no no no but not for your talk.
oh okay.
we're worrying about the the
oh the thesis.
it's just for one guy.
oh i meant this.
that's that's should be very computational.
you know like
okay.
yeah.
and uh
so i would try to i would stay away from one that involves weird construal stuff.
yeah.
right.
you know it's an obvious one.
totally weird stuff.
i mean the the old bakery example might be nice.
but uh
is there a bakery around here?
yeah.
so if you we really just construe it as a
around?
no it's the bakery itself.
is it a building uh that you want to go to or is it something to eat that you want to buy?
oh.
oh oh yeah yeah.
we've thought about that.
right right.
and then
nnn.
no!
what?
bakery can't be something you're going to eat.
no no the question is do you want to do you want to construe do you want to constr strue
it's a speech act.
exactly it's because do you want to do you want to view the bakery as a a place that that for example if
yeah.
where you can get baked goods.
well well that's one.
you want to buy something.
but the other is
uh you might have smelled a smell and are just curious about whether there'd be a bakery in the neighborhood.
or
uhhuh.
um pfff
you know you wonder how people here make their living.
and there're all sorts of reasons why you might be asking about the existence of a bakery.
yeah.
that doesn't mean i want to buy some baked goods.
okay.
but um
those are interesting examples.
but it's not clear that they're mainly construal examples.
yeah.
so it's a lot of pragmatics there that
huh.
there's all sorts of stuff going on.
so
might be beyond what you want to do.
let's so let's think about this from the point of view of construal.
so let's first do a
so the metonymy thing is probably the easiest.
and and actually the
though the one you have isn't quite
you mean the
you mean the steak wants to pay?
no not that one.
that's that's a the sort of background.
this is the uh page five.
about plato and the book?
no.
oh.
no.
um
onward.
just beyond that.
how much does it cost?
where is the castle?
yeah.
a castle.
how old is it?
how much does it cost?
oh.
uhhuh.
to go in.
that's like
two hundred million dollars.
right.
it's not for sale.
uh so
yeah i think that's a good example actually.
yeah that's good.
but as nancy just suggested it's probably ellipticus.
ellipsis.
huh
like it doesn't refer to thing.
it refers to you know
thing standing for most relevant activity for a tourist.
you could think of it that way.
but
yeah.
well shoot!
isn't that i mean that's what
well i mean my argument here is it's it's it's the same thing as plato's on the top shelf.
figuring that out is what this is about.
yeah yeah no i agree.
i'm
you know that you can refer to a book of plato by using plato.
yeah.
and you can refer back to it.
no no i i'm agreeing that this is a good um
and so you can castles have as tourist sites have admission fees.
so you can say where is the castle.
how much does it cost.
um
how far is it from here.
uhhuh.
so
you're also not referring to the width of the object or so.
huh.
www.
uhhuh.
huh.
okay can we think of a nice metaphorical use of where in the tourist's domain
um
huh.
so you know it's
you you can sometimes use where for when.
in the sense of you know
um
where where where was um
where was heidelberg um in the thirty years' war?
or something.
uh yeah.
uhhuh.
you know or some such thing.
um
like what side were they on?
or
what?
yeah essentially yeah.
okay i was like huh.
it was here.
like um
but anyway so there are there are cases like that.
um
uh.
or like its developmental state or something like that.
you could i guess you could get that.
yeah.
um.
um.
i mean there's also things like i mean
um
i guess i could ask something like where can i find out about blah blah blah.
in a sort of
doesn't i don't necessarily have to care about the spatial location.
just give me a phone number.
and i'll call them or something like that.
yeah there certainly is that yeah.
you know where could i learn its opening hours or something.
yeah.
but that's not metaphorical.
huh.
it's another
yeah.
so we're thinking about um
or we could also think about
uh
well i i i
how about i'm in a hurry?
state.
it but it's a state and the the issue is is that it may be just a usage.
huh.
you know that it's not particularly metaphorical.
i don't know.
huh.
right.
so you want a more exotic one version of that.
oh.
yeah yeah right.
uh!
how about
i'm really into
i i i you know i'm in i'm in a state of exhaustion.
or something like that.
do you really say that?
which a tourist
huh?
would you really say that?
a uh well you can certainly say um you know i'm in overload.
yeah.
tourists will often say that.
i'm really into art.
yeah i was going to say like
uh
oh you can do that?
really?
of course that's that that's definitely a uh
fixed.
a fixed expression yeah.
that's a uh
right but
there're too there're all sorts of fixed expressions.
i don't like uh
i'm out of sorts now.
right.
like i'm in trouble.
well i when uh just
the data that i've looked at so far that
yeah.
i mean there's tons of cases for polysemy.
right.
so you know making reference to buildings as institutions as containers as
uhhuh.
right.
you know whatever.
um so in for example in museums you know as a building or as something where pictures hang versus you know something that puts on exhibits.
right as an institution.
so forth.
but
yeah.
um.
why don't you want to use any of those?
huh?
so you don't want to use one that's
yeah well
no but this
that's what i have you know started doing.
the castle the that old castle one is sort of
metonymy polysemy.
i love van gogh.
yeah.
uh!
i want to go see the van gogh.
oh geez.
anyway i'm sorry.
but i think the argument should be uh can be made that you know despite the fact that this is not the most metaphorical domain
because people interacting with h t i systems try to be straightforward and less lyrical
yeah.
construal still is uh you know completely um key in terms of finding out any of these things.
so um
right.
so that's that's that's a that's a reasonable point.
that it in this domain you're going to get less metaphor and more metonymy.
we uh i with a i looked with a student i looked at the entire database that we have on heidelberg for cases of metonymy.
and polysemy and stuff like that.
yeah.
hardly anything.
so not even in descriptions did we find anything um relevant.
i have to go.
all right yeah.
but okay this is just something we'll we'll see.
um
right.
see you.
and deal with.
okay well i guess if anybody has additional suggestions
i mean maybe the where is something question as a whole you know can be construed as locational versus instructional request.
yeah.
so if we're not talk about the
location versus what?
instruction.
instruction oh directions yeah.
sure.
yeah.
oh i thought that was definitely treated as an example of construal.
right?
yeah but then you're not on the lexical level.
that's sort of one level higher.
oh you want a lexical example.
but i don't need it.
well you might want both.
uhhuh.
yeah.
also it would be nice to get ultimately to get a nice mental space example.
we
so even temporal references are
just in the spatial domain are rare.
but it's it's easy to make up plausible ones.
you know?
when when you're getting information on objects.
right.
so i mean
you know you know where
yeah.
what color was this in in in the nineteenth century?
yeah.
what was this
instead of what you know how was this painted.
what color was this painted?
um
was this alleyway open?
yeah maybe we can include that also in our second uh data run.
uh
we we can show people pictures of objects.
and then have then ask the system about the objects.
and engage in conversation on the history and the art and the architecture and so forth.
uhhuh.
okay so why don't we plan to give you feedback electronically.
wish you a good trip.
all success.
for some reason when you said feedback electronically i thought of that
you ever see the simpsons where they're like the family's got the buzzers?
and they buzz each other when they don't like what the other one is saying.
yeah that's the first one i think.
it was a very early one.
the very very first one.
i don't know if it's the first one.
huh.
huh.
and we're going.
so the only status
well first of all we haven't decided whether we're meeting recorder data issues or recognition this week.
i think we were recognition.
what was on the list the
i mean i sent you a couple things.
although i don't remember them.
you only sent me one thing which was demo status.
and asking which one we were on this week.
right.
uh that was the second thing.
right right.
so should we simply assert that this week we are recognition and next week data issues?
huh.
i think that's correct.
and
yeah i think so too.
yeah.
and uh
yeah.
so i think what we should probably do is any quick small stuff we can do every week.
so like morgan asked about the demo status.
we can go ahead and talk about that a little bit.
and then do then alternate in more depth.
by the way i'll i i won't be here next thursday.
i'll be out of town.
okay.
but
actually i may not be here either.
so
i got to double check the dates.
but anyway.
so uh demo status.
first of all i did a little thing for liz with the transcriber tool that um first of all it uses the forced alignments.
so that the words appear uh in their own segments rather than in long in long chunks.
oh.
she said that that she thought that was a much better idea for the other stuff she's working on.
yeah that's great.
um
and that works fine.
except it's even slower to load.
it's already pretty slow to load.
yeah it's more segments?
it's slow.
or
yeah.
is that because the transcripts get longer?
yeah.
the transcript file gets longer.
yeah yep.
yep.
and the transcriber tool is just not very good at at that.
but but that's that's
you didn't have to change the software for that yet.
right?
correct.
it's just formatting the right kind of uh x m l?
yeah it's just writing conversion tools from the format that the aligner.
actually he did a s r t file for it.
uhhuh okay.
and then just back into transcriber transcriber format.
oh good that's very good okay.
yeah so my my decision was for the first pass for this demo that liz was talking about i decided that i would do um only enough to get it working as opposed to any coding.
uhhuh.
right.
and so the other thing she wanted to display the stylized f zeroes i think they're called.
is that right?
yeah the linear fit.
uhhuh.
and uh
so what i did is i just took the file with those in it converted it so that it looks like an audio file.
right.
and so you it shows that instead of the wavefile.
and so that that's working.
cool.
and i think it actually looks pretty good.
uhhuh.
um i'd like someone who's more familiar with it to look at it.
because when i was looking at it we seemed to have lots of stuff going on when no one's saying anything.
that's just background speech.
uh.
yeah.
so
so do you have to pad that out uh so that it looks like it's an eight kilohertz sampled thing?
no i the audio file you can specify any sampling rate.
or
and so i i specified instead of you know sixteen thousand or eight thousand i specified a hundred.
huh.
um
and the only problem with that is that there's a bug in transcriber.
that if the sample rate is too low when it tries to compute the shape file it fails.
huh.
um and crashes.
um
but the solution to that is just set the option so it doesn't compute the shape file and it will work.
and the only problem with that is you can't uh zoom out on it.
you can zoom in but not out.
what's a shape file?
the shape file is
if you think about a wavefile sixteen thousand samples per second is way too many to display on the screen.
uhhuh.
so what transcriber does is it computes a another thing to display based on the waveform.
we're talking about we're talking about that demo.
yeah tried that and it died.
and it displays it at
yeah did it?
and it allows you to show many different resolutions.
so there's a little user interface component that lets you select the resolution.
great.
i see.
and if you don't compute the wavefile you can't zoom out.
you can't get a larger view of it.
but you can zoom in.
huh.
um
and that's all right.
because at at a hundred samples that's already pretty far out.
and uh so i think it looks pretty good.
but i'll let liz look at it and see what she thinks.
uhhuh.
i i got the wavefile.
but
sorry i got the wavefile.
but i couldn't get the words yet.
but the the wavefile part looks looks good.
okay.
we should if you were having problems with the words we should figure out why.
i i'll have done i'm probably doing something wrong.
okay.
sorry this microphone's moving around.
i can't put this over my ear.
it could do you have to put it in your ear?
you you clip that part over your ear.
i can i can do that.
but there's no orientation where the
darn!
we'll all watch liz play with the mike.
does it really need to go in her ear?
um
that that bud.
it doesn't have to go in her ear.
right?
uh no it doesn't have to.
but that that's i find that's the only way to wear it.
oh wow.
is that the bud's in the ear and that the link is over it.
this bud's for
but so anyway i think that looks pretty good.
no.
the only the only other thing we might want to do with that is be able to display more than one waveform.
huh.
and that actually shouldn't be too slow.
uh because it's much lower resolution than a full waveform.
yeah.
the problem with it is just it does require coding.
yeah it
and so it would be much better to get uh dave gelbart to do that than me.
because he's familiar with the code.
and is more likely to be able to get it to work quickly.
it'd be nice if we can do like a quick hack.
just so we can play the audio file too.
right.
um with with the display.
oh okay.
like even if we i think that even if we didn't display the waveform it might be better to rather play the waveform than display it.
i mean like if we were to choose i don't know if i were to choose between one or the other i'd rather have it played.
uhhuh.
i understand what you mean.
yeah.
and then displayed.
right.
but for the demo maybe it doesn't matter.
i'm not sure whether you want to do the demo live anyway.
or just screen shots of what we have.
the problem with doing it live is it takes so long to load that um
i don't know.
so this the this uh the sluggishness of the loading is all due to the parsing of the x m l format.
um i was talking to dave gelbart about that.
right?
and apparently it's not actually the parsing of the x m l raw
that going from the x m l to an internal tree structure is pretty fast.
huh.
but then it walks the tree to assemble its internal data structures.
and that's slow.
huh.
seems like you should be able to spawn that off into a background process.
because not everything is displayed in that tree at once.
right?
uh no.
but what it does is it actually assembles all the user interface components then.
but
right.
and then displays all the user interface components.
i'm i'm confused.
seems like you want to
uh is is this downloading something that happens once?
yes.
and and then when you display different things it's fine?
so in that case
no whenever you load a new meeting or a new transcript.
a new transcript.
yep.
well a new meeting a transcript right.
right.
but but for but for
or audio file.
well actually the audio files are pretty fast too.
for presentation in uh uh i wouldn't be
yeah.
yeah.
you just have to have the thing running before you open your laptop.
yeah.
yeah.
right the only problem with that is if anything goes wrong or if you want to switch from one thing to another.
right.
yeah.
go wrong?
yeah.
i see yeah.
right i guess for the demo you can always play just store the pieces that you're going to display and play those as separate files if we can't you know actually do it.
just make shorter files.
but it's you know it it's
that's true we could just subset it.
yeah.
that's a good idea.
yeah.
that's actually probably the right thing to do.
and just make it
yep.
you know just take ten minutes instead of an hour and a half.
yeah that's what i did for for my talk.
oh oh you're downloading a whole meeting.
yeah.
oh yeah that.
yeah.
yeah so that that's actually the definitely the way to do it.
yeah.
that's a good idea.
yeah.
and then still do it ahead of time.
but then at least you're covered if if uh if there's a problem.
yeah if there are any problems.
right.
yeah i mean even five minutes is probably enough.
huh.
right.
so what happened to is it possible at all to display the words in their aligned locations?
that's what i did.
okay so sorry.
yeah.
you missed that part.
i missed
okay great!
but it
okay.
i couldn't get the words and the waveform at the same time for some reason.
and there must be some
i'll i'll work on it with don and see what i'm doing wrong.
yeah i mean just ask just come by my office.
i can show you as well.
okay great well thanks a lot.
that's really great.
right.
and for the information retrieval uh don has been working on that.
so
yeah so it's coming along.
it looks like
um
just hacking dan's code and stepping through it.
but i think it's close.
great.
and we should be there pretty soon.
with at least like at least with you know being able to search over certain amount of meetings.
just like really basic stuff.
just asking looking for a word and looking through a bunch of different meetings.
and if we have time i'll also add you know like choosing which speakers you want to include and stuff.
but
okay well i'm going to start working on this the week after next.
so that's the point when i'll need to look more carefully at what what what you guys have.
so is the end of the month still the the
right the week after
the monday the week after next is july second.
which is the first day i get back.
okay.
so
okay.
yeah so i think for the stuff liz was talking about we have something that'll work now.
and liz can look at it and see if she wants anything else.
maybe we can work on doing displaying multiple or displaying one and playing back the other.
so do you think it's reasonable to display more than one before the demo?
because
um i think i'd i'd have to ask dave.
i did it once before.
and it was just so slow to scroll that i gave up.
huh.
but the advantage is that these things are much lower sampling rate.
right.
and so then it might be all right.
okay let me know.
morgan when's the demo?
well uh i'm giving a talk on july sixteenth.
it's a monday in four weeks.
so if if raw speed is the problem this thing is written in tcl.
three weeks?
tcl.
right?
i mean john osterhout uh you know he started his own company based on tcl stuff.
and maybe they have the native code compiler or something.
i mean we could check.
i don't think they do.
um there was actually a java back end that apparently is actually a little faster.
it generates byte code.
huh.
but uh
it's always exciting to hear that java's faster than something.
yeah.
well everything is faster than tcl t k.
yeah.
it's a string substitution language basically.
i should probably beep that out in case john osterhout ever listens.
but tcl is wonderful.
but
well it is wonderful.
it is for prototyping and user interface.
it's just really the language is awful.
there you go.
oh.
beep.
beep.
right.
but let me tell you how i really feel.
we're all entitled to our opinions here.
yep.
yeah i like it.
um yeah so it's yeah i think it must be three and a half weeks.
it's great.
uh because july the the meeting is july sixteenth through eighteenth.
and uh my talk's the first day.
so
okay.
i'm flying out there the sunday before.
so um
huh.
i guess you know it'd be desirable if a week ahead of that we basically had thought we had it.
which would allow a week for
oh that's right.
for realizing we don't?
re iterating.
yeah pretty much.
yeah.
then the other issue related to that is data release.
if we want to show this in public it should be
so i uh haven't gotten any other replies from the original email asking for approval.
so i sent out another set this morning.
i i saw that.
and uh we'll see if we get any responses.
i just did it.
very good.
but it is
i did want to say that um
did you notice i put in the filter?
no!
no no!
go ahead.
i just figured you
there's a link there that now says if you want to search by filter by a regular expression you can.
oh my gosh.
okay.
i put that in just for you.
terrific.
well since you didn't answer the
so there was a question i had asked adam whether it's possible to search only for your own name your own utterances.
so that you know you don't have to go through the whole meeting.
and um
and i didn't hear back.
so i thought okay it's probably too hard.
he's overloaded.
i won't say anything.
i'll just do it.
great.
okay so anyway i looked at everybody else's
it's actually an arbitrary arbitrary regular expression.
good.
but if you search your name you'll get all of the things you said and any time anyone said your name.
so
that's great.
so
and it's it's case insensitive.
correct.
yeah.
that's great.
did you actually look through your transcripts?
or you just approved them all?
uh well
i just approved all mine.
i didn't look at them.
i sort of spot checked.
i was trying to remember
oh darn!
i haven't done that yet.
uh okay.
i couldn't find the the keywords for things that i thought i had said wrong.
so
it's hard to find.
that makes it
that's a compliment to you.
he said it's hard to find things you say wrong.
huh
it's hard to find anything that you say in these.
yep.
great.
well thanks for the filter.
you really do have to sort of
uh it's really useful
because if you're only at part of a meeting or something.
so we have our first information retrieval example.
it's a regular expression.
yeah that's right.
yeah!
searcher.
that's actually well it's useful.
and it demonstrates why it doesn't work.
because you really want to go more than one meeting.
yeah.
yeah.
and you need a better user interface for displaying the results.
but this helps a lot.
so
yeah.
you want to say where are where find all the contentious things i said.
great thanks.
yeah really.
find everything that should be bleeped.
that's right we do have that nice marker is that
because we all know we're being recorded whenever anyone says anything like that we then have a conversation about bleeping it out.
right.
yeah you can search for beep or bleep.
so
yep.
yeah.
in somebody else's turn.
um oh and also we actually have a few people who have still not filled out speaker forms.
specifically in the n s a ones.
and i noticed that when i tried to uh uh generate the transcripts for n s a that there are a few with no speaker forms.
and so uh i have a i sent out yet another this morning which i think makes six total emails that i've sent to these people.
and so i think we need to escalate to some other method of trying to contact them.
um right.
stalk them at their
has has has joachim sokol replied?
like in the morning when i leave for work.
nope.
or
i think maybe talk to him first in person.
that's what i would think.
he's not around is the only problem.
uhhuh.
oh is that right?
oh i saw him i saw him on tuesday.
yeah otherwise it'd be a good idea.
yeah he popped in.
but i mean he's basically gone.
oh okay.
cold calling at lunch time uh dinner time i mean.
well if i could find phone numbers that would certainly work.
well did you ask lila?
but
because i bet she has this information.
yeah that's a good idea.
i'll ask her if she can track some of them down.
yeah.
yeah.
yeah and and tell her you know tell her your specific problem.
she'll fix
and it
and uh then there's still um
miguel is still an active member of the group and he's
what i mean is he's an active member and he's still here.
he's right there yeah.
uhhuh.
very helpful.
yeah i didn't actually see who they all were.
um a couple of them were like people at i b m who were here for one of the i b m meetings.
yeah.
and one a guy from s r i who was at one of the s r i meetings.
and so uh those might be harder to track down.
most of them though really were visitors here and lila should have contacts with them.
yep.
nice meetings by the way.
yeah.
i mean they they were people who didn't have accounts at icsi.
so they're they're harder to find.
well not the ones that
are you are you sure?
am i sure about what?
yeah n s a one and n s a three.
there were other people also.
we're talking about those?
there were other other people also who didn't fill out the speaker forms in addition to the n s as.
in in other meetings.
yeah.
yeah.
oh oh i see okay fine.
well s r i people easy to find.
yeah.
yeah.
and uh i b m people also just let us know.
i mean
just we certainly have their email.
but i knew everybody in the n s a meetings.
uhhuh.
so i'm sure that we have uh fresh you know information on them.
right.
yeah none of the emails bounced so i know they're going somewhere.
good okay.
right.
that's all i have.
you want to talk about recognition?
chuck you want to talk about recognition?
i haven't done anything.
liz you want to talk about recognition?
i was
thilo you want to talk about recognition?
i was away for a couple of days.
so
i haven't done a
we're sort of in a stage where we're uh don's going through getting some of the next meetings that jane has.
and uh you know creating a second database.
so we haven't actually run anything yet.
we need to get a critical mass for that.
however i just got an email from thilo saying that we are ready to run
i mean we have segmentations for the old meetings that are from his segmenter.
and check the segmentations.
uhhuh.
yeah.
and so
you you had three different versions with different like pause thresholds between the segments?
great.
yep yep.
yeah just yeah just smooth the the output of the of the detector.
right and you recommended using the one with two maximum of two seconds?
yeah.
a what do you mean a different pause threshold?
yep.
but two
or you can yeah you can use the one with one second or whatever.
do you mean a
i i i there's no not much difference between the the one second and the two second one.
uhhuh.
i mean the only advantage to using the longer threshold would be that you run less risk of missing some some speech.
backchannel.
right?
and i think wouldn't it be better to to have a little longer sequences for the recognizer?
because of the language model as sometimes it happens that that it cuts off within a
yeah.
but two seconds is pretty long.
so
yeah but we can be sure that as or we can be not not totally sure.
no it's not bad.
that's good.
but we can be somehow sure that there is nothing not no speech between those.
huh.
huh
yeah
so it it
what does the two second threshold mean?
i think it doesn't
i i think that's that's good.
it's the same as in the the smoother for the i b m thing.
it combines them if it's if the pause is longer than
yeah.
it's it's no more than six words so roughly on average.
uh.
so yeah.
huh.
so
that's pretty good i think.
right so the the trade off is you get longer utterances but you miss fewer utterances.
it's better than a
but yeah but but the the chunks are already in general are short.
so i i i think it would be better to have to have more of them concatenated together in order to have better language model.
huh.
or language modeling.
i think two seconds huh
i don't know.
i would maybe go with one second.
i don't know it's a
well take a look
yeah.
do that.
yeah.
see what the length distribution is.
yeah but but there's there's really not much difference between the one second and the two second.
yeah.
really?
i wouldn't think that the language model would continue across two seconds.
so just take the one the one second one.
uh i'm i'm i'm just scared that with two seconds you get you get um you you get false recognitions.
yeah.
well yeah you do
you're going to yeah you're going to hurt yourself occasionally by having missing the language model context.
but you might hurt yourself more by having misrecognitions due to background speech or uh noise or whatever.
i i'm not too afraid about that as
when there when there would be something some background speech or something there there would be a a chunk in another channel.
oh i see then oh i see okay.
yeah i think it's better.
and when there is something in between i i i do not concatenate them.
oh right oh that's
the longer is better.
it's just when there is when they are sequentially and
there's there's
uhhuh okay sure.
so i i would use
we can try them all and see which works better.
there's a there's a lot of these cases just like now where i people say uh uh when they're trying to
uhhuh.
there's about a half second pause to a second in between and then another word
yep.
okay.
and it's much better if we can keep those together i think.
yeah it's uh funny looking at some of the transcripts.
i was filtering by person.
uhhuh.
and in one of the one of the early meetings one particular person almost the only thing they said the entire meeting was yeah uhhuh.
it was just a whole list of them.
it was very funny.
i bet i know who that was.
okay so so we need to split the waveforms then.
yeah.
or do you already have them split up?
no you don't.
right?
no.
so so i guess don would need your help to to create a new set of split uh meetings.
sure.
you know you just fake the format that you take as input with the synch times to a new set of synch times.
if
we could
right.
huh
uh do we know about disk?
and
but
oh yeah there's that pressure.
uh abbott disk.
uh i know they're in.
okay.
and uh but i don't know i think he was wasn't he asking about
he had a problem.
right?
yep.
yep.
oh.
well there was an issue.
he wanted to take it down.
and then he tried
he did and then it didn't work.
couldn't format them.
and
no.
i didn't hear anything after that.
the only reason i'm asking is you're going to need space to split them up.
and so i wanted to make sure we had some available for you.
i still have like probably .six seven eight gig on my disk.
okay so we're okay we're still okay for another couple days then.
and i have i have like another un backed i have another six gig which jeremy if you're not using can on x a.
yeah.
so we're okay for for a couple weeks then.
markham probably needs he probably needs us to approve another time to take things down.
to you.
right?
in order to do that.
yeah he he didn't say he hasn't said anything to me about it.
i thought he i thought he said in that mail that he would need to take it down another time.
so i think he
yeah i think
yeah he just didn't say when.
so
yeah he
well no i think he wanted us to tell him when.
how about during the picnic?
uh.
yeah i'm sure he'd appreciate that.
yeah i'm sure he'd love to
well my feeling about that
fortunately markham's not a transcriber.
but um
is is
beep.
well okay that's the point.
so it's jane that we have to coordinate that through.
uh what i was going to say is as soon as possible and i'm willing to not work for an hour to get it done.
uh but
i might not work.
oops i'm sorry.
oops!
whoa!
i'm i'm willing to not work for an hour.
because when abbott's
i know you're willing to not work for an hour.
yeah right.
but i
because when abbott is down you can't work.
you're really dedicated if you're no matter how you parse that one.
okay.
but uh i think the the people it disrupts the most are the transcribers.
well you know uh i all i need to do is mail um send them a mail like two days in advance so they can schedule their time.
i did that with the last outage i i wrote to them letting them know that this
okay.
huh.
so okay it sounds like markham should almost decide when he wants to do it and tell us as long as
was not um
so early next week.
and just as long as we have a little warning.
yeah.
so that means we can't um save meeting data either.
yeah.
right?
uh just not during that time when it's down.
but that it should be down for an hour.
so it just can't so we can't have two meetings in a row where the first meeting's during that hour.
that's all i meant.
right.
well no we can store them here.
that that's happening like today.
we can store them here.
you we just run the risk that if you have a crash we lose the data.
temporarily.
so
they're stored on popcorn.
this is on popcorn or
well no i i mean the
oh okay.
yep.
this is on popcorn or something?
yep.
yeah okay.
we store we store our data on popcorn.
anything else?
that's really great.
excuse me?
i was just thinking we store our data on popcorn.
how many how many institutes can you say do that?
yep.
pop goes the data.
okay.
uh megabytes and many megabytes too.
um what what uh
we have a kernel on popcorn too.
right.
that's very good.
so uh what what's on your queue for for recognition experiments?
can i have butter on my meeting?
let's talk about that for a second maybe.
what was the question?
uh
what what was on his queue for recognition experiments?
okay.
um i'm rebuilding the net that we're going to use for the tandem stuff.
and so what i'm doing is um putting in the stream reader into the quicknet libraries for the s r i feature files.
huh.
which is the right way to do it.
i mean when we did our first experiments and i was uh creating s r i feature files from the icsi front end i just had perl scripts you know and hacked a bunch of stuff together.
just to get it going.
but the the right way to do it is to integrate it in with the icsi tools.
and so
that's what i'm doing now.
and so once i get that done then i'll generate the p files i need.
because we already have the feature files in the s r i format.
uhhuh.
so what i need to do is make it so that the the quicknet stuff can read those.
and uh
is that independent?
or related to also being able to write out the uh feature file in the s r i format?
it's both.
there's a there's an input stream and an output stream.
input reader and an output stream.
oh okay so then you could use um you could use um uh like feacalc and just specify as an output format
yeah.
yeah.
yeah that's the point.
the the
oh okay.
i'm just ignorant about the software architecture of this thing.
yeah.
yeah so if if you
right quicknet is a very nice stream based library.
uhhuh.
so without too much effort once he has the classes written we can incorporate it into all the standard tools.
oh cool.
so
great.
so then it's uh tandem experiments after that?
uhhuh.
and at some point i'd like to get back to uh porting quicknet to the multiprocessor linux box.
yeah.
you know i i have forward passes working but i haven't done training yet.
so speaking of linux so there's some impetus at um s r i to actually uh build support linux as a platform.
so
uhhuh.
what that means is once we have uh everything running on linux we can
also use a
run all our jobs on your machines
yeah
we don't have too many we just have that just have a few linux
i mean if you can't use all the processors on whatever machine we'll help you with that.
yeah that's right.
well that's the nice thing about it is that since it's coarse parallelism you don't have to do anything special.
right exactly.
so i mean that would be a fine use for for that machine.
yeah.
so it's just uh
five more processors.
oh i know what it was.
uh yeah.
um yeah um
some
or if uh you know in the future if linux machines become like way cheaper than uh you know solaris machines then you know that wouldn't be a reason not to use linux anymore.
yep.
so
yeah.
um i think it would be neat at some point in this to do um a recognition uh pass on one of the p z m mikes for these same meetings that you've been
for the meeting?
yeah.
i mean it's going to be terrible.
uhhuh.
but you know we we just don't know how terrible.
it's also an interesting problem to come up with the reference.
uhhuh.
and
so the reference file for the relative times at which
oh yeah that's really hard.
so
well it's an interesting question because i was thinking well you can force align the transcriber transcripts and then of course you try to merge them in time.
it's not determined.
uhhuh.
but how do you score?
i think the first pass is throw out words which are overlapped.
i mean it's just an interesting problem.
that would be a good first pass.
just ignore everything that has any overlap.
huh.
right i mean there's a whole sort of
yeah because you have a set of scores about that.
right we right.
so maybe then that wouldn't be so bad.
but there's a whole interesting discussion.
because of course the alignments are not perfect either.
oh.
and so um in fact we actually don't have a a
right.
yeah.
i mean it's worth trying.
but that'd be a hell of a lot better than what we do with just these.
i
and
right right.
and and again if you rule out the overlap you have some numbers for that.
because that's yet another
but i i'm just concerned of course about that
oh i see you mean when only one person is
yep.
yes we could we could try
because you have scores for that for the other case.
right.
right exactly.
uhhuh.
and
yeah we we should try
we just don't know how bad it will be.
i mean one of the things that dave was noticing we were talking this morning is that it seems like and we don't know this in detail but it seems like you're getting a lot from the channel adaptation the speaker adaptation and so forth.
huh.
huh.
uhhuh.
um so you are already in that recognizer doing something that is likely to affect uh the the far field microphone uh formant.
huh.
right.
so it may not
i mean it's going to be bad.
right.
but uh it may not be like won't decode kind of bad.
right.
it might might only be that it goes from forty percent to eighty or something like that.
yeah eighty percent.
yeah.
right.
do you assume you know the speaker when you do this?
i want us to assume the whatever it was you assumed when you did the other the the close mike.
you just you just
well well there there's only one person who it can be.
because they own that microphone.
right.
i'm just wondering there's
yeah that that becomes another problem actually.
well and then you have the gender detection.
it's not a
yeah.
well but for a
but for for scoring you can do it or not do it as you choose.
right and
oh i'm
right but in terms of for
so
but you're saying for this for the adaptation you mean.
right.
so so not well for everything.
for adaptation right.
for for even feature normalization for uh vocal tract length estimation
you know do you do a supervised adaptation
right.
all of these adaptations
all of all of these assume you know who's speaking.
assume that the same person
so
huh.
you would have to do a speaker segmentation first on the far field signal.
yeah.
well but you can use the
when you're doing the scoring since you're you're going to be scoring against transcript you can use
you mean you want to cheat.
well you're doing that anyway.
well i was just
no if
ooo!
i don't like that term.
so try to cheat in the same way that you're doing with the close talking.
i don't like that term.
i
actually the those are two
i don't like that term.
i have
okay we we're going to bleep that out.
i i have a suggestion.
do the simplest thing first.
yeah right.
because we're going to want to know that anyway.
so wait the simplest thing is you cheat saying
in other words if you
no it's no it's even simpler thing than that is just that you don't know.
you mean you you don't do all those normalizations.
yeah.
yeah.
oh you totally unadapted.
just do or a free
yeah.
yeah because you can get a number uh for that with the other as well.
uh right.
right?
you can turn those things off.
right?
um actually we don't have any models.
yeah.
um you can
no you can
um
you can use a speaker uh
what about gender detection?
actually it's that it's it's we would have to re train models that are not that have none of that stuff uh in it.
but actually we could
we can just run it assuming that it's all one speaker basically.
and see what happens.
yeah.
yeah.
yeah and then put it in correctly and see how much that helps.
yeah.
yeah.
i mean i was just thinking do the one that's easiest first
because you want to know how much that's helping you in these cases anyhow.
actually
but
do you have gender dependent models?
actually no
sorry.
are the models gender dependent?
huh.
yeah.
yeah they're all gender dependent.
yeah.
so
yeah so you can
so we would have to at least do that.
no actually
no you can run both.
yeah.
and you can
yeah.
no actually what here's here's what we would usually do under these circumstances.
and pick whichever is better.
we would actually we would run some sort of segmentation.
thilo's is as good as any probably.
um and then we would do an unsupervised clustering of of the segments to
and and put the similar ones into bins that would be sort of pseudo speakers.
uhhuh.
and then we would do our standard processing on these pseudo speakers.
and that turns out to work very well on broadcast news spine.
those types of tasks where you don't have the speaker segmentation given to you.
does the clustering do you give it sort of a target number of clusters or is it adapted in some way?
um you can either do it by target number or by some measure of dissimilarity that you use as a threshold.
uhhuh.
that's what i'm just thinking one of the big differences with broadcast news and these meetings is we have many fewer participants.
the other thing is that you actually have direction here.
right.
so unlike these corpora that are recorded with other microphones like the right way to do this i guess you know in the future would be well in general thilo's sitting there and this p z m is going to you know he's roughly in that location.
speaker i d.
yeah.
well there're different ways of thinking about it.
i mean that that would be true if you had a meeting situation with multiple mikes.
but if you only had your p d a sitting in front of you
well any case where the people are not all sitting at the same place and they're not moving around too much.
and you have more than one mike.
yeah if you don't have one more than one mike you don't have a very good handle on location.
well you have distance and you have
that's it's not enough.
i mean that um jane's the pickup of adam on this mike is going to be different than me in terms of energy and so forth over the whole meeting.
oh so just from clustering you might be able to cluster it better because of that.
you might get some clustering from the speaker and some of it from the characteristics of the distance.
huh.
from mike.
yeah.
yeah.
and
but say if if you had a a cardioid mike or something sitting someplace then sitting there then it would its its response to him would be about the same as the response to him and so on.
and transfer functions.
right exactly.
well you can do you can you you can do certain normalizations like you know gain control
well i think there're lots of lots of ways of doing it.
they're both picked up in the clustering i guess.
uhhuh.
uh before you do the clustering to rule out those those types of things.
or to just do the clustering knowing that you're capturing both.
it's just that the kind of clustering we've done before hasn't had that uh distance factor or location factor in it in the same way.
i see.
yeah.
and so we're not really modeling it directly.
if somebody does we maybe we add that.
huh.
yeah that's an interesting
because i think it would be a pretty big difference.
when you listen you can sort of tell where people are.
it's a big difference.
yeah.
okay that would be fun fun to try.
not which you know side.
but
well humans are really good at that.
transfer function through the head and things like that.
right even with one microphone.
so you know even if you only have one ear you can still get get good transfers.
yeah.
yeah.
right so our our clustering is not going to be intelligent that way.
so
it's just going to pick up whatever energy difference or whatever is.
yeah.
but anyway i i'd it'd be neat to have that.
because you know we've been at this for a little while.
and we don't have have any results yet with with conversational speech at a distance.
uhhuh.
okay.
so um
huh.
yeah.
we should at least get a first one.
something.
yeah.
okay.
huh.
um and the other thing this would kind of be a hail mary.
uhhuh.
but but uh uh dave does have this stuff that is helping on digits.
and you know and so with then it'd be you know just throw that in and see.
oh yeah then we should
yeah so it'd be cool to see if it helped.
well first you have to filter the whole training set and re train.
yeah.
uhhuh.
yeah.
that would be quick.
uhhuh.
since i think he did it in matlab.
uh well he can do it in something else.
huh interesting.
yep.
but i mean you know it's
can't you export c from matlab?
um
actually we're experimenting with phase stuff now.
or is that mathematica?
and and uh this uh first result he got uh was really great.
it actually uh uh didn't exactly eliminate the reverberation.
but it completely got rid of the speech.
oh.
that would solve all of our problems.
wow.
yeah well i was thinking that.
wouldn't it?
well so just take the inverse and you're fine.
so it's
that's
you think i didn't tell him that?
no i got pretty excited.
because it completely got rid of the speech.
so i was thinking well so
so that's a speech detector.
that's great.
that's interesting.
you know it could be useful for lots of things.
so we have to
did it get rid of other stuff too though?
huh.
what?
did it get rid of other stuff besides the speech?
yeah just subtract that
well we have to sort of check that out.
subtract that from the original signal and you're set.
that's
okay.
wow interesting.
yeah.
yeah.
right then you can do like an you can estimate the the noise estimates.
noise estimate.
signal to noise.
right yeah.
that's great.
uhhuh.
reminds me of when when herve and i were first playing with uh context dependent things for nets.
and and at one point we took out the speech input.
so we only had priors and our performance went up.
huh.
wow.
i guess that's why herve always talks about using the priors as one of the mixtures in in his all ways combos.
huh.
yeah.
well of course it was a bug.
but i mean it's it's a but it was pretty it was pretty funny anyway.
but still
wow interesting.
yeah.
huh.
so if you run uh your recognizer with all probabilities equal what do you get out?
probably garbage.
whatever the language model says.
i bet the pruning
yep.
the pruning probably prunes everything out.
you get out switchboard.
that's just the the language model.
yeah that's right.
so that's how it was generated.
yeah so we have this new speaker adaptation.
um
a oh it's a sort of feature normalization.
uh like speaker adaptation.
which uh which i which i wrote about in the last status report.
which seems to be helping about a percent and a half on hub five.
so um we haven't tried that yet on the meetings.
uh but hopefully it'll help there too.
i want to ask um so you know that the data i have upgraded it considerably.
so i've probably made i probably corrected something like
well
it's really a substantial amount of things that i've caught changed um added to it.
including a lot of uh backchannels.
so when you're running things if you run it on the old
so if you if you run it on the new version then the numbers will be um and you compare it to the to runs on the old version then you're going to end up with more of an improvement than would actually be the case.
huh.
different.
uhhuh.
well we we have a frozen we do all our experiments with a frozen version of the transcripts as of i don't know.
as of february?
as of
no a little
i don't know.
when when did we grab the transcripts?
so a like the h l t paper?
the for the it was march probably.
sorry.
the
for these meetings?
we're talking about which which version we're using for evaluating the recognition.
which version of the transcripts.
right they're somewhere in between january and late march or something like that.
yeah something around there.
okay well so long as you have the same baseline then you'll be able to tell.
but they are channelized ones though?
no no.
yeah obviously yeah.
they're meetings that are now channelized but they were not from
so long as so long as it's the same baseline you'll be able to tell.
the uh the other thing is
the the
but but i'm just i'm just saying that if you were to compare that with running that on the new data that it would be an a more optimistic outcome.
huh and
well the and the other thing is it takes only a a minute to rescore all the old outputs with
if you had new transcripts then we we just rescore the old
right because you haven't done any training.
sorry?
right because we're not doing it for training.
yeah we haven't modified the recognizer at all.
right so it's really it would be really easy to re do it.
so actually um at some point we should update and rescore everything with you know the corrected transcripts.
we just we save
yeah it'd be interesting just to see how much it changes.
well the the
just to
i bet it wouldn't change a lot.
right.
what are the nature of most of the changes?
yeah.
we can take we can have a pool.
sometimes the changes are um cases where the recognizer would get it wrong anyway.
because it was some word that we didn't have in the vocabulary.
that's just what i was thinking.
so i i the thing is when
or
but it does help to get the backchannels back in and things like that.
right.
so whenever the right now the the scoring is based on segments.
um which is not great.
because for instance
so so the the other way to do the scoring is using a a nist format called s t m s.
uh segment time marked.
yep i we know about it.
where
so um i have to convert the uh transcripts into this format and then the scoring program actually looks at the times.
and uh you know it you can have a different segmentation in your recognizer output than in your references.
and it will still do the right thing.
so that's what we need to basically uh to
huh?
transcriber will export s t m.
oh.
in case you care.
well but then there's other changes.
so i mean there's other
okay.
we we strip away a lot of the mark up uh in the transcripts.
which you know isn't relevant to the scoring of the speech recognition output.
right.
so
would that doesn't that only change the scoring if a word has moved into a different segment?
i mean i don't think that's i hardly ever see that.
i think most of them are pretty good.
no i mean if
i i assume you also changed some boundaries.
huh i did.
right?
so if we want to use new transcripts with a different segmentation then we can't use them in the current way we do scoring.
we have to switch to this
oh you mean you would need to re run the recognition.
no we have to
there's a difference
i mean if you re ran the recognition then you just run it
right right.
oh i see.
if you want to use the old
you can actually never though really infer what you would get with a different
that's true.
you know it's it's probably more fair to re run the whole re run that.
uh in other words it's not really a scoring script problem.
it's
no but if you just want to see what
like suppose you fixed some no jane fixed some typos and you want to see what effect does that have on the word error.
then we can we
right but but if the segments change that won't work.
yeah yeah.
you it'll sort of work but it's not exactly what you would maybe get from recognition.
well i mean what happens if you break one segment into two?
you never know.
suddenly they don't match at all and you can't line them up anymore.
no but that's what the
well that's what i'm saying.
well you you you
you can line them up based on the times.
you can
yeah.
so the scoring program with if you give it an s t m reference file it will actually compare the words based on their time marks.
right.
yeah.
so therefore you can
um
does s t m do it per word or per utterance?
it's per utterance.
but it it allows as long as you hypothesize the word in the right segment in the reference it gives you credit for that.
what i thought.
all right.
so it does a it does a word alignment like you have to do for scoring.
uhhuh.
and then it does also some
but it constrains the words to lie within the the time bins of the reference.
within the segment.
yep.
i see.
and for you to get a credit for it.
so
that sounds great.
so so it's it should be just a straightforward re formatting issue of the references.
so
yeah i mean i was thinking the other day that this is not just conversational speech.
the fact that is has so much technological jargon in it it makes it considerably harder to um to transcribe and to to double check and all those things.
so i think you're going to find a substantial gain in terms of the word accuracy.
so long as those words are in your vocabulary.
well it definitely helps with um forced alignment too.
one percent.
because you know when we know the the the true words and we're adding them to the vocabulary and training a language model and so forth for future meetings especially the like the front end meetings or meetings with a lot of jargon in them that is not represented in switchboard or callhome then it then it really helps.
well all of them are all of our all of our meetings have a lot of jargon in them but we know what those words are.
so
yeah.
i mean i don't know how you get them into the vocabulary.
but it would seem
now i have to i have to i really need to uh raise a question about the term cheating.
okay?
and the reason is um if i understand that cheating is a term which is used to apply for basically what all of linguistics corpus linguistics does and what what my transcribers are doing and what i do which is a methodology whereby you actually physically mark things in the data.
all
like the transcription.
like the words that are actually there.
all we mean by that is that we're giving the recognizer more information than it would have if you were running it raw.
over a meeting that no person has ever listened to or transcribed.
okay i mean that that part is okay.
but i but i do wonder sometimes if it might be possible to use a term that's a little bit less evaluative like um hand marked.
uh but cheating is is
sure but but cheating is is is pretty commonly used to mean this.
it's a it's a technical term.
it i guess it it's been around for a long time.
it really is
it is yep.
and it it's sort of um because it's so strong a word people don't take it that seriously.
yeah.
yeah
in it's not negatively viewed.
it just really means
it's
it's even more than that.
i think it it really gives a very strong perspective that you know that what you were doing is not an un biased experiment.
if you don't say that then people think oh they did that and they threw that but that doesn't represent what would happen in the real world.
but if you say we did a cheating experiment which is really the standard way you'd say it it says you deliberately put in a piece of information that you would not have in the real world so that you can learn something.
just as part as your process.
so it's i i don't know.
i
okay i guess what i'm i'm thinking is just in if uh when these are presented in an interdisciplinary context it might be nice to add that explanation.
because otherwise it sounds like a pejorative statement on an alternative methodology.
because it sounds like it sort of devalues the the um all other approach which is to to put those distinctions in.
maybe i i've heard this at i c s l p for years and years and years though.
i mean it's it's pretty pretty common people say this.
if well you in a to a broader audience you could call it a diagnostic experiment rather than a cheating experiment.
actually jane is right.
like in conversation analysis i've never heard people use this.
because they're not using an automatic system.
so it really
but they don't run experiments.
yeah.
right i'm well
they can do experiments.
you can do experiments.
but the it's cheating relative to what we call a system where we can completely control this black box.
and it's not a very smart system.
it it only knows what we give it.
and if if it knows more than we would really give it when it runs we call it cheating.
but it's
yeah it's only used in a community that does some type of computational modeling.
yeah in pattern recognition or in
i think it's really not used in any kind of community doing experiments on human perception.
yeah.
or
right.
you know so she certainly can define it.
yeah it's machine machine experiments.
um i mean when when the uh the neural net uh wave hit in the mid-eighties
and by the late eighties there were uh we were reviewing thousands of papers that were coming out in neural nets.
it was really hot.
everybody thought it would do everything.
um and a really common error that people were making was they were just reporting their uh uh classification results on the data that they were training on.
and so i think it was it was it was very important for people then who were doing something diagnostic to say hey i know i'm doing something that isn't kosher.
huh.
and to make it really clear that they knew it.
so that was i think why why it became a popular term.
yeah it just seems like you know if it were bootstrapping or if it were i mean there are other ways to maybe get the point across.
but it isn't bootstrapping.
uh it's it's really cheating.
it's it's using information you wouldn't normally have.
it's just
yeah yeah.
well okay.
but it's cheating in a way that's it's it's announcing to everybody hey i'm cheating by doing this.
so
it's saying so it's all above the table.
i'm actually using this other thing.
okay.
that's that's that's the reason.
okay all right.
bootstrapping would imply it was actually legitimate in some kind of way.
huh.
well i i think
but
we're not de legitimizing the data.
we're de legitimizing the experiment.
we're not saying that the data is cheating data.
we're saying we are cheating by using this data.
yeah.
okay.
yeah.
because normally you wouldn't have that data available.
okay okay.
yeah.
anyway.
does seem to me that it that it carries over some baggage with it that uh that
i can understand it in context as you described it.
but it it seems to me that um it does import some negative evaluation that um would maybe not be good if
it has some shock value.
it has shock value.
and
which is part of why it's used.
to the to the wrong audience i think that that might be a negative shock.
yeah i i think to the wrong audience i agree that um
uhhuh.
it's like a disclaimer.
to the wrong we should just explain what it what it means here and that it's a common term.
yeah we started with hand marked data or with uh you know hand transcribed data.
and
we're
or
shoot!
yeah.
did you break the microphone?
yeah.
just the clip.
nnn okay.
now we have to delete that expletive.
okay well i feel better now.
thank you very much.
huh the first time i heard that i i thought you know the same thing.
thank you andreas.
and i guess you after a while it becomes almost a it's a bit of a a humbling thing when somebody says that.
it's it it really is part of the jargon.
but it's also is
if they get good results but we were cheating on this feature because we took it for granted even though we can't really assume that then it's actually the opposite.
right.
it's
the trouble is that
you know i understand it in that context.
but it but it is almost resentful.
it's almost like you know resentful of the data resentful of the um the hard work that's going into preparing the data.
well
is
no no it has i don't think you're saying the data is cheating.
well it's it's
i think you're saying in my experiment i cheated in this way.
that's what i was saying.
yeah.
i mean another is what liz was talking about how in the switchboard tests in all the switchboard tests we've been doing we've been making the same uh same uh uh standard using the same standard way of of getting the data to test on.
which means that we weren't actually running it on on uh on data that had had no speech.
and in a sense that was cheating.
okay.
so so it it's it's a good wake up call if people well we have this performance but you have to keep in mind we're doing we're we're not doing the whole real task in this way and this way and this way.
but i'll convince you that that it's still important for you to listen to what i have to say next because of this and this.
okay okay.
and so it's just a way of putting it all out on the table uh as opposed to
uh and it's used for a lot of different types of data.
so whether you have segmentation or not is it male or female or not um do you know the signal to noise.
yeah.
huh.
like that's another one i see all the time where you assume it's known.
yeah sure.
oh interesting.
yeah.
and you say it's cheating because you don't actually compute it.
huh.
i didn't know that.
huh!
in
in the multi band experiments in the first one we really wanted to find out what if you knew which band was really noisy.
uhhuh.
right.
i mean suppose you just know that.
same cheating.
and then then even if you know that can that help you?
i mean what can what strategy can you do to do well without that particular band in the spectrum?
huh.
and so that was important to know as a baseline.
and and then once you knew that then you go well now how do i know that that's noisy.
huh.
okay.
and and and uh
but you know i i should also say that there's a lot of it's not just the word cheating but there's lots of other things that we talk about which as soon as you go outside of your narrow little group it gets very very confusing to people.
uhhuh.
oh sure terminology is always context dependent.
and
no question about it.
it's just that it seems like the the point without the negative evaluation would be.
however you know i understand your point that it it has a long tradition in this field.
it's not even really negative.
i didn't realize.
and is used
this is interesting what what adam said about it being used also for a bunch of other uh dimensions.
well the example i was thinking of also was this thing that that herve and and hynek and i made about uh increasing the error rate.
and so we we did a number of papers and talks and so forth about uh uh the the virtues of increasing the error rate.
oh.
okay.
and i mean and the whole point of it was not that it was good to increase your error rate but it was good to be willing to risk increasing your error rate by trying risky things and trying because there's this notion of a local minimum that if you just have some system that's very complex
and you you you turn some knobs to try to make it better and better you'll never get out of this local minimum.
fine tune little bits.
uhhuh.
you have to be willing to jump to something that's quite different.
huh.
interesting.
and the first time you jump to something quite different or maybe the first ten times or a hundred times you do it's going to be much much worse.
because you've optimized the other system.
so the effect the immediate effect is going to be to increase your error rate.
interesting.
and so it was we had a couple papers like towards increasing the error rate in speech and and so on.
huh.
and we really did get feedback from a few people some of whom were you know fairly senior that that um well you know you you you we're really concerned about you misleading people into thinking they should be increasing the error rate.
huh.
yeah.
and uh and we go we thought oh come on.
but
did you read the paper?
but weren't you cheating in those experiments?
yeah.
that's
we weren't.
that's why we increased the error rate.
interesting.
actually i think of cheating as a way to do some work where you can't address all of the computational tasks.
like if we want to study um speaker habits but we can't do speaker detection.
but we want to assume let's say we know this is jane and we know this is chuck.
even though automatically to look at the habits we would need to also first figure that out.
uhhuh.
but we can sort of cheat on that factor.
because it's somebody else's research.
and then we just
so it's then we want to make a proof of concept for
what what this
right what would this be like if it were perfect?
if someone else
if this component were perfect.
right uh or but we really can't work on that problem.
yep.
we don't have time.
we're not interested or whatever.
huh.
and so you you assume it's given.
it's too hard usually.
because it's um you want to go forward with your research and assume that you have that information.
uhhuh.
so
i guess if i don't ever think of it as negative more like it's something we're not building.
it's not pejorative at all.
well the term itself is.
you know?
but it's not pejorative towards the data.
but but it and it's pejorative with respect to a certain purpose.
it's pejorative towards
i understand.
it's pejorative to ourselves.
i i wanted to raise the issue.
self
right?
to say i am cheating in this experiment is not saying that the data is bad.
yeah.
yeah.
but
it's saying that my experiment is bad.
yeah.
anyway it's colloquial.
anyway.
and and uh it's it's interesting to hear that that someone coming from a different direction it it sounds the way it sounds to you.
but but uh i i'd never heard that before.
yep.
okay that's interesting okay.
so so so that's interesting.
well i wanted to raise the issue.
and um
uhhuh.
i i uh i appreciate the discussion.
i wanted to ask one other question which is a different matter.
which is with respect to um this thing that you've been working on for the recording monitoring script.
so the idea you had this script that you're working on to be sure that the microphone values are in are kept in.
oh right i haven't gotten back to that recently.
okay okay.
but
huh.
i assume you're saying you want me to get back to it.
because you know
well i was just wondering if
because i i am finding that um in double checking i've run across um one data set where the microphone was off early on.
and then two other speakers their microphones went off later.
huh.
and this is out of like seven speakers.
so out of seven speakers four microphones you know basically three three microphones basically flaked.
which makes it hard for the data to be used for all possible purposes.
do you think the battery ran out?
that's what i think.
or
i think the two that flaked late i think it was the battery.
but you know if the script could um you know alert the recording person to that
i mean i don't know if there's a way to replace a battery if it happens in the middle of a meeting.
well
maybe that's hopeless anyway.
well you can but you'll lose a lot of data.
but
yeah.
but i mean that doesn't really help.
because often the recording person isn't in the room.
oh i see.
so what are you going to do?
i mean it will the person if you're looking up at the board and i disable the screensaver you will see that the mike is off.
but that doesn't necessarily help.
okay then i guess that raises the question of uh whether we should screen the data before they get transcribed.
so
because although i think that the data are still useful in terms of providing content.
and and that i know that having three out of seven microphones out of commission during some part of the meeting restricts the uh usefulness of the data for other purposes.
yeah.
uhhuh.
i i think that's a really good idea.
and maybe you don't want to have it
because for all kinds of studies we don't really enjoy meetings where some of the where the signal's going off at times.
it just makes it hard to study any kind of parameters.
so
if there's a way to check the signal quality before transcribing it and you find any problem at all it'd be much better to go to another meeting i think.
i actually think that thilo's in a in a a you know when you do the pre segmenter and you and you run across troubles he he runs across some of these that are
yeah.
yeah some of these things are captured by yeah um by looking at at the minimum and maximum and whatever.
so we have some part of that.
you find in the channels.
and
it's just hard to tell between that and just someone not talking.
i
but
yep yep yeah yeah.
so
well the and the other aspect of it is um that when the microphone is not well adjusted then even if it's not a lapel mike you can get lapel mike type behavior.
right.
because i'm i'm expecting for example with this that you're going to end up you know picking up other peoples' signals.
i've already written some notes here.
so i
uhhuh.
um right uh
yeah i mean it's just you know the microphone is intended for certain adjustments.
well we should be getting new equipment in so we don't have to use the earplug any more.
okay.
but it's my ear.
i'm sure it's something wrong with my head.
it's your hair.
huh.
but
um actually is there a way though to use whatever you're using for background noise to check post hoc that a microphone was constantly on?
i mean
i mean you you can do sort of a check.
uh like this
but it will be very hard to tell the difference between that and um someone not talking.
just so it doesn't when the microphone's dead it doesn't put out zeros?
no.
or
uhhuh.
what does it put out?
really?
johnson noise.
but then how are how are you detecting during a meeting?
little bit of noise.
yeah.
i use a threshold.
if it's below a particular value it it flashes yellow.
so as i uh but it's not perfect.
but is it better than nothing?
yeah probably.
because it would really be
i mean this is the reason why i haven't gotten back to it is because my first pass at it didn't really work.
because all the mikes have different noise levels.
right.
huh.
and so i have to do something a little more clever.
it's just the noise from the connections and the everything.
but you could
yep yep.
so you could run that post hoc on a already recorded meeting in the sense that you know not everyone as you just said you won't be there.
and then if we find any problems have the transcribers listen and
yep.
i really think it's better not to transcribe a meeting that's going to have problems once you've spent all this effort.
yep.
the only argument for doing so would be with reference to the content content maps.
yeah yeah.
uh but maybe then it should be done in the old original way.
instead of channelizing and having the uh uh you know elaborate coding of the backchannels.
huh.
yeah you could still uh
yeah there's uh the the standard deviation of the signal gives you a good clue.
i mean if that is too low then you can be pretty sure that it's uh empty.
uhhuh.
huh.
yeah.
yeah sometimes you you capture that when you mix it together.
and
right exactly.
yeah yeah.
and actually an an alternative to even doing that level of transcription would be to have a transcriber listen and and you know maybe just
oh i don't know if a uh
well someone who's associated with the meeting could have like a summary of points handled in the meeting.
you know maybe if we could pay one person who knows that subject matter to do an outline of the meeting's content.
uhhuh.
instead of losing the content altogether.
well it's even if if you could still transcribe the words based on the far field microphone or something you could uh still use it for say language modeling.
is there a
far field.
yeah.
you know.
i guess the trouble in my mind is that it that it's not a very neat corpus.
if you say these data are available.
yeah.
right.
but these are imperfect.
yeah.
because of the you know the batteries flaked.
well we'll just we'll we'll just have to note those.
i'd we're i'd much rather have you know meetings that have all the channels.
even if we had to skip a meeting or something.
well if
right.
just for all these other purposes.
so
and we have so many.
i guess there is no
you could put them at the back of the queue or something.
well i i i think we can't throw away that data.
yeah.
because otherwise we'll end up with very few meetings.
right but just
there's there's no shortage of meetings.
yeah.
but
yep.
so
right there but there's a shortage of transcription power.
we can afford we can
we have such a backlog i mean to be able to get through the backlog of the of the good meetings it would be nice to not have energies diverted.
huh.
yeah.
do we have a
i'm sorry for interrupting.
do we have an e d u meeting at four?
that's fine.
huh
oh good question.
you know what?
they their four o'clock cancelled.
i don't think so.
and they're starting at four twenty.
and i'll set up at four fifteen.
okay great.
good.
uhhuh.
huh hey i bet there's tea.
yeah but there's
because otherwise i was going to say we have to cancel.
yeah.
there's tea also though.
yeah.
okay.
yeah so you'll have
so maybe we should maybe we should do a uh simultaneous digits digits read in the interest of of getting the snacks.
a simultaneous digit okay.
uhhuh.
hey i've never done one of those.
oh you never have?
oh it's a treat.
you have to plug your ears or just prepare not to laugh.
right.
okay.
everyone ready?
reading simultaneous digits three two one!
yeah.
yep yep.
okay so we're live.
okay.
are we live or are we memorex?
we're somewhere in between.
okay so that's
okay so we have some sheets for uh some standard doohickies to
wait.
right so
you're going to sit there?
is that all right?
yeah okay.
maybe i'll put this thing here then.
okay.
okay so this is february second uh two thousand.
this is uh meeting number one with adam and dan and morgan about five o'clock uh in the afternoon.
a range of microphones.
yeah.
what's this one here?
this this one is has a couple of cheezy electrettes and
yeah that's that's the dummy dummy p d a.
that's connected too or
yep.
yeah it is.
that's connected.
yeah both channels.
it's got two to both channels.
oh hang on.
yeah.
yeah it is at the moment.
okay.
yep.
okay.
this is
although the gain is pretty low.
so for the um read numbers task i have extracted these
uhhuh.
hang on let's let's okay let's just let's just name the microphones.
okay.
so i'm speaking on the ear mounted uh wired headset thing.
okay.
right and i'm now talking on microphone number two.
uh the wireless microphone number two.
yeah.
um i have no idea which one i'm i'm on.
i'm
oh one!
you're on one.
oh i'm number one.
absolutely.
okay which is a a a lapel.
in fact it's sticking through my lapel.
yes.
okay so this is this is the p z m nearest the uh uh the machine room end of the table.
actually
then there's
i know but yeah.
it's number three it says but we'll be able to figure it out.
this is at the middle of the table.
this further down the table.
and this this one is right at the end of the table.
okay?
and then we've got the uh
this is the left side of the dummy p d a and this would be the right side.
i think listening to this is going to be like watching somebody's home movies.
yes it'll be pretty horrible.
yeah okay.
very exciting.
so what i have on these forms here is for the read numbers read digits tasks and it's extracted directly from aurora.
and uh
so what i was thinking is we could start just by filling it out and then reading reading the numbers on the form.
okay what do we do with the stuff on top?
fill it out.
but i
do we say it after we
no.
no you don't need to.
just write it down.
what are we why are we writing it down?
so that when we transcribe the data we can figure out who said it and their gender and the date and the time and what mike they were on and all that sort of thing.
oh.
okay that's good.
so i don't need to fill that out right now.
so maybe we should just say it.
why don't you want to fill it out right now?
okay.
just so that
so then we're not spending the time writing it out.
so that we're spending the time talking.
well how about you give me one of those.
and we can fill it out.
and morgan can read.
and then we can do it like that.
i'm just uh concerned about a bunch of dead time while we all sit here and fill out the things.
okay.
yeah yeah yeah.
right?
and so you have to be sure to pause between each line since we're going to be segmenting it and then doing it that way.
oh.
right.
okay so this is morgan.
i'm going to read some numbers now.
that's my group.
so i definitely wasn't pausing enough between them.
this is these are things we will find out.
that'll be an issue.
yep.
yeah.
i don't know if i said mine was transcript zero.
but it was.
uhhuh!
oh good.
okay.
okay.
that's a little bit.
can i have something to write on the back of?
so so where are we in this?
thanks.
let's see so we now we now have a few mikes that work.
how long did it take you to set this up?
oh!
it didn't doesn't take well i mean as you see we haven't really set it up that well but it doesn't it doesn't take long to set up.
i mean we just came up here one night after recording so it took like twenty minutes something like that.
right and that that included a fair amount of fiddling around since it's it's more or less the first time we've tried to do it.
i mean it's
yeah i mean the equipment
right.
the me and adam went through it yesterday just looked at everything.
but it's all kind of here and set up so it's just a question of turning it all on.
and um running the program.
i mean eventually we'll want to like tape that over and run them up through the center so that wires aren't aren't people aren't tripping over the wires and so on.
right.
are we going to i mean is it going to be over there or is it going to be in there?
um it's it's going to it's for reasons because of the microphones runs we want that chunk there.
there's more equipment in there.
but there's the idea is that there are these microphones on the table.
there are some cables coming down sort of under a nice mat.
and then that's they a bunch of them get sort of converged with digital there.
and the idea is that this gets replaced by a cabinet with doors so that so that it's not open to fiddling.
so that's what's going to happen.
right.
that's
do we have like a cabinet on order or do we just need to do that?
we need to do that.
it's on a list of things to do.
i keep on hoping that they're going to get done but they don't.
well i i can help them get done um one question is budget.
i
yeah.
do we have any money at all that we can go out and spend on things like cabinets or a hard drive or things like that?
oh i mean i don't know.
did we did we spend our the budget already that we had?
yeah.
uh how much are we talking about here?
um i don't know.
a cabinet is probably going to cost a hundred dollars two hundred dollars something like that.
yeah i mean you know we we can spend under a thousand dollars or something without without worrying about it.
i mean
yeah.
i mean i think um i'm more worried about things being kept in a funky state long enough that that uh stuff gets broken or or you know.
yeah.
well i think our intention at this point is when we're not using it to record a meeting we'll coil them all up and put them under so that people will not be tripping over them and so on.
yeah.
so
but well we should get to a a nice kind of working state of our set up as quickly as possible.
yeah and there's also the minor matter it'd be kind of nice to able to bring people in and say yes this is where we do such and such and then be able to see uh you know some kind of i mean it's less important than doing what we're doing right now.
right.
not have it
yeah.
but
yeah.
so
i mean there's one big issue with the equipment still which is um ultimately we're going to well the idea is to have any number of these wired headsets.
but there's this amplification problem.
i built this thing it's that's it's very noisy.
we're actually recording i'm i'm going through it at the moment but it's so noisy that i am not using for this.
it's it's designed to be used for the for the desktop meeting as well.
but i'm using running that through my uh damp machine as a pre amp as an alternative.
well we should be able to just go buy some preamps i would think.
yeah.
it's just it uh
you see how clumsy this is.
right?
i have to use these batteries to bias them.
and i'm only get two channels and there're a bunch of there are like three connectors in the circuit.
the thing is like it's it's not it's not that the thing doesn't exist commercially but there isn't one unit which does the whole thing in one in one swoop.
so it seemed like it was a good idea to make one.
um and i'm still not sure.
i mean i think i probably know how to fix this.
yeah.
i can probably build it better so it doesn't have such noise but it's kind of a buy.
well ultimately we're going to need to build one anyway.
are we?
sure.
well we could just
because we're going to have to put it in a p d a.
well
right?
if we have microphones in a p d a it's going to need bias and a pre amp.
i guess i'd sort of figured that that jim would do that at some point.
uhhuh.
right?
yeah.
well
yeah.
jim is busy at the moment.
that's the problem.
i was kind of hoping that jim was going to build this box.
but um but he hasn't had time.
yeah.
well i think i think he is quite interested in doing it.
but
okay.
well if it's too noisy and if there's something we can buy again for you know moderate amount of price even if it takes a couple boxes or something maybe we should just do it.
yeah.
yeah.
and then and then later we can uh replace it with
because there may be other considerations that will go into the design.
i mean uh our getting some experience with this may help to determine some of things that are i don't know might change this design so.
uhhuh.
right.
there was uh a statement made uh which i don't think is right by the way but there was a statement made at the uh darpa communicator meeting i was at recently that um microphone multiple microphones at a distance of less than three feet or something like that were totally useless.
completely irrelevant.
oh!
yeah.
you you mean for noise cancelling or
for noise cancelling.
um my counter argument is is an uh uh um a white rat.
uh which has it's two microphones exceedingly close together and it works pretty well.
oh.
um but yeah.
okay.
this happened.
do they mean omnidirectional?
is that their point?
i mean all the noise cancelling mikes they're you know they're a few millimeters away from each other pointing in different directions.
i think uh i mean uh my guess is that means it's just that if it's if it's near field the mathematics of beam beam steering is different.
but
oh i see.
if the if the wavefronts are not approximately planar then you can't just do delay and sum and expect it to work.
it's oh okay so they're not talking about how far apart the mikes are they're talking about how far they are from the speaker.
i guess.
yeah.
right.
but i think that if my point is i guess if you if you have a um a uh a volume in between the two the two uh uh sensors and you do something cleverer there clearly are ways to get more out of it.
yeah.
yeah.
yeah yeah.
right right right.
i mean the um the the weird thing is that actually delay in some beam forming is not is not the clever thing to do.
yeah.
the the the underwater guys do this stuff which is different.
and it actually means that you should get the microphones as close together as possible.
rather than trying to get them far apart so you can get separation you sort of you want them very close.
and and the math is too difficult for me to understand but it's but it's there.
yeah.
well there are various points in meeting where people were making uh uh very strong statements as a matter of fact like one shouldn't even question it.
and this was one of them that was kind of funny.
um but i i i what i take it to mean is doing doing the dumb thing uh with two microphones would be very difficult to get too much out of it.
yeah.
yeah.
but i think it will be interesting to do other things that aren't dumb.
so i i still think it's potentially interesting to do that.
huh.
although we'll have to be uh aware of this if we're writing proposals about it wherever that there may well be reviewers who will say well everyone knows that you can't get anything out of it if it's if it's close.
yeah.
do they believe you can get speaker location out of it?
because that's that's one way we could sneak it in.
uh that is an interesting point.
yeah.
yeah.
certainly you can get some
well we could i mean
by the time we'll have we'll have data so we can actually you know demonstrate that.
yeah.
well
hopefully.
oh you mean for location?
yeah.
okay.
well we're getting some we're getting some right now.
depends on who's working on it.
yeah right.
but someone has to write some software to to actually do it.
right.
right.
but i mean
well someone someone might be interested in in it.
someone might.
oh that's uh
i was just thinking one thing we may want to do is put the fake p d a right in front of someone instead of in the middle of the table like we have it now.
yeah.
on the thought that my vision of it is you know each of us will have our little p d a in front of us and so the acoustics uh you might want to try to match the acoustics.
maybe.
yeah.
but on the other hand suppose that uh you're the only one uh who is advanced enough in this thinking to actually bring such a p d a to to a meeting.
no and i don't need to meet with those people.
and and and therefore you bring it in.
and you probably want it to be as near to people as possible so you might in fact shove it to the middle of the table.
uhhuh.
well
yep yep.
for those those we can make several several more of these and then we can have actual recording with you know three p d as.
yeah.
well i think we should.
i mean i think at least with two.
uhhuh.
right?
right.
because uh one of the one of the fantasies that was at least fun to talk about was that uh
uh okay maybe you you're the range of things you can do with multiple microphones is limited if they're a few inches apart.
but what if you have two of these things
yeah.
and you know we have this this idea of the handshake back and forth and say i'm here i'm here.
yeah.
right.
we're now an array.
right right.
yeah.
and that that seems very uh easy to believe scenario i mean given the number of palm pilots we've got on the table right now so
that's right.
three.
right?
yeah.
yeah.
so i guess this a pretty dense group.
but it's getting a lot higher.
yeah but but that it's
we're not even you know it's we're just it's not like we we we may be technical but we're not particularly p d a people.
so it's that's just how it is.
well i'm trying to be.
so
well i know.
well that's true.
i mean
except i got a p d a way before
well i was doing i was doing wearable computers at boeing so i guess i sort of count.
okay.
but
so uh so where are we uh now?
i mean so we've just uh we're doing this recording so we show we can we're capable of doing recording.
right.
uh there's i guess a little more wiring to do or something to have to have more if we had more people participating in the meeting we'd wouldn't quite be ready for that.
uhhuh.
uh.
well we've got we've got right now we could have two more people on wireless.
uhhuh.
with a trip to leo's audio we could get a third extra person on wireless.
and uh right now we could have three more three more people wearing mikes like this and that would just work.
i mean i'm not sure how how good the quality is on this mike but i think it's going to be okay.
so we could actually have eight people with with headset mikes with i mean basically no extra effort.
well five with wireless.
so we could have eight.
yeah.
nine.
nine.
excuse me nine.
yeah.
five with wireless and four without and four with wired.
yeah.
um we don't have headsets though.
yeah.
we have the um the ones we've been using the ones we got for the p c.
oh okay so we just have to wander around and collect them up.
yeah yeah.
i think we have three of those.
uhhuh.
so and the
uh there's one in my office.
so
right right.
they don't all work but the nice ones the um plantronic ones work.
okay.
so we'll just have to see if that works.
so a little bit of futzing there's the issue about a cabinet we might want to change some pre amps.
yeah.
so we can do some more recordings but there's there's a little little bit of ramp up on that over the next month say.
yeah yeah.
right and also i would like to work out some of the software issues a little more completely.
so um you know we're going to want to resample and and bring them over to another machine and uh um organize the how we're going to store it all.
yeah.
yeah.
yeah.
yeah.
yeah we were talking about that a little bit over coffee today that that um uh if you downsample it sounded like it came it sounded like it was something like a gigabyte an hour if you didn't downsample.
and once you downsampled then it was something like four hundred megabytes an hour if you used all if you used all
per channel or
no if you used all the channels.
something like that?
we figured out that it was twelve twelve gigabytes an hour.
it was more than that.
for for all the channels.
was that with or without the
i thought it was one gigabyte.
it couldn't be twelve.
because no we've only no i mean we've only got four gigabytes and that
we said four gigabytes would give us twenty minutes before we did anything else.
yeah.
right.
and then we then we cut it in half.
and then we cut in half by
how'd you cut it?
but by saving sixteen bits on instead of thirty instead of thirty two.
okay so that's so that's two gigabytes.
and then
well no i mean it's uh four gigabytes for forty minutes.
that's forty minutes.
right.
so six gigabytes.
but that's at forty four kilohertz with sixteen channels.
forty eight kilohertz yeah.
so we can actually triple that.
so you can
right so you can
yeah.
so hang on.
so that would okay
i came out at the gig an hour at one point.
that that's one computation i did for sixteen channels sixteen bit.
well i think it's i think it's two gig an hour.
is it two gig an hour?
i'm off by an order of
that's all right.
order of two.
because like we don't maybe we don't we don't need to save all channels.
factor of two.
right?
so on average we could we don't we we wouldn't use all the channels necessarily.
oh we'll do the higher math later maybe but it's so it's so maybe like a gig or two a an hour.
um but um
yeah.
okay that is a bit more than i thought.
how much here would we have to reformat the drive in there to get some of the more space?
because it i mean it's a nine gig drive and there's four gig on on scratch and there's about three gig unused it looked like on the home directory.
yeah.
i'm sure.
yeah.
i just don't know enough about linux.
how hard is it to repartition?
it's a pain.
it's a pain.
i mean you know because because we've got data on user.
right?
and then we'd have to back it all up and copy it back on again.
uhhuh.
yeah so this'll take some thought because we have we um
if we are in fact going to have forty or fifty hours at some point and we we want to have part of our work be on feature extraction it means that we need to have waveforms available somehow.
yeah.
huh.
right.
so i've i've already created a u u doctor speech data m r directory or had jane do it and uh um but there's not a lot of free space around.
no.
so it it it has currently about four gig on the drive that i think she's going to put it on um.
uhhuh.
uhhuh.
and so we need we definitely need to go buy uh you know forty gig fifty gig of drive drive space.
yeah.
we don't need all we don't necessarily need it all online.
i mean it will be different subsets.
so you won't you won't you know do all sixteen channels in a single
that's true.
i hadn't even thought about that although it'd be kind of interesting.
yeah.
what would you do?
you typically train i mean you'd have you'd have fifty hours.
i mean
we'd only use one of the p z m channels.
something like this.
and so that would you wouldn't have to use all sixteen channels at once.
but it's kind of painful for because basically if we put them on c ds then each c d will have all the channels for one recording.
and then when you want to build the p files you just want to get one of those files off each of the sessions.
and
yeah it's pretty horrible.
so what would we do for training?
so if we're if we're training distant we'd just use one of them?
yeah.
well maybe two of them but i mean we wouldn't use we'd only use the distant mikes.
okay.
right?
yeah so it might be four.
right?
but it wouldn't be sixteen.
well although we wanted to get alignments from the close mike stuff.
but that's a separate training.
right.
but you don't want to be constantly take putting that on disk and off disk and on disk.
no wait.
more disk is better.
absolutely.
i mean my my preference would be to get get ourselves a hundred gig of disk.
yeah.
the thing is it's really not very expensive right now.
yeah.
and just not bother.
so it's more of an issue of of structuring it with uh the uh sys admin folks so we don't panic them and figuring out where it goes.
and back up.
well we don't it doesn't need to be backed up.
right if if we if we burn it all on c d rom then um
right then it doesn't need to be backed up.
in fact we could you know we could throw away the broadcast news stuff and put it on there or something like that.
yeah we were talking about that.
yeah.
i'm not sure if anyone's still using it.
i think the only one using it has been javier.
somebody used it recently.
but at this point i think it's he's only been using it for features that he calculated quite a while ago.
so if nobody else is doing feature calculation
yeah.
i'll send mail to speech local.
pardon?
i'll send mail to speech local and see if anyone's still using it.
how much space is that using up?
do you have any idea?
it's uh twenty c ds.
so that's twelve gigabytes.
so that would last us for a while.
yeah couple hours.
well anyway for this this recording we're fine.
yeah.
because we're only we're only
how many mikes are we recording right now?
well right now we don't have any choice.
it still records all sixteen channels.
it's just most of them are all zeroes.
so that that's on our list too.
okay.
yeah well that we can do something about.
yeah.
yep.
it's mostly a question of user interface.
how do we how do we specify which ones are on?
yeah.
yeah so we don't store any of our audio formats compressed in any way.
do we?
so these large sections when you guys aren't talking
yeah.
probably we could we could probably just run shorten on them or something and it will get a big win.
i think that's a good
run what on those?
shorten.
tony robinson's uh lossless compression thing.
okay.
what's it do?
um it it does a r modeling
and so it looks at a section it runs an a r model to sort of to whiten it and then quantizes the residual with a nice with the three bits it can get away with.
something like that.
so i think it on on each block it'll it'll compress you know if there's load on in any range it'll use ratio bits.
uhhuh.
i think it does that.
yeah so p z ms.
yeah well
yeah.
yeah it's something that that we definitely need to look at with this.
maybe we shouldn't have subsampled it down to sixteen bits for our first try.
well it's okay.
it's only the first try.
i what i'm thinking is um obviously we can do the we can we can choose any sixteen bits from the twenty four so we can just have different gain things that we use you know per channel.
right.
yeah so we might want to see if maybe the low sixteen bits on the p z ms are better than the middle sixteen bits.
yeah.
i think probably the bottom four bits are not any good.
regardless yeah.
right?
although you know it claims they are.
but probably the
i was just noticing on the on the there's a view meter software view meter and the p z m gain is real low.
yeah so i think initially we should take up more space until we've analyzed this and get a better sense of it.
so
because uh we i mean we're not talking about forty or fifty hours at this point.
uhhuh.
yeah.
we're talking about a half hour of of speech.
yep.
and so uh we should look at this carefully.
yeah.
but
yeah i was interested when i saw the twenty four on there.
oh yeah.
wow i mean that's probably eight bits of noise.
oh yeah.
yeah.
so
yeah that's right.
we've got lots of bits of noise.
these theories things are pretty amazing.
yeah.
and and there's help help they seem they seem good though they seem to be
it's some sort of oversampling thing i imagine.
right?
didn't we already get that?
oh god knows.
yeah yeah.
because you you can't have uh component tolerances that are uh sixteen million one in sixteen million parts.
right.
so
right.
right the other other thing another issue is just getting a sound card up here.
you know i assumed that we could just put a sound blaster in if we want sound output.
that that card that machine's got sound they've got a sound blaster built into it.
oh it's just not working.
i just i just disabled it because i wanted to find out how well the sound infrastructure would deal with both at once.
okay.
but i think yeah that's uh probably it will be fine.
yeah that way we don't we can't listen to it on this machine.
i'm
there's no
oh!
for a couple of reasons.
well but the there are two ways we could listen to it.
there's a sound there's actually sound output built into this thing.
yeah.
but the driver i haven't the driver may not support it yet.
and then there's also the sound or the sound card built in.
huh.
but i don't know how to i don't know how the operating system would like having two sound cards.
but we can f t p stuff over.
right?
right.
so it's connected.
yeah.
and we've looked at it so we know it's working.
i mean you can see the little squiggles and it looks like speech so
yeah.
yeah.
you just can't listen.
yeah.
okay.
um
one suggestion i have is that um i think this maybe we should just read some more you made more of these.
right?
right.
i think we should read some more because because this is very
okay.
we we need to to work the the the uh the talkers more.
because we're not going to have that many meetings.
uhhuh.
and this is a pretty tiny amount of data.
uhhuh.
so maybe we should do another number list and
well right i i just did ten per page i could easily do twenty per page.
it's just a question of uh of how much uh annoyance i wanted to put uh people going to meetings through.
yeah.
well this is us.
we'll have a fair amount of annoyance.
uhhuh.
yeah.
yeah i think.
uh
so can we do we do another set?
sure.
are we are we are we sort of
sure.
we talked out about what we're
well no okay the question i still have about what we're going to do is what is what uh okay so we can get the equipment set up so that basically we're in a situation where we can with you know ten minutes of preparation we can bring nine people in here and do a multi channel recording.
okay.
right.
um
what are when are we are we going to do that?
who are they going to be?
what meetings are we going to record?
us to begin with.
so which which meetings what meetings do we have?
i think that's the intention.
i mean like us being the meeting recorder people or us being anything that speech local does?
well i think i think um certainly us meeting recorder.
yeah.
um us other groups.
it just depends on how many meetings we're we're having.
um.
jerry also said it would be he would be willing to have his group some of the a i meetings do it as well.
and we should have like we should have decoder meetings and stuff.
and uh
exactly exactly.
i think there are enough incidental meetings for us to to do this.
and if we can get it set up so it is convenient enough um then you can just do spontaneous meetings as well.
yeah i mean that's the thing.
it's okay to
right.
which i'm interested in.
i mean that's one of the things the whole reason i want a p d a as opposed to a wired room is for spontaneous meetings.
yeah.
right.
well we've got to make it really really easy to start it going then in that case.
right.
because that that's the real goal.
that's right.
that's why i put ten on here instead of thirty or forty or fifty.
right.
but i mean if we if we want jerry's group to use it then we probably won't ask them to read any numbers.
right?
right.
because
well i i bet they would be willing to do it the first few times just for the novelty.
yeah.
yeah.
i think they would.
and also he's he's he's got at least one woman.
that's great.
oh that's a good point.
i know.
and we're we have a little problem of no women.
boy that's i had never i hadn't even thought about that.
i know it's a disaster.
yeah well when when liz uh shriberg starts coming then there's another woman although she
oh man!
it's all right.
she doesn't mind as long as we don't video her.
yeah.
right?
uh that's right.
that was it.
so
maybe we should have the reading group here and force jane to come.
yeah.
yeah.
no well actually we could have jane uh jane's interested and curious about this stuff so she could attend one of
uhhuh.
yeah.
probably not the decoder meeting but some of the other stuff she would be interested enough and have her read
we could we could have the reading group in here.
yeah i mean it's a different style.
yeah.
yeah.
i mean
but i think that's all right.
yeah it's great.
yeah.
um we were joking about having lunch up here but eating with the microphones.
um i had another question briefly about transcription.
okay.
so are we going get an external service or are we going to do it?
i mean who's going to generate the transcripts?
um i think that one of the neat things about having this little pilot thing from today is we should try it a couple different ways.
uhhuh.
so we should try to do it internally.
maybe ask ask jane if we could get her to do it or or or have her supervise a linguistics student or somebody.
uhhuh.
uhhuh.
and uh see how long it takes and you know multiply out the cost.
and then send send the same thing out or maybe split it in halves or something something like that to a to an outside service.
uhhuh.
and uh see what that costs and how much hassle that is.
right.
and then we can assess it.
yeah the the problem of course is that it's you know in in this meeting it's three four five six seven eight it's nine transcripts potentially.
well except that you wouldn't transcribe them all individually.
no.
right?
you would send i mean uh a transcriber is used i mean i don't know assume presumably a transcriber transcribes a single meeting.
no.
right?
and so they they just have to indicate that the speaker changes.
right.
this was this was the point of having near zero skew.
right?
right?
so you just pick one with the best sounding one and give it to somebody and they transcribe it.
oh you're right yeah yeah yeah.
yeah.
right.
so the only thing we'll have to be careful with is that they're
well what are they listening to?
so so they they're going to they're going to have to make speaker assignments or something like this.
they're going to have to make speaker assignments.
and and which recording are they going to listen to?
for one of one of.
or we could mix we could mix the the headset mikes you know just to make a single high high quality.
right.
and we could do and we could do all kinds of weird things like having you know squelch so that you only you know you only you cut it out when there's no energy.
oh that would be interesting.
so you don't noise.
uhhuh.
yeah you could do fancy things but it seems like that's enough.
yeah.
yeah.
i mean there's it's if we if we do something which is have a little bit of human input to get the words then we can leverage that a lot by some clever cross thing to figure out which channel they really belong to and stuff like this probably.
okay.
uhhuh.
yet
so that i mean that's one chunk of work and just segmenting for this is going to be another chunk of work.
um
yeah.
and yeah.
as much work as you want.
yeah but uh i don't know.
this meeting so say it's maybe going to be a half hour or something.
and and uh we've heard estimates ranging between ten and twenty times uh real time to do this kind of transcription.
yeah.
um so you know in in principle then it'll take somebody a day for somebody to deal with this.
uhhuh.
wow!
yeah.
and uh we'll see which one of those seems closest to right.
and if they ramped up if towards the end it was faster you know get some sort of sense from them uh from whoever does it.
uhhuh.
and um then uh like i say check it out with an external service and see if it's faster cheaper better worse whatever.
right.
then we'll go from there.
uhhuh.
yeah one of the things i was thinking with transcript is that if we take the near field and we do an initial recognition pass is it easier for a transcriber to correct a transcript if the near field mike does a reasonably good job?
huh.
i bet it isn't unless it's really close to right.
yeah.
i have just because it's such an
the transcribing is sort of something i can imagine doing whereas having this distraction of having sort of something that looks like it might be right but has got some very wrong bits in is sort of a less less easy thing to do.
right.
some errors yeah.
i don't know.
i i don't know the answer to that.
i'm sure the professional transcribers obviously don't have any use for
i don't know.
yeah yeah that's true.
well i mean there's this cyber transcriber service.
right?
that's true.
and at least supposedly the way they work is by feeding them what is the output of the recognizer.
yeah by fixing up transcripts.
so maybe it depends on what the software tools are like.
yeah but yeah.
if it's you know if they can easily flip through stuff and get to the right thing
or do you point?
or i don't know.
there's supposed to be some out out out of work shepherders or something in scotland but
yeah.
uh the internet.
they should all be in india right now.
that's where the good but the cheap english speakers are.
yeah.
not in scotland.
inexpensive.
i'm sorry yeah.
not in scotland.
well they don't speak such clear english.
they they sheep they they they speak cheap english.
right?
okay so we're degenerating.
right?
are are are we done with uh m r business?
yep.
sure.
or
i mean what what's maybe we should what are our action items?
so there's there's the continuing process of cleaning up the stuff and making it easier to do that.
there's taking the results of this meeting and putting it in some kind of decent form.
probably getting some kind of single channel merged sum data or something so that you can give a simple sound file of some sort to a transcriber to deal with it rather than this massive thing.
uhhuh.
uhhuh.
uhhuh.
that's a little bit of stuff to do.
right.
right?
and then do some segmenting and recognition initial recognition would be interesting to do.
yeah although it it it it may be separating out these numbers from the rest.
that's what i mean.
yeah.
and then
yeah just doing a digits on it uh connected digits.
yeah and uh
right.
so with this next
you want to read?
sure.
okay.
how many more you got?
oh i i did ten but i can i have a script to generate them automatically.
well let's do some more while we got them here.
wow!
okay.
what fun it is.
well you know it's not but
it's interesting that um it it's not clear whether it's better to have them written as as words or or figures.
but of course to get zero and o s distinguished we you have to do it.
that's why.
yep.
but it's kind of weird to read.
what are we doing?
i since i've been gone all week i didn't send out a reminder for an agenda.
so
yeah and i'm just
do we have anything to talk about or should we just read digits and go?
i wouldn't mind hearing how the conference was.
what conference?
uh i had one question about
yeah really.
it's all a blur?
aren't the u w folks coming this weekend?
yep.
no the next.
right?
next weekend week from
next weekend.
that is right.
sorry not not not the days coming up but
the next weekend.
it's like the
a week from saturday.
yeah.
within ten days.
that's when they're coming.
that's correct.
so are we do we have like an agenda or anything that we should be
no but that would be a good idea.
okay.
so so the deal is that i can um uh i can be available after uh like ten thirty or something.
why don't we
i don't know how how early you wanted to
they're not even going to be here until eleven or so.
oh okay.
that's good.
so
wait this is on on sunday?
because they're flying up that day.
saturday.
or saturday?
saturday.
saturday.
okay.
well
saturday.
uhhuh.
yeah.
eurospeech is due on friday.
and then i'm going down to san uh san jose friday night.
so if you know if we start nice and late saturday that's a good thing.
no i mean they're flying up from from
seattle.
down from seattle.
they're flying from somewhere to somewhere.
yeah.
and they'll end up here.
so and also brian kingsbury is actually flying from uh the east coast on that that morning.
excellent.
so i i will be i mean he's taking a very early flight.
oh.
and we do have the time work difference running the right way.
but i still think that there's no way we could start before eleven.
it might end up really being twelve.
so when we get closer we'll find people's plane schedules and let everybody know.
uh so that's good.
but uh yeah maybe an agenda or at least some things to talk about would be a good idea.
well we can start gathering those those ideas.
but then we we should firm it up by next next thursday's meeting.
will we have time to um to prepare something that we in the format we were planning for the i b m transcribers by then?
or
oh yeah.
absolutely.
okay.
so have you heard back from brian about that?
chuck.
yes.
um
he's i i'm sorry i should have forwarded that along.
uh oh i i think i mentioned at the last meeting.
he said that um he talked to them.
and it was fine with the beeps.
they would be that's easy for them to do.
great.
okay.
so uh oh though thilo isn't here.
um but uh
i i have the program to insert the beeps.
what i don't have is something to parse the output of the channelized transcripts to find out where to put the beeps.
but that should be really easy to do.
so do we have a meeting that that's been done with?
that we've tightened it up to the point where we can actually give it to i b m and have them try it out.
he's he's
he generated um a channel wise presegmented version of a meeting.
but it was robustness rather than e d u so i guess depends on whether we're willing to use robustness.
uhhuh.
well for this experiment i think we can use pretty much anything.
okay.
this experiment of just
well we had we had talked about doing maybe e d u as a good choice though.
well whatever we have.
well we've talked about that as being the next ones we wanted to transcribe.
right.
but for the purpose of sending him a sample one to
okay.
yeah maybe it doesn't matter.
great.
i i don't think it
i'll i'll i'll um get make that available.
okay and has it been corrected?
oh well wait.
um
hand checked?
because that was one of the processes we were talking about as well.
right so we need to run thilo's thing on it.
that's right.
and then we go in and adjust the boundaries.
yeah that's right.
yeah we haven't done that.
and time how long it takes.
i i could set someone on that tomorrow.
right.
okay.
and we probably don't have to do necessarily a whole meeting for that if we just want to send them a sample to try.
i think they're coming
okay.
what would be a good number of minutes?
i don't know.
maybe we can figure out how long it'll take to to do
um
i don't know.
it seems to me we probably should go ahead and do a whole meeting.
because we'll have to transcribe the whole meeting anyway sometime.
yes.
except that if they had if there was a choice between having fifteen minutes that was fully the way you wanted it and having a whole meeting that didn't get at what you wanted for them
it's just dependent of how much
like i i mean i guess if we have to do it again anyway.
but uh
yeah.
i guess the only thing i'm not sure about is um
how quickly can the transcribers scan over and fix the boundaries?
uhhuh.
and
i mean is it pretty easy?
i think it's going to be one or two times real time at
wow excuse me two or more times real time!
right?
because they have to at least listen to it.
can we pipeline it so that say there's uh the transcriber gets done with a quarter of the meeting and then we you run it through this other other stuff?
well the other stuff is i b m.
uh
i'm just thinking that from a data keeping track of the data point of view it may be best to send them whole meetings at a time and not try to send them bits and pieces.
okay so
oh that's right.
so the first thing is the automatic thing.
right.
and then it's then it's then it's the transcribers tightening stuff up.
and then it's i b m.
uhhuh.
uhhuh uhhuh.
right.
okay so you might as well run the automatic thing over the entire meeting.
and then and then uh you would give i b m whatever was fixed.
and have them fix it over the entire meeting too.
right.
well yeah but start from the beginning and go to the end.
right?
so if they were only half way through then that's what you'd give i b m.
okay.
right?
as of what point?
i mean
the i guess the question on my mind is do we wait for the transcribers to adjust the marks for the whole meeting before we give anything to i b m?
or do we go ahead and send them a sample?
why wouldn't we
let their
if they were going sequentially through it why wouldn't we give them
i mean are we trying to get something done by the time brian comes?
well i i i mean i don't know.
that was the question though.
so if we if we were then it seems like giving them something whatever they had gotten up to would be better than nothing.
yeah.
uh that i agree.
i agree.
well i don't think
i mean they they typically work for what four hours something like that?
huh i huh.
i think they should be able to get through a whole meeting in one sitting.
i would think.
unless it's a lot harder than we think it is.
which it could be certainly.
if it's got like for speakers then i guess i mean if
or seven or eight.
we're just doing the individual channels.
right?
individual channels.
yeah.
so it's going to be depending on the number of people in the meeting
um
i guess there is this issue of you know if if the segmenter thought there was no speech on on a particular stretch on a particular channel.
well
and there really was then if it didn't show up in a mixed signal to verify then it might be overlooked.
so i mean the question is should should a transcriber listen to the entire thing or can it can it be based on the mixed signal?
and i uh so far as i'm concerned it's fine to base it on the mixed signal at this point.
and
that's what it seems to me too in that if they need to just like in the other cases they can listen to the individual if they need to.
and that cuts down the time.
yeah.
but they don't have to for most of it.
yeah that's good.
so yeah.
i don't see how that will work though.
good good good.
what what aspect?
so you're talking about tightening up time boundaries.
yeah.
so they have the normal channeltrans interface where they have each individual speaker has their own line.
so how do you
yeah.
but you're listening to the mixed signal and you're tightening the boundaries.
correcting the boundaries.
you shouldn't have to tighten them too much because thilo's program does that.
should be pretty good.
yeah.
except for it doesn't do well on short things remember.
right so so you'll have to i
it will miss them.
it will miss most of the really short things.
uhhuh.
like that.
but those would be those would be
uhhuh.
uhhuh.
it will it will miss
yeah you have to say uhhuh more slowly to to get
sorry.
i'll work on that.
no i'm i'm actually serious.
so it will miss stuff like that which
well so so that's something that the transcribers will have to have to do.
i
yeah but presumably most of those they should be able to hear from the mixed signal unless they're embedded in the heavy overlap section.
when in which case they'd be listening to the channels anyway.
right.
that's that's what i'm i'm concerned about the part.
and that's what i'm not sure about.
yeah.
i am too.
and i think it's an empirical question.
can't we uh couldn't we just have um
i don't know maybe this just doesn't fit with the software.
but i guess if i didn't know anything about transcriber and i was going to make something to let them adjust boundaries i would just show them one channel at a time with the marks and let them
oh they can
well but then they have to do
but then they for this meeting they would have to do seven times real time and it would probably be more than that.
yeah that's it.
yeah.
right?
because they'd have to at least listen to each channel all the way through.
but but it's very quick.
and if
uhhuh.
right?
i mean you scan i mean if you have a display of the waveform.
oh you're talking about visually.
yeah.
i just don't think
well the other problem is the breaths.
because you also see the breaths on the waveform.
i've i've looked at the uh i've tried to do that with a single channel.
and and you do see all sorts of other stuff besides just the voice.
uhhuh.
yeah and i i think that they're going much more on acoustics than they are on visuals.
well that that i'm not sure.
so
what you the digital what the digital task that you had your interface?
i know for a fact that one of those she could really well she could judge what what the number was based on the on the waveform.
yeah that's actually true.
yeah you're right.
you're absolutely right.
yeah i found the same thing that when i was scanning through the wave form i could see when someone started to read digits just by the shapes.
yeah she could tell which one was seven.
um maybe.
yeah.
so i don't
but
i'm i'm now entirely confused about what they do.
so they're they're looking at a mixed signal?
or they're looking
what what are they looking at visually?
well they have a choice.
they could choose any signal to look at.
i've tried but usually they look at the mixed.
but i've i've tried looking at the single signal and and in order to judge when it when it was speech and when it wasn't.
oh.
but the problem is then you have breaths which which show up on the signal.
but the procedure that you're imagining i mean people vary from this is that they have the mixed signal wave form in front of them.
yes.
yes.
and they have multiple
uh well let's see there isn't we don't have transcription yet.
so but there's markers of some sort that have been happening automatically.
right.
yes.
and those show up on the mixed signal?
there's a clicks?
the
oh!
they show up on the separate ribbons.
right.
so you have a separate ribbon for each channel.
there're separate ribbons.
and and it'll be because it's being segmented as channel at a time with his with thilo's new procedure.
then you don't have the correspondence of the times across the bins uh across the ribbons.
uh you could have
and is there a line moving across the waveform as it goes?
yes.
yes.
okay so the way you're imaging is they kind of play it.
and they see oh this happened then this happened then and if it's about right they just sort of let it slide.
right.
yeah.
right.
and if it if it there's a question on something they stop and maybe look at the individual wave form.
right.
oh well not not look.
well they wouldn't look at it at this point.
they would just listen.
they they might look at it.
well the problem is that the the interface doesn't really allow you to switch visuals.
right?
not very quickly.
the problem is that that the tcl t.k. interface with the visuals it's very slow to load waveforms.
you can but it takes time.
that's it.
uhhuh.
and so when i tried that that was the first thing i tried when i first started it.
oh oh.
visually.
right?
you can you can switch quickly between the audio.
but you just can't get the visual display to show quickly.
so you have to
it takes i don't know three four minutes to
well i mean it takes it takes long enough
yeah it's very slow to do that.
it takes long enough because it has to reload the
i i don't know exactly what it's doing frankly.
because but it it takes long enough that it's just not a practical alternative.
that
well it it does some sort of shape pre computation so that it can then scroll it quickly.
but you can cancel that.
yeah.
yeah.
but then you can't change the resolution or scroll quickly.
oh really?
so
huh!
now you could set up multiple windows each one with a different signal showing and then look between the windows.
maybe that's the solution.
i mean we we could do different interfaces.
what if you preload them all?
right?
i mean so so we could use like x waves instead of transcriber.
yeah.
and it loads faster certainly.
what if you were to preload all the channels or or initially?
like doesn't
well that's what i tried originally.
so i i actually before uh dave gelbart did this i did an interface which showed each waveform and a ribbon for each waveform.
uhhuh.
but the problem with it is even with just three waveforms it was just painfully slow to scroll.
oh okay.
so you just scroll a screen and it would you know go kur chunk!
uhhuh.
and so it just was not doable with the current interface.
you know i am thinking if we have a meeting with only four speakers.
and you know you could fire up a transcriber interface for you know in different windows multiple ones one for each channel.
and it's sort of a a hack but i mean it would be one way of seeing the visual form.
i think that if we decide that we need that they need to see the visuals we need to change the interface so that they can do that.
yeah.
yeah.
that's actually what i thought of loading the chopped up waveforms i mean you know that that would make it faster.
so
huh.
but isn't
the chopped up waveforms.
the problem is if if anything's cut off you can't expand it from the chopped up.
so
isn't that
right.
right but if you
and wouldn't that be the same as the mixed signal?
at some point
no i mean the individual channels that were chopped up that
it'd be nice to be able to go back and forth between those short segments.
uhhuh.
because you don't really like nine tenths of the time you're throwing most of them out.
but what you need are that particular channel or that particular location.
yeah.
and
um might be nice because we save those out already um to be able to do that.
yeah.
but it won't work for i b m of course.
it only works here because they're not saving out the individual channels.
well i i do think that this this will be a doable procedure.
yeah.
and have them starting with mixed.
okay.
and um then when they get into overlaps just have them systematically check all the channels to be sure that there isn't something hidden from from audio view.
yeah.
yeah hopefully i mean
the mixed signal the overlaps are pretty audible because it is volume equalized.
so i think they should be able to hear.
the only problem is is you know counting how many and if they're really correct or not.
so i don't know.
i don't know that you can locate them very well from the mixed signal.
right but but once once you know that they happen you can at least listen to the close talking.
but you would know that they were there and then you would switch.
right.
and then you would switch into the other
so
but right now to do this limitation the switching is going to be switching of the audio is what she's saying.
right.
yeah.
so
right.
so so.
did dave did dave do that change where you can actually just click rather than having to go up to the menu to listen to the individual channels?
so they're using their ears to do these markings anyway.
yes.
yeah.
yes.
click
um
i had suggested it before.
i just don't know whether he did it or not.
i'm not sure what click what click on the ribbon?
yeah you can get that.
yeah.
oh oh!
get you can get the uh you can get it to switch audio?
yeah.
uh not last i tried.
but um maybe he's changed it again.
we should get him to do that.
because uh i think that would be much much faster than going to the menu.
i disagree.
there's a reason i disagree and that is that uh you it's very good to have a dissociation between the visual and the audio.
there're times when i want to hear the mixed signal.
but i want to transcribe on the single channel.
so right now
then maybe just buttons down at the bottom next to it.
maybe i just don't i don't see that it's a
just something so that it's not in the menu option so that you can do it much faster.
well i mean that's the i i think that might be a personal style thing.
i find it really convenient the way the way it's set up right now.
well it just seems to me that if you want to quickly well was that jane no was that chuck no was that morgan right now you have to go up to the menu.
and each time go up to the menu select it listen to that channel then click below.
and then go back to the menu select the next one and then click below.
that's fine.
yeah it's true.
so you can definitely streamline that with the with the interface.
yeah it could be faster.
but you know i mean in the ideal world
what?
yeah.
no i i agree that'd be nice.
okay.
yeah.
okay.
so um we're done with that?
then today
i forget.
does anybody uh working on any any eurospeech submission related to this?
i would like to try to do something on digits.
but i just don't know if we have time.
i mean it's due next friday.
so we have to do the experiments and write the paper.
so i'm going to try.
but uh
we'll just have to see.
so actually i want to get together with both andreas and uh uh stephane with their respective systems.
yeah.
yeah there was that
that's right.
we had that one conversation about uh what what what did it mean for uh one of those speakers to be pathological.
right.
was it a
and i haven't had chance to sit down and listen.
oh i haven't i haven't listened to them either.
i was going to do that this afternoon.
but there must be something wrong.
i mean
unless our
well morgan and i were were having a debate about that.
whereas i think it's probably something pathologic.
and actually stephane's results i think confirm that.
he he did the aurora system.
also got very lousy average error like fifteen or or uh fifteen to twenty percent average.
but then he ran it just on the lapel and got about five or six percent word error.
so that that means to me that somewhere in the other recordings there are some pathological cases.
but you know we that may not be true.
it may be just some of the segments they're just doing a lousy job on.
so i'll i'll listen to it and find out since you'd actually split it up by segment.
right.
so i can actually listen to it.
yeah.
did you run the andreas the s r i recognizer on the digits?
yeah.
oh i thought he had sent that around to everyone.
did you just sent that to me?
no i i didn't.
oh.
since i considered those preliminary i didn't.
but yeah if you take
it wasn't
it was bimodal.
so if you
yeah it's actually um it uh it was trimodal actually.
oh was it trimodal ? okay.
trimodal.
yeah.
so
there's zero a little bit and a lot.
there were there was there was one one bump at around zero which were the native speakers.
yeah.
the non pathological native speakers.
yeah.
zero percent error?
yeah.
then there was another bump at um oh like fifteen or something.
oh was it fifteen?
this is error you're talking about?
yeah.
yeah.
yeah.
okay.
those were the non natives.
and then there was another distinct bump at like a hundred which must have been some problem.
oh wow!
i can't imagine that.
oh okay.
what is what do you mean by pathological?
i'm sorry i don't
just just something really wrong with
oh.
in the recording.
a bug is what i mean.
so that it's like
oh okay.
i see.
and there was this one meeting i forget which one it was where like uh six out of the eight channels were all like had a hundred percent error.
which probably means like there was a the recording interface crashed.
right.
or there was a short you know someone was jiggling with a cord.
but
or uh i extracted it incorrectly.
but
it was labeled.
uhhuh.
it was transcribed incorrectly.
something really bad happened.
okay.
and i just haven't listened to it yet to find out what it was.
so if i excluded the pathological ones by definition those that had like over ninety five percent error rate and the non natives then the average error rate was like one point four or something.
if what we're calling
oh!
huh.
which which seemed reasonable given that you know the models weren't tuned for for it.
oh.
yeah.
and the grammar wasn't tuned either.
it was just a
and it didn't matter whether it was the lapel or whether it was the
i haven't split it up that way.
but it would be
but there's no overlap during the digit readings so it shouldn't really matter.
right.
right.
yeah.
so it should
no but there's a little difference.
there's a lot.
and we haven't looked at it for digits.
yeah.
right?
and so
yeah so i was curious about that.
because because what he was what i was saying when i looked at those things is it it i was almost going to call it quadrimodal because because there was a whole lot of cases where it was zero percent.
uhhuh.
they just plain got it all right.
yeah.
and then there and then there was another bunch that were a couple percent or something.
but if you if you actually histogrammed it and it was a nice uh you know it it was zero was the most of them.
yeah.
a normal.
yeah.
but then there were the others were sort of decaying from there.
yeah.
yeah.
and then there was the bump for the non natives and then the pathological ones.
i see.
so
i see.
yeah because some of our non natives are pretty non native.
so
yeah.
you did you have uh something in the report about uh about uh uh forced alignment?
have you have you started on that?
oh well yeah so i've been struggling with the forced alignments.
um so the scheme that i drew on the board last time where we tried to um allow reject models for the speech from other speakers.
um most of the time it doesn't work very well.
so um and the i haven't done i mean the only way to check this right now was for me to actually load these into x waves and you know plus the alignments and play them and see where the
huh.
and it looks and so i looked at all of the utterances from you chuck in that one conversation.
i don't know which you probably know which one i mean it's where you were on the lapel and morgan was sitting next to you.
and we can hear everything morgan says.
huh.
but and and some of what you i mean you also appear quite a bit in that cross talk.
so i actually went through all of those.
there were i think fifty five segments um in in x waves and and sort of did a crude check.
and more often than not it it gets it wrong.
so there's either the beginning mostly the beginning word where you um you know chuck talks somewhere into the segment.
but the first um word of what he says often i.
but it's very reduced i.
that's just aligned to the beginning of someone else's speech uh in that segment.
which is cross talk.
so um i'm still tinkering with it.
but it might well be that we can't get clean alignments out of this out of those uh channels.
so
unless maybe we do this uh um cancellation business?
right but that's i mean that was our plan.
yeah right.
but it's clear from dan that this is not something you can do in a short amount of time.
oh the short amount of time thing right.
so
so we you know we had spent a lot of time um writing up the h l t paper.
and we wanted to use that uh kind of analysis.
yeah.
but the h l t paper has you know it's a very crude measure of overlap.
it's not really something you could scientifically say is overlap.
it's just whether or not the um the segments that were all synchronized whether there was some overlap somewhere.
high correlation.
and you know that pointed out some differences.
so he thought well if we can do something quick and dirty because dan said the cross cancellation it's not straight forward.
if it were straight forward then we would try it.
but
so it's sort of good to hear that it was not straight forward thinking if we can get decent forced alignments then at least we can do sort of a overall report of what happens with actual overlap in time.
but um
i didn't think that his message said it wasn't straight forward.
well if we'd just
well
uhhuh.
i thought he's just saying you have to look over a longer time window when you do it.
the but there are some issues of this timing um in the recordings.
yeah.
and
right.
so you just have to look over longer time when you're trying to align the things.
you can't you can't just look
well are you talking about the fact that the recording software doesn't do time synchronous?
is that what you're referring to?
sure.
that seems to me you can do that over the entire file and get a very accurate.
i don't i i don't think that was the issue.
i
yeah that was sort of a side issue.
the issue was that you have to you have have you first have to have a pretty good speech detection on the individual channels.
i didn't think so either.
and it's dynamic so i guess it was more dynamic than some simple models would be able to
so so there are some things available.
and i don't know too much about this area.
where if people aren't moving around much than you could apply them.
and it should work pretty well if you took care of this recording time difference.
right which should be pretty straight forward.
which at least is well defined.
and
yeah.
um but then if you add the dynamic aspect of adapting distances then it wasn't
i guess it just wasn't something that he could do quickly and not in time for us to be able to do something by two weeks from now.
so
well less than a week.
so um so i don't know what we can do if anything that's sort of worth you know a eurospeech paper at this point.
well andreas how well did it work on the non lapel stuff?
yeah that's what i was going to say.
i haven't checked those yet.
it's very tedious to check these.
huh.
um we would really need ideally a transcriber to time mark the you know the at least the beginning and ends of contiguous speech.
um and you know then with the time marks you can do an automatic comparison of your of your forced alignments.
because really the the at least in terms of how we were going to use this in our system was to get an ideal an idea uh for each channel about the start and end boundaries.
oh m n c m.
alignments.
uhhuh.
we don't really care about like intermediate word boundaries.
no that's how i've been looking at it.
so
i mean i don't care that the individual words are aligned correctly.
right.
yeah.
yeah.
but you don't want to uh infer from the alignment that someone spoke who didn't.
right exactly.
so so
so that's why i was wondering if it
i mean maybe if it doesn't work for lapel stuff we can just not use that.
yeah.
i haven't i just haven't had the time to um do the same procedure on one of the
and
so i would need a i would need a channel that has a speaker whose who has a lot of overlap but you know is a non lapel mike.
and um where preferably also there's someone sitting next to them who talks a lot.
huh.
so i
so a meeting with me in it.
maybe someone can help me find a good candidate and then i would be willing to
we you know what?
maybe the best way to find that would be to look through these.
you know
because you can see the seat numbers.
and then you can see what type of mike they were using.
and so we just look for you know somebody sitting next to adam at one of the meetings.
actually we can tell from the data that we have.
from the insertions maybe?
from the
um yeah there's a way to tell.
it might not be a single person who's always overlapping that person but any number of people.
right.
and um if you align the two hypothesis files across the channels you know just word alignment you'd be able to find that.
so so i guess that's sort of a last
there're sort of a few things we could do.
one is just do like non lapels if we can get good enough alignments.
another one was to try to get
somehow align thilo's energy segmentations with what we have.
but then you have the problem of not knowing where the words are because these meetings were done before that segmentation.
but maybe there's something that could be done.
what what is why do you need the um the forced alignment for the h l t i mean for the eurospeech paper?
well i guess i i wanted to just do something not on recognition experiments.
because that's way too early but to be able to report you know actual numbers.
like if we if we had hand transcribed good alignments or hand checked alignments then we could do this paper.
it's not that we need it to be automatic.
but without knowing where the real words are in time
so it was to get it was to get more data and better to to squeeze the boundaries in.
to to know what an overlap really if it's really an overlap or if it's just a a a segment correlated with an overlap.
uh okay.
yeah.
and i guess that's the difference to me between like a real paper and a sort of promissory paper.
so um if we
it might be possible to take thilo's output
and like if you have um like right now these meetings are all.
ugh!
i forgot the digital camera again.
um
every meeting.
you know they're time aligned.
so if these are two different channels and somebody's talking here and somebody else is talking here just that word.
uhhuh.
if thilo can tell us that there're boundaries here we should be able to figure that out.
because the only thing transcribed in this channel is this word.
but um you know if there are things
two words.
yeah if you have two and they're at the edges it's like here and here.
and there's speech here then it doesn't really help you.
so um
thilo's won't put down two separate marks in that case?
well it it would but um we don't know exactly where the words are because the transcriber gave us two words in this time bin.
thilo's will but
and we don't really know
well it's a merging problem.
i mean.
yeah it's
if you had a if you had a if you had a script which would
i've thought about this.
i mean if you have any ideas.
um and i've discussed i've discussed it with thilo.
i would
um the i mean i i in principle i could imagine writing a script which would approximate it to some degree.
but there is this problem of slippage.
well maybe maybe that will get enough of the cases to be useful.
yeah.
right.
i mean that that would be really helpful.
that was sort of another possibility.
you know because it seemed like most of the cases are in fact the single word sorts.
or at least a single phrase.
huh.
well they they can be stretched.
in most of the bins.
i wouldn't make that generalization.
yeah.
because sometimes people will say and then i and there's a long pause.
and finish the sentence.
and and sometimes it looks coherent and and the
i mean it's it's not a simple problem.
but it's really
and then it's coupled with the problem that sometimes you know with with a fricative you might get the beginning of the word cut off.
and so it's coupled with the problem that thilo's isn't perfect either.
i mean we've it's like you have a merging problem plus so merging plus this problem of uh not
right.
huh!
if the speech-nonspeech were perfect to begin with the detector that would already be an improvement.
but that's impossible you know that's too much to ask.
right.
yes.
and so and you know i mean it's
i think that there always there would have to be some hand tweaking.
but it's possible that a script could be written to merge those two types of things.
i've i've discussed it with thilo and i mean
in terms of not him doing it.
but we we discussed some of the parameters of that.
and how hard it would be to in principle to write something that would do that.
i mean i guess in the future it won't be as much as an issue if transcribers are using the tightened boundaries to start with.
then we have a good idea of where the forced alignment is constrained to.
well it's just you know a matter of we had the revolution we had the revolution of improved uh interface um one month too late.
oh.
so i'm i don't know if this
tools.
but it's like you know it's wonderful to have the revolution.
oh it's it's a
so it's just a matter of of you know from now on we'll be able to have things channelized to begin with.
yeah.
right.
and we'll just have to see how hard that is.
yeah.
that's right.
so so whether the corrections take too much time.
that's right.
i was just thinking about the fact that if thilo's missed these short segments that might be quite time consuming for them to insert them.
yeah.
good point.
but he he also can adjust this minimum time duration constraint and then what you get is noises mostly.
yeah.
spurious.
but that might be okay.
it might be easier to delete something that's wrong than to insert something that's missing.
right.
and you can also see in the waveform
what do you think jane?
yeah.
if you can feel confident that what the yeah that there's actually something
yeah.
yeah.
because then then you just delete it and you don't have to pick a time.
that you're not going to miss something.
yeah.
i think it's
well the problem is i you know i i it's a it's a really good question.
and i really find it a pain in the neck to delete things.
because you have to get the mouse up there on the on the text line.
and and otherwise you just use an arrow to get down.
i mean it depends on how
i mean there's so many extra things that would make it one of them harder than the other or or vice versa.
it's not a simple question.
but you know i mean in principle like you know if one of them is easier then to bias it towards whichever one's easier.
yeah i guess the semantics aren't clear when you delete a segment.
right?
because you would say you would have to determine what the surroundings were.
you could just say it's a noise though and write you know a post processor will just
all you have to do is just
if it's really a noise.
or it'll just say it's just put x you know like not speech or something.
and then you can get
i think it's easier to add than delete frankly.
yeah or
because you have to uh maneuver around on the on both windows then.
to add or to delete?
to delete.
okay.
anyways so i i guess
that maybe that's an interface issue that might be addressable.
but i think it's the semantics that are that are questionable to me.
it's possible.
that you delete something
so let's say someone is talking to here and then you have a little segment here.
well is that part of the speech?
is it part of the non speech?
i mean what do you embed it in?
there's something nice though about keeping and this is probably another discussion keeping the stuff that thilo's detector detected as possible speech and just marking it as not speech than deleting it.
because then when you align it then the alignment can you can put a reject model or whatever.
oh i see.
so then they could just like put
oh that's what you meant by just put an x there.
and you're consistent with the automatic system.
uh that's an interesting idea.
whereas if you delete it
so so all they so that all they would have to do is put like an x there.
yeah or some you know dummy reject
so blank for blank for silence.
s for speech.
x for something else.
whatever.
yeah.
that's actually a better way to do it.
because the the forced alignment will probably be more consistent than
well like i think there's a complication which is that that you can have speech and noise in the
i mean if it's just as easy.
but
uh you know on the same channel the same speaker.
so now sometimes you get a microphone pop.
and uh i mean there're these fuzzy hybrid cases.
and then the problem with the boundaries that have to be shifted around.
it's not a simple not a simple problem.
anyway quick question though at a high level do people think
let's just say that we're moving to this new era of like using the um pre segmented you know non synchronous conversations.
does it make sense to try to take what we have now which are the ones that you know we have recognition on which are synchronous and not time tightened and try to get something out of those for sort of purposes of illustrating the structure and the nature of the meetings or is it better to just you know forget that and
well i think we'll have to eventually.
i mean it's
and my hope was that we would be able to use the forced alignment to get it.
right.
but if we can't
that was everybody's hope.
but if we can't then maybe we just have to
and maybe we can for the non lapel but
is it worth
if we can't then we can fake it even if we're we report you know we're wrong twenty percent of the time or ten percent of the time.
well i'm thinking
are you talking about for a paper or are talking about for the corpus?
uh uh that's a good question actually.
i mean because for the corpus it would be nice if everything were
actually that's a good question.
because we'd have to completely redo those meetings and we have like ten of them now.
we wouldn't have to re do them.
we would just have to edit them.
well and also i mean i still haven't i still haven't given up on forced alignment.
no you're right.
actually
i think that when brian comes this'll be uh an interesting aspect to ask him as well
when
when brian kingsbury comes.
oh brian.
you i thought you said ryan.
and it's like who's ryan.
okay.
yeah good question.
well ryan could come.
uh no that's a good point though.
because for feature extraction like for prosody or something i mean the meetings we have now
it's a good chunk of data.
yep.
we need to get a decent
okay.
that's what my hope has been.
so we should at least try it even if we can't.
and that's what that's what you know ever since the the february meeting that i transcribed from last year forced alignment has been on the on the table as a way of cleaning them up later.
right?
on the table.
right?
and and so i'm hopeful that that's possible.
i know that there's complication in the overlap sections and with the lapel mikes.
there's
yeah.
but
i mean we might be able at the very worst we can get transcribers to correct the cases where
i mean you sort of have a good estimate where these places are because the recognition's so poor.
right?
and so you're
yeah we were never just going to go with these as the final alignments.
i agree.
yeah.
i agree.
we were always going to run them past somebody.
so we need some way to push these first chunk of meetings into a state where we get good alignments.
absolutely.
i'm probably going to spend another day or so trying to improve things by um by using um acoustic adaptation.
um the right now i'm using the unadapted models for the forced alignments.
and it's possible that you get considerably better results if you uh manage to adapt the uh phone models to the speaker and the reject model to the to to all the other speech.
um so
could you could you at the same time adapt the reject model to the speech from all the other channels?
that's what he just said.
that's what he was saying.
that's what i just said.
yeah.
oh not just the speech from that of the other people from that channel.
right.
right.
but the speech from the actual other channels.
oh oh i see.
um
i don't think so.
oh.
i don't think that would work.
no it
right?
because you'd
a lot of it's dominated by channel properties.
exactly.
but what you do want to do is take the even if it's klugey take the segments the synchronous segments the ones from the h l t paper where only that speaker was talking.
so you want to
use those for adaptation.
because if you if you use everything then you get all the cross talk in the adaptation and it's just sort of blurred.
that's a good point.
yep.
if you
and that we know i mean we have that.
and it's about roughly two thirds i mean very roughly averaged.
yeah.
that's not completely negligible.
uhhuh.
like a third of it is bad for adaptation or so.
cool.
i thought it was higher than that that's
it really it depends a lot.
this is just sort of an overall.
so
well i know what we're not turning in to eurospeech.
a redo of the h l t paper.
right.
that i don't want to do that.
yeah i'm doing that for avios.
but
yeah.
but i think we're oh morgan's talk went very well i think.
bleep.
uh bleep.
yeah really.
i think morgan's talk went very well it woke
excellent.
you know it was really a well presented and got people laughing.
some good jokes in it?
yeah.
especially the batteried meter popping up.
that was hilarious.
yeah.
right when you were talking about that.
you know that that was the battery meter saying that it was fully charged.
it's full.
yeah.
yeah.
you said speaking about energy or something.
but that was funny.
yeah.
he he he was onto the bullet points about talking about the you know the little hand held and trying to get lower power and so on.
that was very nice.
low power.
and microsoft pops up a little window saying your batteries are now fully charged.
that's great.
yeah yeah yeah.
i'm thinking about scripting that for my talk you know put put a little script in there to say your batteries are low right when i'm saying that.
yeah.
yeah.
no i mean in in your case i mean you were joking about it but i mean your case the fact that your talking about similar things at a couple of conferences it's not
these are conferences that have really different emphases.
where as h l t and and eurospeech pretty pretty pretty similar so i i i can't see really just putting in the same thing.
are too close.
yeah.
no i i don't think that paper is really
but
the h l t paper is really more of a introduction to the project paper and um
yeah.
yeah for eurospeech we want some results.
if we can get them.
well yeah it it's probably wouldn't make sense.
or some or some
but
i mean i would see eurospeech if we have some eurospeech papers these will be paper uh submissions.
these will be things that are particular things.
aspects of it that we're looking at rather than you know attempt at a global paper about it.
detail.
right right.
yeah.
overall.
i did go through one of these meetings.
i had uh one of the transcribers go through and tighten up the bins on one of the uh n s a meetings.
and then i went through afterwards and double checked it.
so that one is really very very accurate.
oh!
i i mentioned the link.
oh.
i sent you know that one?
so
the which one?
i'm sorry.
um i'm trying to remember.
those are all
i don't remember the number off hand.
it's one of the n s a's.
i sent e mail before the conference before last week.
oh okay.
that might might have been the one one of the ones that we did.
what i mean is wednesday thursday.
uhhuh.
i'm sure that that one's accurate.
i've been through it myself.
okay.
so that might actually be useful but they're all non native speakers.
so we could compare before and after.
and see
yeah.
yeah that's what i was going to say.
yeah that's the problem with the n s a speakers.
oh darn!
the problem with those they're all german.
and and and extremely hard to follow like word wise.
so
yeah.
i bet the i mean i have no idea what they're talking about.
so
i corrected it for a number of the words.
um
i'm sure that um they're they're accurate now.
uh actually i have to to go.
i mean this is tough for a language model probably.
it's
right.
but but that might be useful just for speech.
well
okay andreas is leaving leaving the building.
uhhuh.
yeah.
see ya.
see ya.
um oh before you go
i don't think we'll go much longer.
i guess it's all right for you to talk a little without the mike.
i noticed you adjusting the mike a lot.
did it not fit you well?
oh.
well i i noticed when you turned your head it would it would tilt.
maybe it wasn't just tightened enough or
maybe the yeah the thing that you have tightened a bit.
actually if if you have a larger head that mike's got to go farther away which means the the balance is going to make it want to tip down.
oh.
okay.
anyway.
yeah.
okay.
because i'm just thinking you know we were we're we've been talking about changing the mikes uh for a while.
see ya.
i was just
yeah.
and if these aren't
acoustically they seem really good.
but if they're not comfortable we have the same problems we have with these stupid things.
i think it's
this is the first time i've worn this.
i find it very comfortable.
i find it very comfortable too but uh it looked like andreas was having problems.
and i think morgan was saying it
well but i had it on i had it on this morning and it was fine.
can i see that?
oh.
oh you did wear it this morning?
yeah.
okay it's off so you can put it on.
i yeah i don't want it on.
i just i just want to um say what i think is a problem with this.
if you are wearing this over your ears and you've got it all the way out here.
then the balance is going to want to pull it this way.
yeah.
right.
where as if somebody with a smaller head has it back here
it's more balanced.
right?
so we have to
oh!
yeah.
then it then it falls back this way so it's
well what it's supposed to do is the backstrap is supposed to be under your crown.
and so that should be should be
if it's right against your head there which is what it's supposed to be that balances it.
uh.
so it doesn't slide up.
so this is supposed to be under that little protuberance.
yep right right below
if you feel the back of your head you feel a little lump.
um and so it's supposed to be right under that.
yeah.
so it's really supposed to go more like this than like this.
yes exactly.
but then isn't that going to
that that that tilts.
well i guess you can control that.
right?
in lots and lots of different ways.
so i'm not saying anything about bias towards small headsize.
about heads?
but does seem uh
it would be an advantage.
well wonder if it's if if he was wearing it over his hair instead of under his hair.
well we should we we should work on compressing the heads and
i think probably it was
yeah.
it probably just wasn't tight enough to the back of his head.
i mean so the directions do talk about bending it to your size which is not really what we want.
the other thing that would do it would be to hang a five pound weight off the back.
yeah.
right.
that's good.
what did you say?
a little
hang a five pound weight off the off the back.
we did that.
um
hang a five pound weight off the back.
we at boeing i used i was doing augmented reality so they had head mounts on.
weight.
and we we had a little jury rigged one with a welder's helmet.
counter balance.
and we had just a bag with a bunch of marbles in it as a counter balance.
or maybe this could be helpful just for evening the conversation between people.
if people those who talk a lot have to wear heavier weights or something.
yeah!
and
um
anyway.
um so uh what was i going to say?
oh yeah i was going to say uh i had these uh conversations with nist folks also while i was there.
yep.
and and uh um so they they have their their plan for a room uh with um mikes in the middle of the table and uh close mounted mikes.
and they're talking about close mounted and lapels just because they're
and arrays.
and arrays.
and the array.
yeah so they were
yep.
which is the interesting
and cameras.
and video right.
and yeah like multiple multiple video cameras covering every everybody every place in the room.
uh the yeah the the mikes in the middle the head mounted mikes the lapel mikes the array uh with
well there's some discussion of fifty nine.
fifty nine elements.
they might go down to fifty seven.
because uh there is uh some pressure from a couple people at the meeting for them to use a kemar head.
uhhuh.
i forget what kemar uh stands for.
but what it is is it's dummy head that is very specially designed.
oh that's right.
yep.
right.
and and and so what they're actually doing is they're really there's really two recording systems.
that's a great idea.
so they may not be precisely synchronous but but there's two two recording systems.
one with i think twenty four channels and one with sixty four channels.
and the sixty four channel one is for the array.
but they've got some empty channels there.
and anyway they like they're saying they may give up a couple or something if for for the kemar head if they go go with that.
right.
yeah it is a good idea.
yeah uh jonathan fiscus did say that uh they have lots of software for doing calibration for skew and offset between channels.
so
uhhuh.
and that they've found that's just not a big deal.
yeah.
so
yeah i'm not too worried about that.
i was thinking.
but they're still planning to do like fake
scenario based.
they have to do something like that.
right.
their their legal issues won't allow them to do otherwise.
right?
yeah.
but it sounded like they were pretty well thought out.
yeah that's true.
and they're they're going to be real meetings.
it's just that they're with with people who would not be meeting otherwise.
uhhuh.
uhhuh.
did did they give a talk on this or was this informal?
so
no.
no.
it's just informal.
no we just had some discussions various discussions with them.
uhhuh.
uhhuh.
yeah.
yeah i also sat and chatted with several of the nist folks.
they seemed like a good group.
what was the um the paper by um lori lamel that you mentioned?
um yeah we we should just have you have you read it but i uh we've all got these little proceedings.
huh yeah.
but um basically it was about um uh going to a new task where you have insufficient data and using using data from something else and adapting and how well that works.
uh so in in fact it was pretty related to what liz and andreas did uh except that this was not with meeting stuff it was with
right.
uh like i think they didn't they start off with broadcast news system?
their broadcast news was their acoustic models.
and then they went to
and then all the other tasks were much simpler.
yeah.
so they were command and control and that sort of thing.
t i digits was one of them and uh wall street journal.
yep.
what was their rough what was their conclusion?
yeah read wall street journal.
it works.
yeah.
well it's it's a good paper i mean.
yeah.
yeah yeah.
yeah that was one of the ones that i liked.
bring the
that it not only works in some cases it was better.
which i thought was pretty interesting.
but that's because they didn't control for parameters.
so
you know the broadcast news nets were not nets.
probably.
right.
acoustic models were a lot more complex.
did they ever try going going the other direction from simpler task to more complicated tasks?
not in that paper.
or
that might be hard.
yeah well one of the big problems with that is is often the simpler task isn't fully doesn't have all the phones in it.
and that that makes it very hard.
yeah.
uhhuh.
yeah.
but i've done the same thing.
i've been using broadcast news nets for digits.
yeah.
like for the speech proxy thing that i did.
that's what i did.
yeah.
sure.
so it works.
yeah.
yeah and they have i mean they have better adaptation than we had than that that system.
yep.
so they
um
you mean they have some.
yeah we should probably what would
actually what we should do uh i haven't said anything about this.
but probably the five of us should pick out a paper or two that that uh you know got our interest.
and we should go around the room at one of the tuesday lunch meetings and say you know what what was good about the conference.
present.
yep.
do a trip report.
yeah.
well the summarization stuff was interesting.
i mean i don't know anything about that field.
but for this proposal on meeting summarization
um i mean it's sort of a far cry.
because they weren't working with meeting type data.
but he got sort of an overview on some of the different approaches.
right.
do you remember who the groups were that we're doing?
so
well there're
a lot of different ones.
this was the last day.
i think
but i mean there's that's a huge field and probably the groups there may not be representative of the field.
uhhuh.
i i don't know exactly that everyone submits to this particular conference.
was were there folks from b b n presenting?
but
yet there was let's see this was on the last day mitre b b n and um prager.
mitre b b n i b m.
uh
maryland?
um i it was
columbia have anything?
no.
no it was
wasn't who who who did the order one?
this was wednesday morning.
the sentence ordering one was that barselou and these guys?
ugh ! i'm just so bad at that.
oh.
anyway i i it's in the program.
i should have read it to remind myself.
but that's sort of useful.
and i think like when mari and katrin and jeff are here it'd be good to figure out some kinds of things that we can start doing maybe just on the transcripts.
because we already have
yeah we do have word transcripts.
uhhuh.
you know.
so
yeah.
well i like the idea that adam had of of um maybe generating minutes based on some of these things that we have.
because it would be easy to to to do that.
just you know
right.
and and
it has to be though someone from this group because of the technical nature of the thing.
someone who actually does take notes.
um i'm very bad at note taking.
but i think what's interesting is there's all these different evaluations like just you know how do you evaluate whether the summary is good or not.
i always write down the wrong things.
i do take notes.
and that's what's was sort of interesting to me is that there's different ways to do it.
a judge.
and
yep.
was s r a one of the groups talking about summarization no?
huh huh.
no.
it was an interesting session.
and as i said i like the microsoft talk on scaling issues in uh word sense disambiguation.
one of those
yeah.
that was interesting.
yeah that was an interesting discussion.
the
uh i
it it it was the only one it was the only one that had any sort of real disagreement about.
the data issue comes up all the
so
well i didn't have as much disagreement as i would have liked.
but i didn't want to i i didn't want to get into it because uh you know it was the application was one i didn't know anything about.
yep.
uh so it just would have been you know me getting up to be argumentative.
but but uh
i mean the missing
so so what they were saying it's one of these things is you know all you need is more data sort of.
but i it that's that's dissing it uh improperly.
i mean it was a nice study.
uh they were doing this it wasn't word sense disambiguation it was
yeah yeah yeah
well it sort of was.
was it was it word sense?
but it was it was a very simple case of to versus too versus two and there their they're.
yes.
and there and their and
yeah yeah.
okay.
and that you could do better with more data i mean that's clearly statistically
right.
yeah.
and so what they did was they had these different kinds of learning machines and they had different amounts of data.
and so they did like you know eight different methods that everybody you know uh argues about about oh my my kind of learning machine is better than your kind of learning machine.
and uh they were started off with a million words that they used.
which was evidently a number that a lot of people doing that particular kind of task had been using.
so they went up being microsoft they went up to a billion.
and then they had this log scale showing a
you know and and naturally everything gets
them being beep they went off to a billion.
they
well it's a big company.
yeah i didn't i didn't mean it as a anything negative.
yeah.
but
you mean the bigger the company the more words they use for training?
well i think the reason they can do that is that they assumed that text that they get off the web like from wall street journal is correct and edit it.
so that's what they used as training data.
yeah.
it's just saying if it's in this corpus it's correct.
okay.
but i mean yes.
of course there was the kind of effect that you know one would expect that uh that you got better and better performance with more and more data.
um but the the real point was that the the different learning machines are sort of all over the place.
and and by by going up significantly in data you can have much bigger effect then by switching learning machines.
and furthermore which learning machine was on top kind of depended on where you were in this picture.
so
this was my concern about the recognizer in aurora.
uh
that
that the differences we're seeing in the front end is
yeah.
are irrelevant.
are irrelevant.
once you get a real recognizer at the back end.
yeah.
if you add more data?
or
yeah.
you know?
huh.
yeah.
could well be.
so so i mean that was that was kind of you know it's a good point.
but the problem i had with it was that the implications out of this was that uh the kind of choices you make about learning machines were therefore irrelevant.
which is not at as for as i know in in tasks i'm more familiar with is not at all true.
what what is is true is that different learning machines have different properties.
and you want to know what those properties are.
and someone else sort of implied that well we you know all the study of learning machine we still don't know what those properties are.
we don't know them perfectly but we know that some kinds use more memory and and some other kinds use more computation.
and some are are have limited kind of discrimination but are just easy to use and others are
but doesn't their conclusion just sort of you could have guessed that before they even started?
because if you assume that these learning things get better and better and better.
you would guess
as you approach there's a point where you can't get any better.
right?
you get everything right.
yeah.
it's just no
but
no but there was still a spread.
so they're all approaching.
they weren't all
they weren't converging.
they were all still spread.
it
but what i'm saying is that they have to as they all get better they have to get closer together.
but they
right right.
sure.
but they hadn't even come close to that point.
all the tasks were still improving when they hit a billion.
yeah.
but they're all going the same way.
right?
so you have to get closer.
eventually.
one would
but they didn't get closer.
oh they didn't.
they just switched position.
well
well that's getting
i mean
yeah the spread was still pretty wide that's that's true.
yep.
but but uh i think it would be intuition that this would be the case.
but uh to really see it and to have the intuition is quite different.
i mean i think somebody let's see who was talking about earlier that the effect of having a lot more data is quite different in switchboard than it is in in broadcast news.
well it's different for different tasks.
yeah.
it was liz.
yeah.
yeah.
so it depends a lot on whether you know it disambiguation is exactly the case where more data is better.
right?
you're you're you can assume similar distributions.
yeah.
but if you wanted to do disambiguation on a different type of uh test data then your training data then that extra data wouldn't generalize.
right.
so
right.
but i think one of their they they had a couple points.
uh i think one of them was that well maybe simpler algorithms and more data are is better.
less memory.
faster operation.
simpler.
right?
because their simplest most brain dead algorithm did pretty darn well.
uhhuh.
when you got gave it a lot more data.
and then also they were saying well you have access to a lot more data.
why are you sticking with a million words?
i mean their point was that this million word corpus that everyone uses is apparently ten or fifteen years old.
and everyone is still using it.
so
yeah.
but anyway i i i think it's it's just the the it's it's it's not really the conclusion they came to so much as the conclusion that some of the uh uh commenters in the crowd came up with.
but we could talk about this stuff.
i think this would be fun to do.
yeah.
right.
that uh you know this therefore is further evidence that you know more data is really all you should care about.
and that i thought was just kind of going too far the other way.
machine learning.
and and the the uh
one one person got up and made a a brief defense.
uh but it was a different kind of grounds.
it was that that uh the reason people were not using so much data before was not because they were stupid or didn't realize data was important but in fact they didn't have it available.
um but the other point to make again is that uh machine learning still does matter.
but it it matters more in some situations than in others.
and it and also there's there's not just mattering or not mattering but there's mattering in different ways.
i mean you might be in some situation where you care how much memory you're using.
right.
or you care you know what recall time is.
or you care you know and and
or you only have a million words for your some new task.
yeah.
or or uh
or done another language or
yep.
i mean you so there's papers on portability and rapid prototyping and blah blah blah.
yeah.
right.
and then there's people saying oh just add more data.
yeah.
so
and there's cost.
uhhuh.
these are like two different religions basically.
cost.
yeah.
there's just plain cost.
that's a big one.
you know?
so so these i mean the in the in the speech side the thing that always occurs to me is that if you
if you uh
one person has a system that requires ten thousand hours to train on and the other only requires a hundred.
and they both do about the same because the hundred hour one was smarter.
that's that's going to be better.
yep.
because people i mean there isn't going to be just one system that people train on.
and then that's it for the for all of time.
i mean people are going to be doing other different things and so it these these things matters matter.
yeah that's it.
so i mean this was a very provocative slide.
yeah so that's one of the slides they put up.
she put this up and it was like this is this people kept saying can i see that slide again.
yeah.
yeah.
yeah.
and then they'd make a comment and one person said a well known person said um you know before you dismiss forty five years including my work.
forty five years of research.
yeah.
yeah.
but you know the same thing has happened in computational linguistics.
right?
you look at the a c l papers coming out and now there's sort of a turn back towards okay we've learned statistic
you know we're basically getting what we expect out of some statistical methods.
and
you know there's arguments on both sides.
yep.
i think the matters is the thing that that was misleading.
so
that was a very offending.
yeah yeah.
very offending.
is that all all of them are based on all the others.
right?
just you you can't say
right.
maybe they should have said focus or something.
yeah.
i mean so
and i'm saying the same thing happened with speech recognition.
right?
for a long time people were coding linguistic rules.
and then they discovered machine learning worked better.
and now they're throwing more and more data and worrying perhaps worrying less and less about uh the exact details of the algorithms.
and and then you hit this
except when they have a eurospeech paper.
yeah.
yeah.
anyway.
anyway.
shall we read some digits?
tea is tea is uh starting.
are we going to do one at a time or should we read them all at once again?
let's do it all at once.
we let's try that again.
yes.
yeah that's good.
okay.
so and maybe we won't laugh this time also.
so remember to read the transcript number so that uh everyone knows that what it is.
and ready?
yeah.
three two one.
is it on?
okay here we go.
uh so uh so i i definitely have to leave at three thirty today.
so
for whatever that's
definitely have to go to tea.
so
yeah they're they're having birthday cake and such.
that too.
but
and so i can't miss that.
you're a may birthday.
right?
i'm a may birthday yeah.
so i have to
oops!
barbara told me.
yeah.
barbara peskin.
um so
only a couple of agenda items since no one sent me email for agenda items.
uh the first is the i b m transcripts i'd like to uh mention.
i i spoke with several of you about this.
but just to brainstorm a little bit about what we can do with it.
uhhuh.
so we got back the i b m transcript and the accuracy looks okay.
there are i mean there are areas which are clearly wrong.
and there are a lot of areas where they put question marks where the acoustics weren't very good.
and i think that's fine putting it in question marks is better than doing it wrong.
the big problem was they had the wrong number of beeps in it.
oops!
so it didn't align.
and that's exactly what i was afraid of.
and i went in and more or less by hand corrected it by loading it
i wrote a script that will convert it to the the channeltrans format looked at it in in channeltrans and found the places where it got unsynchronized.
and then either
huh
inserted ones.
and in this case only only had to remove beeps.
because that was always the error is that they put in it extraneous beeps.
huh.
but it's not easy to do.
because the whole thing gets offset.
and basically it's assigning the wrong speaker to the text and it all gets unaligned.
so it's very difficult to do.
huh.
so it took me several hours just to do that.
so this is
for one meeting.
this is one it's one hour of of of speech?
yep.
and it took you a few hours to to do it.
it took me a couple hours to write the scripts.
and then it it probably took me maybe forty five minutes just going through it.
now obviously it will be a lot faster from now on.
because now i sort of understand how it works.
but i think if we had some way uh some tool that made it easier to insert and delete beeps it would make it a lot easier.
because one of the problems is that it takes a very long time when you make a change to load it back up in the channel transcriber.
yep.
yeah that's true.
i've noticed that yeah.
one of the things that adam said that i thought would be a good idea would be um if we used different beeps.
and so there may be
which makes sense.
so for example if we alternated between two beeps a high and a low.
he said that?
uh well in in some email that uh we had.
oh on the email.
for
oh oh okay.
yeah.
i didn't just say that no.
yeah um because then
so so this is the the other approach which is what do we do in the future to try to make this less common?
yeah.
but it's still going to happen.
well and then the other possibility was that we provide them with a a file that already has the beeps in it or something.
so
huh.
wasn't that what you said?
sort of a text template.
but i don't know how we can do that.
uhhuh.
yeah we don't know what their process is.
so i had two ideas.
the first was to provide them a text template that had both the beeps in it and the speaker i ds.
yeah.
you know just male female english nonenglish one two three four.
um
can i i i just
and they just filled it in.
how many how many beeps are there and how many do were they were were off?
it was like a hundred twenty.
and they had a hundred twenty three or something like that.
all right.
is that right?
well i thought it was a lot more than that.
and is it a
well i i don't remember.
they were three off basically.
yeah and it was a hundred ish.
and this is the mixed signal?
oh!
so so if you get one
huh.
uh uh excuse me.
if you get one off it everything is off because of the alignment.
right?
yep.
yep.
uh and this is all in one file that's continuous.
yeah.
yep.
so why don't you make it multiple files?
how would that help?
because then if they have one off then then it would only screw up that file.
oh you mean just break the one hour into a few chunks.
yeah.
i mean it seems now if you just make one mistake then everything is off.
well the only the only question about that that i have is um i think brian has to put all this stuff onto a tape.
so i don't know whether
yeah.
i'm sure they would prefer it sort of one hour.
can he send them i guess he could send them multiple tapes.
oh.
uh
yeah just say okay is ten minutes is
i mean maybe there's other things to do that are more clever.
but this just seemed to be the sort of the obvious thing.
because again if you make one mistake then everything after that mistake is off.
uhhuh.
and uh
well the only other issue is we'd have to worry about reassembling them in the proper order.
uh
yeah.
that i mean just naming schemes will get
we can definitely do that.
so so one question is are there good places in the files where we can really do that?
well the other thing you can
i mean so so so someone will have to go through them and listen to them and pick a place to break them.
yeah.
the other thing you can do
this is on the mixed signal?
or are these the individual
the ones with the beeps that they're getting that they made mistakes on is the mixed one?
they are played from the individual.
individual channels.
individual channels.
or they're
if it's individual um we can probably run a forced alignment.
where you include the beeps as words and pretty much figure out where the errors are.
i don't think we could do it on the mixed signal.
because it
but on the individual channels that would probably work if they have enough words in the transcript.
yeah we in fact that's yeah maybe we
so i mean that's pretty quick to run.
it it's um
i mean we could try it.
because if a beep is treated as a unique word
you mean so actually put put the wavefile in with beep in it.
beep
put in put beep in as a word.
and and have a beep model.
we have the original wavefile with all the beeps.
yeah yeah well the beep model actually you could train a beep model.
that that's a good idea.
it would be very good.
um
yes it would be because it's very very regular.
yeah.
yeah so um
oh kind of low variance.
wouldn't that be?
yeah.
yeah very low.
yeah.
you sort of might end up with some infinities there.
um that might actually
you have to may add some jitter to it.
but
can i see that?
i mean that might actually work.
where's the case?
because you'll i guess the trick is figuring out you know where it doesn't align.
but if it's a problem of extra beeps rather than missing beeps then um that's easier.
it could it could there could be some missing ones.
and it might work.
oh.
but
so is this a
it's just in this case they were not.
seven or something?
or
what is it?
m one hundred.
so i think uh chuck had a good explanation for it which i liked.
m one hundred oh.
which is these they're listening to it and so they write something down and let's say they miss it they rewind.
well they hear the beep again.
was that that beep or was that a different beep?
oh yeah.
so that's why chuck was suggesting different tones.
my original thought was you could have a different tone for each speaker.
you know there's plenty of tone space.
and that might help cue the transcriptionist.
you know give them a little cue about where they are.
or you could you could have ascending tones or something.
what do you mean?
uh or
yeah.
increasingly high higher frequencies.
you could have um each speaker in the meeting say beep and record it.
and that will be their
no i just i don't know if you have a hundred or something just just sort of move them up up a a half tone or something for for each one.
sorry.
and then
that's what i was saying is that that you could have each speaker could have their own tone.
oh was it?
oh.
see i should listen.
yeah?
huh
and you know since there are no more than you know ten speakers or so per meeting there's plenty of tone space.
oh.
well
but
uh okay yeah so the only
that might work too.
yeah i was just thinking that actually temporally just uh uh uh an increasing tones so that
oh okay.
they would really have a sense of whether they were going you know forward or backward.
bo eep be oop!
in the end?
yeah.
well you
or
so just truly chromatically up all the way through the entire meeting.
yeah.
yeah.
well it's a thought.
interesting.
oh oh oh i see.
it'd be impossible to to misorder them.
yeah.
yeah.
but i guess you'd still get the mistake that you mentioned.
because the person
well
i mean if that's some of the errors those would probably stay the same regardless.
yep.
i mean chuck's suggestion of just two beeps is nice.
yeah maybe.
because then you could have them actually transcribe h beep or l beep.
high beep or low beep.
and then when we get them back if we see two l beeps in a row
it would be much easier.
that's probably better.
yeah you have like a sanity sanity check on them.
yeah.
scratch my other idea.
it's pretty simple.
pretty simple.
that's a good idea.
yeah.
yep.
alternating.
yeah.
and it would it seems like it would help them.
you know if they just finished a long passage and they heard one beep and then they hear it again they know it's the same beep.
versus if they heard a slightly different one.
yeah.
huh interesting.
yeah that's a good like error checking approach i think.
yeah we can
yeah like do do uh one for each one for each person in the meeting you can only hire people who have perfect pitch.
and they can say a flat b.
well well.
high low is pretty straightforward.
great idea.
yeah.
yeah.
no two is probably better than what i was suggesting.
because what i was suggesting in order to make it through a reasonable range for the whole thing you'd have to have the the tonal differences between the two kind of small.
and actually you want them large.
yep.
so
you'd have to they'd have to have the acoustics of dogs hear into the high high ranges.
uhhuh.
yeah.
so i think certainly doing two beeps is is no brainer.
yeah.
and then the other question is if they can
if they do something on a computer in a format we can handle we could give them a text file that was a template with speaker i d and beeps already in it.
and then they could just fill that in.
yeah.
maybe i should write to brian and tell him what the problem was.
so what
and what our proposed solutions are.
right.
yeah.
see what he thinks would work best with them.
i mean what's going wrong on the other end?
what are they doing?
well as i said i think we don't actually know.
what uh
but i think chuck's hypothesis was a good one.
which is you you listen to something you write down what you thought you heard but you want to listen to it again so you rewind.
and there are some
and then you hear the beep again and then you say well is that that beep.
or is that a new beep.
i don't remember.
and so you know a couple times they got it wrong.
and there are some cases where there's um very little speech before you hear a beep.
so you hear a little bit of speech beep.
yeah.
uhhuh.
a little bit of speech beep.
maybe three of those in a row.
and was that three beeps?
was that two beeps?
um see they have brian said the setup that they have is um
uh um
they've got headphones.
and then i guess they have a computer.
and they have a foot pedal.
which lets them quickly scan back and forth through the tape.
makes the tape go real fast forward or fast backward.
and so they can hear something then step on the pedal and quickly rewind and go back and hear it again.
and so they're
i think they're probably going over and over sections.
and then they get confused about whether they've already put that beep.
or if they heard you know three beeps in a row was that three or two?
and
it's a shame we can't really give a number.
okay.
so it would be a number and a beep.
it's a shame we can't do that.
well we could.
oh that's an interesting idea.
we could but then that starts getting pretty long.
no no no but wait maybe maybe what we have instead of a beep is a uh synthesized number.
yep.
and they put the number in when they hear it.
that'll work.
so replacing the beep with twenty three
twenty four
twenty five
and then they have to transcribe what that number is each time.
wow that'd also be shorter to type.
yeah just a two three two four that would be
well they would have to have a mark.
there'd be some kind of mark in front of it.
interesting.
well because they're distinguishing between twenty three
that's your indicator there and twenty three that somebody happens to say in a meeting.
oh that's true.
well it would have yeah it would have to be um you know either obviously a sounding synthesized uh little
yeah.
but they might miss that.
right?
or maybe could it be a click and the number.
yeah.
or a beep and the number.
well it's sequential.
or a beep and the number and then and then you just do the the you do it as a a digit.
right you would go sequential so unless you got pretty unlucky
uh right.
yeah.
what the person was saying and the number
in fact we could put it at the beginning of each utterance.
we could say beep one.
and then they'll hear the speech.
i think having a beep too is good.
so beep number
yeah
or a number beep
it's just getting pretty long.
you know the utterances are very short.
yep.
uhhuh.
and so you're going to be talking beep number yes beep number no.
uhhuh.
well if if it saves time
yeah and i think it would i actually i i like that.
yeah.
and when they transcribe this meeting it's going to be really impossible.
i think it would
yeah that's funny.
we're saying beep yeah for instance.
yeah yep.
yeah.
but see that would that would definitely keep
it's not a bad idea.
yeah.
keep things from getting out of sequence.
if they heard it over again they would know it would be obvious from their transcript whether they did that one or not.
it might actually help them in terms of their place holding.
that would help get them synchronized.
yeah.
and it might be helpful to them in terms of
yeah.
i think it would be.
because they would know their place.
and we have plenty of digits data.
so
yeah.
they would know their place darn it!
know
those transcriptionists need to know their place.
oh.
well no no no no.
but it sounds like a a a a short conversation with brian might be might be helpful to get a sense of what's going on.
yeah i'll i'll i'll write him an
yeah the numbers are a good idea.
yeah.
okay.
or just
other than lengthening the transcript i think it would be very helpful.
just instead of the sequential number
perhaps just the speaker i d or something.
just say speaker one or
or just one two three four for four speakers or something.
could could be helpful.
i
yeah you don't have to
you could keep them short by not or just go one through ten one through ten one through ten or one through twenty.
yeah yeah yeah yep.
and then also if we i mean i don't know what their setup is but your template idea might be a way of of of
uhhuh.
we could actually build the numbers into a template.
yep.
well
i'll talk to brian and see what he
yeah single digit numbers
that also appeals to me so that you don't have to you know
yeah.
that's a good idea.
very simple.
one hundred twenty four.
yeah.
yeah.
well although
beep one thousand three hundred forty two.
you're saying it's it's about an hour meeting.
uhhuh.
and there were a hundred and twenty three
i don't remember.
it's forty five minutes.
chuck was saying there was more than that.
huh?
forty five minutes for that one.
more beeps than that.
i was thinking there was on the order of fifteen hundred segments.
but maybe i'm
there must be.
because otherwise you're going a whole
i think it should be more.
it's possible.
yeah.
that was at least forty five minutes.
right?
yeah that's right.
was it that many?
i just don't recall.
yeah it was forty five minutes.
that's the shortest meeting we have.
most of them are double that or
yep.
well i was just wondering what the average length thing was.
so
oh of a of a segment that they're hearing?
it's
yeah.
i don't know.
huh interesting.
easy enough to figure out.
uh
i don't know.
so if there are in the in the hundreds and or thousands of these
yeah.
we have all that data.
it must be in the high hundreds of them at least.
so right so
so that's getting a little cumbersome for them to type like five hundred.
yeah.
well with just one digit though.
i think
but yeah if they recycle through
well
yeah if we if we did mod ten then
yeah.
right but my point was you know if it's a big difference to me if there's a thousand or there's a hundred.
if there's a thousand then that means that this is only happening three times out of a thousand.
which means you know even just splitting it up into a few files would probably take you know nearly all the time.
there would be no
yeah.
if it's a hundred then you know that's not the issue.
but if it's a hundred then the average length of each one is pretty pretty long.
and so having a little thing at the front of it is no big deal.
uhhuh.
so that's why i was curious.
yeah you're right.
it had to have been more than that.
i mean it was a forty five minute meeting.
so
right.
and it certainly was not a minute a chunk.
it was a few seconds a chunk.
that's what i
uhhuh.
yeah so it'd be a lot of overhead to type.
so i'm just mis remembering.
i'm just mis remembering.
oh i
so
well and the other thing is maybe the numbers would help them keep their place in the transcript and speed the process
yeah.
it might make the transcript faster.
yeah.
i mean i a quick conversation with brian would be good.
uhhuh.
yeah.
so
okay.
and other than that
actually if it is
i mean it's not a bad idea to try the alignment.
um if there's only like three of them then if you align the first point at which things get messed up is sort of the location of the first problem.
uhhuh.
and then the second
i mean you could do it that way.
or you could
it's it's
it would be able to handle any errors.
so if they make on the average that many and it costs them more time to do something different than what they're doing which i don't know but if it does then we could try doing that as a post process.
right.
and um have a student or a transcriber run this alignment.
or we can do it.
yep.
and
you know actually what all that we would
then you can iteratively figure out where the problems are.
you know liz all we would
it would take a little work.
but not any real human not a lot of human work.
right i mean that's what we want to avoid.
i mean if in terms of the alignment actually all we would need is we wouldn't even need words.
we would need a general speech model and a beep model.
because all we're really concerned about is missing beeps or too many beeps.
well the the forced alignment will run
so if we had a
otherwise it'll take forever.
i mean to really run recognition
uhhuh.
well i guess what i'm thinking of is that that a lot of times there will be they'll put in question marks that represent some unknown amount of speech.
or
right.
so we're going to have to have some kind of
yeah then we just map it to a reject model.
in and in fact that's what we do now.
because there's cases even after jane listens where you know we have
right.
an expert can't even tell what they're talking about.
yeah.
um so that's okay.
just map that to reject.
right well what i was thinking is map all the speech to the reject model.
and
and then you have a beep model.
yeah.
but that won't work.
yeah you're still talking a forced
because it's
the forced alignment might not work then.
although we can try it.
um
well but all we'll get
well with the beep models is what's really you'd have really it would be really good though.
the beep model is is going to match like really well.
right.
so
so it's so it's really just doing yeah a keyword like a spotting for beep with
right but it'll it'll grab the next beep in other words.
it'll be
you'll get back another offset.
um i mean i was thinking of.
then i and then i realized well the recognizer will just go along and line up all the beeps.
and then there'll be all the extra beeps at the end.
if there were more beeps then you wanted.
right you you need but you need the text.
so you need the word you need the word to sort of control the relative location.
to tell it where it got it wrong.
yeah.
no i'm not i'm not suggesting that we don't have anything between the beeps.
it will tell you
oh.
but i what i'm saying is what's between the beeps we don't really care about.
i think you do care about it.
because you have to know which ones match which.
unless you want to know if they're
right.
otherwise you just count up the total number of beeps.
and we can do that with wordcount.
right we already know that that we're three short.
yeah.
and so if we just did uh beep and non beep all you would get is that you have three mistakes.
but you wouldn't tell you which ones were wrong.
right.
you need some of the words in there so that you can say well this segment matches this one.
this segment matches this one.
and this one doesn't match at all.
unless we insert a beep.
i mean the only thing i'm worried about with that approach is that if we need to figure out the beep alignment problem before the transcribers do the corrections here then we're in trouble.
okay.
in other words if the transcripts aren't sort of good enough that the aligner constraints are good enough to sort of show you where the errors are then it wouldn't work.
but it might work.
it might work to do this if their transcripts are pretty close on the words that usually get recognized correctly.
which are the you know function words.
the common words.
well actually you know there's an even easier way.
we don't really need a beep model.
um just extract the segments and do a forced alignment.
and if the score is good then you say it matches.
right you can also
in fact that's what we do
right the individual segments between the beeps
um if beeps were like the segments that we get from um the transcription tool already that that's what we have done.
and it works very well.
you can see these segments align and these don't.
um then you just have to go back in and figure out where the endpoints of those segments are.
because some of them will be wrong.
because the the beeps were missing by definition.
right.
so do you get a score for each alignment?
so it might actually work.
is that what you get?
um you you get um
definitely when they don't align at all it it it fails.
i mean that that's how we found a lot of problems before with um words being on the wrong place or something.
uhhuh.
so a failed alignment is a very good indicator that that the words don't match up.
uhhuh.
i'm just wondering when what it means to fail.
um
is it the the likelihood scores below some threshold
yeah is uh it right.
it can't match up the
that's why you actually need the text.
in order to force you to try to match something that gives you a model to match against.
so
it's just an idea if if it turns out that
i mean i also like this idea of high and low beeps or
but it if suppose we get one or two errors still per you know per transcript then we we might want to try some approach like that.
well the numbered ones would make it a lot easier.
because you could then really localize where the error is.
uhhuh.
yeah.
where it is.
right.
if so if that doesn't add time for them that'd be great.
yeah.
yep.
you know like
i think it would save them time.
we have to ask them.
i think it would help them keep the track track of it.
uh uh again especially if there's hundred many hundreds or thousands i still think it would it would cut the incidence of this a lot if it was possible to break it into a couple files.
interesting.
yeah because it you know it's
yeah.
i guess also even for the transcribers it's quicker to load a smaller file.
into the you know for the checking problem.
yeah i mean we could certainly break the meetings.
we could certainly break the meetings into pieces.
into twenty minutes chunks or something.
or
so just
i mean you've already broken them into chunks.
so if you have if you have a if you have a thousand chunks then make ten things of a hundred or or you know.
just
just accumulate it till you get twenty minutes worth?
there's one file.
that's true yeah.
yep.
and then it will be you know this every now and then there'll be a a beep missing.
so someone's going to talk to brian.
yeah i'll
i think that was chuck.
i'll talk to him.
okay.
yep.
i also looked over the text.
and i was impressed by the accuracy.
it looks really good.
yeah it does look good.
i mean i found several errors.
but they were not significant.
they were all things that i could easily listen to and sometimes convince myself they said one thing and sometimes the other.
so
yeah it's there.
uhhuh.
i haven't looked at these.
um
because i was gone last week.
but but don had told me that there's a difference in some of the conventions.
oh yes but this is not important.
so but those are all easy things.
it's systematic.
right?
yeah.
yeah okay.
and and brian actually forwarded me in advance.
he he he very nicely worked out a set of conventions which is intermediate between ours and the ones that they're used to.
which is which is really a good a good way to do it.
okay.
so we can just map.
map them to
no problem.
great.
huh
okay.
um
we still haven't really sat down and talked about file reorganization.
and directory reorganization.
so we still have to do that.
but i don't think we need to do that in this meeting.
but uh it is something that needs to get done.
and i want to also coordinate it with dave so that we do a level zero backup right after.
so we don't waste a lot of tapes.
but uh
so let's let's try to do that sometime.
okay?
okay?
yeah we can do it this week.
yeah.
um
we also still have to make a decision about mike issues.
what we want to do with that.
i thought we were going to get two more of the ones
and just swap them in and out?
yeah we could certainly do that.
so why not?
so uh morgan just to
uh since you weren't at the meeting last week uh apparently a bunch of the e d u folks really hate this style mike.
um i wouldn't well it they didn't say hate.
but they they they come on time to their meetings in order to not be left the last person who has to sit by those mikes.
can i i i
i mean that's that's that's a fairly strong indication of dislike.
i actually really like them though.
um
which one is these?
there were a few people you and like three or four people who really like them.
the uh
the crown.
uhhuh.
yeah i do.
and and all the others really don't.
so what i was thinking is we could get a few more of the sony ones.
and just unplug them and plug them in.
and the only thing is that when you fill out the digit forms you have to be sure to indicate which mike was actually used.
so i mean that's easy to do.
um
it it moves us away from this uniform all the mikes are the same.
which some people had said was a benefit.
but
the other thing too is you said that the acoustic quality of the one that you're wearing the fancy one is actually somewhat better than
it is yeah.
so i don't know if it's a noticeable difference.
but it is
i mean i i personally really prefer that model.
so i think you know we've got this problem of people having different different preferences.
you know there are so many things about this that are non uniform that i think having having some modicum of uniformity is a good thing.
but you're just not going to i mean we've
okay.
uh
for instance just the fact that people sit in different positions different times.
the exactly where they put it with respect to their mouth is different each time.
yep.
uh
i i just don't
i mean i think we we're we're getting rid of major differences.
uhhuh.
like we're not using the lapel anymore and so forth.
but
the other stuff is just going to some variability we're going to have to accept.
it's i i understand.
that's fine with me.
i i would say that um i find this model easier to adjust.
that's one of the reasons i don't like these.
because i keep you know bending it.
uhhuh.
and then it's and i never really know
yeah see i i happen to like the old one better.
you do?
but
okay.
yeah.
what about just a different headband thing?
and some people do.
but
or even if we could attach two headband
i mean
like i don't mind those.
we
but it they bounce around.
we have lots of choices of microphones.
i mean can we keep the microphones and just somehow attach a more comfortable thing over your head?
uhhuh.
huh
yeah.
that's my problem with this one.
the the ear thing comes out to here.
it doesn't even fit over my ear.
right.
and for some of the others it's their ears were shaped in a way that didn't hold the
huh
right.
huh
i mean so we can go microphone shopping and get get more microphones.
it's just
then they have to be rewired though.
i'm just not sure what we should do.
yeah.
don't they?
specially.
you know i mean here's another argument against worrying too much about uniformity.
we have something like forty five hours of data collected.
and uh we're also going to be integrating in this corpus stuff that's that's being done at u w uh with entirely different microphones.
yep.
so i i think you know getting away from the lapel having close mounted mikes having people adjust them as they're most as as as comfortable as they can
well i
i think that's sort of the big deal.
i agree with that.
and so my feeling is we should get mikes that people like.
yeah.
yeah.
and so my feeling is going out and buying a few more mikes is fine.
yeah.
yeah that's right.
so should i just go do that?
yeah.
okay.
what about these
and then also should we go ahead and get another wireless system?
you know for whatever it's going to be uh
uhhuh.
uh another wireless system.
you mean another transmitter?
or or
uh no actually get we need another box.
because each each box in the back room can only take six.
oh to get to get rid of the wired things.
so we
yep.
yes absolutely.
okay.
yeah.
uh because because we've had with the exception of forgetting to change the batteries from time to time uh we've really had no no trouble with this at all.
so i'll just go i'll do that.
yep.
right?
and and the wired stuff is just going to continue to degrade as you know
and nobody will remember how to
another one just bit the dust.
yeah nobody will remember how to repair a jimbox or a jimlet or a
really?
i think there's a problem with this one.
yep.
you know it's
okay then i will just go do that.
yeah.
and send the bill to uh what's her name?
whoever will pay.
yeah.
whoever will pay.
you.
yeah yeah.
i have a quick question about microphones.
um
i got this crazy idea that um in the future people will just walk around with the microphones that they use for their cell phones.
you know these little boom ones.
like and really go to meetings with close talking mikes.
if they're their own personal microphone.
and so i'm wondering if we can get a couple of
i don't know how good quality they are.
but it would be really interesting to see if they're good enough.
do you mean the kind that hang from the ear sort of thing?
the kind that guys that like to look like they're really cool at airports wear.
or the
yeah.
yeah.
yeah but i don't know that one.
so what's that mean?
this it's it's a cell phone jack.
you can't miss it.
oh yeah you do.
you cannot miss it.
is this just this little
they're the guys going around
they're probably talking to nobody.
but
oh they they have little headsets?
yep.
yes.
or what do they
yep.
they wear the
well there you go.
oh you've you've got one.
yes!
that's that it looks sort of like that.
except that one's even bigger than most of the ones i've seen.
so where where does how do you wear that?
most of the ones are just a little boom.
what does that
you you put it over the corner of your ear.
yeah over the corner of the ear.
i can't i'm having competing microphone headphone problems here.
there's another kind which goes in your ear.
and it hangs down.
there's a little bulb
yep it just just hangs down there.
yeah that i've seen.
yeah.
that hangs has the mike on it.
okay so whatever people sort of wear to use
this one.
yeah.
uhhuh.
yeah something like with their cell phones.
yeah.
it'd be really great i think if we can argue that.
it's an interesting idea.
people like that.
the acoustics
the acoustics on this are really better than when i talk into it directly.
yeah the the biggest issue is the stupid cable.
but what
are they?
why would it be cool?
that sony
the connector that sony has that uh
oh okay.
so it's they're not compatible?
it's non standard.
and so you can't just plug something in.
you need to get it wired.
oh i'm sorry.
you're talking about that as a possible thing to plug into the sonys.
well
um just as an example for you know the future of the fact that maybe people will wear those microphones.
right.
no?
how standard are these?
works with this.
i don't know.
or some people might.
well some people will yeah.
to meetings.
you know.
not a i'm not saying there's not a far field microphone uh application.
but
if we have a i've always wondered how well they would work.
it is an interesting idea.
if we could build an adaptor that went from
because i think mine looks like this too.
if we had this style plug that went to that then we'd just have adaptors.
right.
and people would plug them in.
you could use they could use their own.
oh right so they could use the
i wonder if you could do an adaptor.
i don't know if you can or not.
anyway it's just an idea.
yeah.
um
an adaptor might be a better idea than redoing the wiring all the time.
huh
well i mean another
another side to that is that um in principle uh if it all comes together supposedly we're going to get two um i ram boards.
right.
uh little later on in the year.
and so we're going to want to be thinking in terms of some wireless connection to it.
uhhuh.
um
so we have you know some kind of little something.
and uh we have a wireless connection this board that's doing the computation for the the uh the recognition.
and so you know we want to be able to show off to somebody.
you know we have a base station somewhere.
and you're wandering around uh hands free.
and
yep.
i don't understand.
so so there's a mike that plugs in
you're saying there's a mike that plugs in to the i ram board?
no no what i'm saying is that we're going to have to put together some kind of a little superstructure some sort of demo box that it that doesn't do a whole lot.
or
demo.
but it somehow connects to uh remotely to this i ram board.
which will be doing the real computation.
oh i see.
in other words we're we're not going to attempt to build uh it do the real work of building a portable device that's little that is doing all the computation we want to do for the recognition and the far field mike uh compensation and all that stuff.
so we're going to do all that on this powerful board.
and uh have some wireless connection to it.
and then the question is what is what does it look like?
does it look like something that you you hold like this?
or do we make a big deal out of it that you hold it arm at arm's length so that you can see some display?
or uh
do we have one of these
you know i don't know.
so it these are all possibilities.
yep.
i mean what i was envisioning was a p d a with a cellular link in it.
you know with one of the short range wireless.
and let just do capture the audio on the p d a and send the audio over the wireless net.
yeah so is bluetooth the sort of thing that'd be uh
but
uh bluetooth is shorter range.
but you could use bluetooth.
whatever the network guys already have hooked up for this floor.
i don't remember what it's called.
huh
okay then uh another issue on file reorganization is uh making data available to people outside icsi.
so specifically the u w folks have been wanting to get access to it.
so i think the right thing to do for that is figure out how to do c v s without uh compromising security.
some s s h tunneled c v s.
well i was
and then give them access.
i was reading through some of the c v s documentation.
and you can just substitute c v s for your r s h command.
yes except that the command has to not take a password.
huh
because it's non interactive.
and so the only way to do that is with uh s hosts.
which is insecure.
huh
so we still have to sort of look at it a little bit.
i mean i'm sure it's been solved.
and i just haven't found the solution yet.
but i am sure that people have done it.
because it's going to be a problem a lot of people have.
but um
who well we're talking about a fairly limited group.
are are is it a small enough group that they would could just have accounts?
does that solve anything?
uh not really.
because then we have the coordination issue.
i mean so one thing they could do
i mean i guess that's true.
they could simply log in to icsi
yeah.
well they're going to have to have accounts
and do everything locally.
if they use s s h they have to have an account anyways.
yeah except they could share one.
well but that's not
the uh the s s h accounts and the user accounts don't have to be similar.
so
i mean there are lots of ways of doing it.
but but if we can't figure out a remote way of doing it just letting them log in to icsi might be okay.
uhhuh.
so the intention was to put everything except the audio files themselves under revision control.
and the audio files it's not worth doing.
because they're too big.
you're never going to be copying them around and making working copies of them.
right.
and then the other issue jane and i spoke of briefly is just general permission issues.
that right now lots of people create the files.
and then we have group meeting recorder.
but that means anyone in the meeting recorder group can overwrite the files.
and so that's a pretty coarse level of granularity.
so we might want to think about doing a meeting recorder user owner for those files.
and then doing group slightly differently.
but
i think what we have now
can you just do like s u x or something to modify
i mean if you just have one user that
another option is to make a non make it owned by a not real user.
and then root will be able to do it.
but root is pretty tightly controlled here.
uhhuh.
i had another thought.
so i don't think that's a good solution.
would would it be
there are scripts that uh run.
and and when it's finished compressing would it be possible for the script to change the permissions on the file to be more strict?
have them
yes but if you do that then someone who isn't the owner can't unchange it.
but after the meeting's done would they want to?
yeah as i said it's just a question of uh do you want to have to track down root if you have to make any change?
but it doesn't have to be root.
it could be some other user that we all can s u x to.
that's true.
you have security issues with that.
so
yeah that's true.
i mean
that's what we do with doctor speech though.
right?
no with doctor speech it's just a group.
so there're lots of different owners of of the files but the group is always doctor speech.
or real usually.
but there is a user doctor speech.
right?
i mean i i s u x to doctor speech when i need to make a a directory on uh one of the
oh do you?
oh so that's what i meant.
it just some user like meeting root or something.
oh okay.
i don't know.
i mean i'm not too worried.
because the group's pretty small.
but
yeah.
also it seems like once it's been stored that night it's it's possible to recover from backups.
so that the main risk is avoiding it being accidentally deleted during the day that it was recorded.
well and just you then also just have the general problem of permission on the other files.
that do you want people checking out a transcript file when they find an error and correcting it?
or do they do you want it to go through you?
yeah yeah.
and i'd prefer it to go through me.
okay but that means that you're going to have to be available for people all the time who say there's an error here.
rather than just them just going in and correcting it.
well i at this point
well with c v s though you don't actually have to do it that way.
yeah.
yeah.
well but you have a choice.
either you let people do it themselves or you don't.
yeah.
what i'm saying is that it can be the case that people can do it themselves and it can be reviewed.
right?
right.
i mean it's not like it's one or the other.
we can do both.
yeah you can
right so you could get email.
send email to jane that someone has done a revision.
right.
i i want to also say that you know this is a
uh uh my my view on this is that once it's in a final version and i'm working toward all these being final versions then it can go into this next this next realm.
but it it's sort of counter productive to have changes made which i would be making if i'd just gotten that far in the file.
you know it's it's um
well so you don't have to release it to the world until you're ready to.
yeah.
that's right.
so the question is
and then after that then this would probably work.
yeah.
so you would keep it in a separate file structure until you were ready to put it in the repository.
good good.
i mean
or just keep the lock on it.
is c v s like
yeah just keep the lock on.
i mean just check it out check it out.
until you're
yep.
and don't let anyone check it out
no it does not the c v s doesn't work that way.
oh.
it oh it doesn't.
there's no there's no lock when you check something out.
oh okay.
not like r c s.
in other words everybody can check out anything.
oh.
the c stands for concurrent.
so why not use r c s at that point just at that stage?
because it's not remote.
it's not remote.
the nice thing about c v s is is you can be on a different machine.
but at that point we only want probably jane to be in control anyway.
and she
i mean it's just an
no because u w wants to have access to these files.
that's the whole point.
but not to modify them.
right?
just to read them.
well if they if they record a meeting and they create a key file and they find an error in their key file they shouldn't have to tell us about that.
so that they can create correct an error in one of their files.
well the key file is
so that's why i want to do it
i don't know.
key file's different.
uh i mean
key
different than what?
well key file has the extra attributes that if you notice that there are lots of spikes on channel.
so-and-so and it's useful as a repository for putting that there.
and also if you notice that there was an extra speaker.
and didn't get mentioned in the key file.
but they were only there for a second.
but it's useful to mention that they were there.
so i i think that the key file is for notations related to the meeting.
well
but i mean the meetings are going to be the same way.
but it's different from the transcript.
so u w records a meeting.
we transcribe it.
yeah.
a few months later they listen to it.
and they say ope they got that acronym wrong.
yeah.
then they should send that through
why why
well it's sort of not really a question about permissions.
but more of procedure.
either those all go through jane or through someone.
uhhuh.
or they all don't.
but it's sort of
and if they all do then there isn't a problem.
right?
because once they give us the data it's ours.
and if they want to make changes
i mean ours to sort of transcribe and annotate.
the the the point here is that it would be nice for there to be one repository.
and if they want to make changes they can do that.
yeah.
yeah.
and if we don't let them modify their own data they're not going to store their data in our repository.
or if we make it inconvenient for them to change their data.
so then if they do have
i i'm just looking at it the other way around.
okay so what if they have what if they have accounts here and they use r c s?
so so let's say we
at that point where you can really lock a file.
i mean i'm worried if you can't lock a file this this to me sounds very scary.
huh.
um
and if they have accounts here and they're modifying it
if they're if they're so closely linked that they're actually modifying transcripts and key files then they could do it by you know secure shelling into icsi under r c s at that point.
well i don't see a real difference between r c s and any other system.
i mean it's just mechanisms.
the nice thing about c v s is that you can have multiple people modifying.
and if the changes don't overlap each other they can just do it.
huh
that's interesting.
so the problem with r c s is that if two people want to modify a file at the same time they can't.
yeah.
because the granularity of locking is at the file level.
what does c v s do if they if they do overlap?
uh there're lots of different policies for dealing with it.
uhhuh.
but typically a person has to uh mediate you know which changes actually get go through.
so
yeah.
oh.
yeah to decide which one is
oh.
yeah.
oh
so maybe there is a way in c v s to effectively lock something if you don't want people to make any changes?
at the repository level there certainly is.
uhhuh.
you can mark a file as as you can't check this out.
oh.
so
c v s is pretty good.
so then you could use c v s.
and you know just have remote access.
okay.
yep.
but then it's up to whoever is sort of responsible for that level of transcription to decide how and when to put these locks.
well it's it's just like source code.
or
right?
so that when you're developing it and it's really rough you don't put it in the repository.
yeah.
you wait to the point when you're ready to release it.
and then you put it in the repository.
yeah so the
uhhuh.
and if other people want to copy
the maybe the model
it's a little bit different in c v s.
when you check something out you actually create your own directory with copies of everything in it.
copy.
and that's what you work on.
so until you're ready to check it in nobody sees anything you've done.
and then when you check it in it puts sort of you know every change that you've done is goes into the central
right.
so if that's at
huh
i guess it depends on jane's
uh you know
if that's if that model works for the transcripts then that's fine.
but if that um allows someone to come in and modify while you're modifying and they turn out to be changes that you know would have been better to wait until your version came out then that's really up to you.
not up to the software.
i think it's a very good point.
because what if the uh what if one set of changes one person was making was with reference to some typographical convention?
uhhuh.
and the other person quite unknowingly was changing it the other way around.
then you could end up with a real mess.
in terms of like consistency of conventions across a file there is an argument to be made for having a single editor editing at that level.
but i like i like what liz is saying about
well yeah there would there would have to be somebody to enforce the consistency.
that's right.
and i also like i also like liz's uh um phraseology of at this level of i don't remember how you put it but this at this level of transcription or whatever.
yeah.
i mean it's like there are different layers of whatever.
and different types of conventions.
and if it were an acronym then i would i think that it would make sense for them to have input into that level of things.
but i wouldn't want them to be changing
i'm just
imagine that we reverse this.
and they were keeping the repository and we were collecting meetings and sending them to them.
yeah.
uhhuh.
if i found an error and i sent them email and said here's this error and i didn't get a response for a couple weeks i would stop sending them the my data.
and i would start collecting it here.
and we would end up with two corpora.
well
and i don't want that to happen.
yeah.
i agree.
so we have to make it convenient for them to make changes that they want to make.
well
well i see now we're back into the same situation as when we were uh talking about allowing people to edit the transcripts before they become public.
so there's a question of how easy or how hard to modify.
and how that impacts the amount of changing that's done.
uhhuh.
and i i think that it's um
i i have received some suggestions for change which i've made.
and um i think with an acronym that's a clear case where it would be an easy change to incorporate.
but it would worry me to have people um having free reign to change these things because of the need to have consistent conventions.
and people who are only just visiting the data might not have a sense of the larger system.
yeah that's what i would be worried about too.
well i think
yeah i think there are probably ways that you can say that you know only certain users are allowed to change things.
yeah.
uhhuh.
uh so we can probably restrict it to be
you know casual users who are just browsing can just get it read only.
can't check in to the archive.
uhhuh.
whereas we could say other people you know these people are allowed to actually submit changes.
right.
i mean i i think one thing that's probably everybody would agree on is that there will never be a point where you know we can say that for sure there's no problems with any of the transcripts.
i mean there's
uh you know something's going to get overlooked at some point.
or somebody's going to want to make a change somehow.
so we need to have some mechanism for um handling changes in the future.
i i agree but i have to also add one other thing which before i forget it.
which is adam's point about when you were reviewing the i b m transcripts.
this was mentioned at the beginning of the meeting.
you said that there were some places where it wasn't what was written already but you could see how they might have thought that's what it said.
so there it's not just a right and wrong thing.
sometimes there there are absolutely stretches where there is no no if you were at the meeting and you could ask the person.
yeah exactly.
uhhuh.
maybe you know they could they could've told you.
but five minutes later they might not have been able to tell you.
right.
even if they were listening to it.
so sometimes it isn't right and wrong.
yeah.
sometimes it's multiple interpretations.
and
good point.
yeah.
now but of course this does raise the possibility of allowing um an extra annotation for you know alternative interpretations of a stretch.
uhhuh.
but i just wanted to say it isn't simply right or wrong.
yeah that's a good point.
or overlooked or not overlooked.
sometimes they're truly ambiguous.
and um uh um
uhhuh.
nice to have a notation for that.
got to go.
but yeah.
yeah.
i mean i like that idea.
because even a casual user can always send email to whoever's in charge.
uhhuh.
and say you know we'd like these changes.
well
and you know hopefully we'll give them a response.
i just i don't
and if they really do it a lot and they say we're a casual user but we want a chance to change the transcripts then we can face that if it happens.
i don't see that this is
yep.
can you write unread on that?
but
yeah thank you.
i don't really see the people at u w that i know of right now making huge investing huge amounts of time in changing transcripts.
but i could be wrong.
probably not transcripts.
but it would be nice to have one mechanism for all the files.
and they're certainly going to want to change tools.
yeah.
and so you know it's the same as source code.
you you release source code at some point.
and either you have to do everything or you share responsibility.
yeah i mean a lot of the open source projects face this same problem.
right.
you know anybody can check out the code.
but not everybody can check it in.
right and there's some happy medium.
yeah.
and we don't know what that is yet until we get feedback from people.
but what if it's okay to just handle it with sort of a person in charge of the philosophy behind the changes?
and some people with permissions maybe by request to make changes.
that we don't just give people permissions if they're not going to make changes.
because i've overwritten a a file by mistake.
not wanting to have done that.
because i didn't think i had permission when i did.
uhhuh.
um and then just seeing if that is enough to to handle the transcript changes.
uhhuh.
i'm just worried about letting everybody go in and make changes.
because it's real easy when you're trying to i don't know run alignments and there's a word you want to fix to go in and do that and then mess up other things.
if you don't know you know the overall philosophy behind the the conventions.
uhhuh.
right.
then there is also the possibility remember in c v s when we talk about making changes there's sort of two types that can be made.
and it
uhhuh.
you can back everything out.
to your local or to the global.
uhhuh.
and so people can make changes to their local.
right.
and if they screw those up that's you know in their that's only on them.
exactly.
uhhuh.
it's it's the checking in part that we really care about.
and that we can control with you know who can check things in and stuff.
so we can i think we can do this.
or just yeah start by make it a really tight control.
uhhuh.
uhhuh.
and then as people really need the control you can ascertain whether or not to
yeah they can still check it out
yeah because they can still check out full copies and make whatever changes they want to their local copies.
right.
and it won't affect the the the global official ones at all.
right.
right but what i want to avoid is ending up with two corpora.
yeah.
yes.
definitely.
and so if you make it too hard for them to check back in they'll check it out once.
yep.
they'll make all the changes.
they'll never tell us about the changes.
uhhuh.
and we'll get two different corpora.
uhhuh.
right?
i think we're already going to i mean there's already some chance that different annotations different places you know
but you can control that by knowing you're making two corpora.
or knowing that you're adding um annotations on one version and you don't have the latest corrections maybe at that point.
and then you finish the project and you realize that there were corrections made on your originals.
and then you have to merge them.
and the the thing that makes it okay to do that is knowing where the synch time boundaries are.
because you can automatically pretty much merge things if you've only got twenty words or so in an utterance.
it's when you get the whole meeting.
and the synch times have changed.
or you can't correspond to a previous version with synch times that you get in trouble.
so
was there anything else on your list?
nope.
shall we do some digits?
now wait are are we doing them simultaneously or one at a time?
i wasn't planning on doing it simultaneously.
all right.
lee you liked that
huh?
oop ouch!
ooo!
okay.
did you solve speech recognition last week?
almost.
all right.
let's do image processing.
yes again.
great.
we did it again morgan.
all right!
doo doop doo doo.
what's wrong with
okay.
it's april fifth.
actually hynek should be getting back in town shortly if he isn't already.
is he going to come here?
uh
well we'll drag him here.
i know where he is.
so when you said in town you mean oregon.
uh i meant you know this end of the world.
oh.
yeah is really what i meant.
doo doo doo.
uh because he's been in europe.
doo doo.
so
i have something just fairly brief to report on.
huh.
um i did some uh uh just a few more experiments before i had to uh go away for the well that week.
great!
was it last week or whenever?
um
so what i was started playing with was the again this is the h t k back end.
and um
i was curious because the way that they train up the models they go through about four sort of rounds of of training.
and in the first round they do uh i think it's three iterations.
and for the last three rounds they do seven iterations of re estimation in each of those three.
and so
you know that's part of what takes so long to train the the the back end for this.
i'm sorry.
i didn't quite get that.
there's there's four and there's seven.
and
i i'm sorry.
yeah.
uh
maybe i should write it on the board.
so there's four rounds of training.
um
i i i guess you could say iterations.
the first one is three then seven seven and seven.
and what these numbers refer to is the number of times that the uh h m m re estimation is run.
it's this program called h e rest.
but in h t k what's the difference between uh a an inner loop and an outer loop in these iterations?
okay.
so what happens is um at each one of these points you increase the number of gaussians in the model.
yeah.
oh right.
this was the mix up stuff.
yeah.
the mix up.
right.
that's right.
i remember now.
and so in the final one here you end up with uh for all of the the digit words you end up with uh three mixtures per state.
yeah.
uh in the final thing.
so i had done some experiments where i was i i want to play with the number of mixtures.
uhhuh.
but um
uh one two.
uh i wanted to first test to see if we actually need to do this many iterations early on.
uhhuh.
and so
um i i ran a couple of experiments where i reduced that to to be three two two uh five i think.
and i got almost the exact same results.
and but it runs much much faster.
uhhuh.
so um
i i think it only took something like uh three or four hours to do the full training.
as opposed to
good.
as opposed to what sixteen hours or something like that?
yeah it depends.
i mean it takes you have to do an overnight basically the way it is set up now.
uhhuh.
uhhuh.
so
uh
even we don't do anything else doing something like this could allow us to turn experiments around a lot faster.
and then when you have your final thing do a full one so it's
and when you have your final thing we go back to this.
yeah.
so um
and it's a real simple change to make.
i mean it's like one little text file you edit and change those numbers.
and you don't do anything else.
oh this is a
and then you just run.
uhhuh.
okay.
so it's a very simple change to make.
and it doesn't seem to hurt all that much.
so you you run with three two two five that's
so i
uh i i have to look to see what the exact numbers were.
yeah.
i i thought was like three two two five.
but i'll i'll double check.
uhhuh.
it was over a week ago that i did it.
okay.
oh.
so i can't remember exactly.
uhhuh.
but uh
uhhuh.
um but it's so much faster.
huh.
it makes a big difference.
so we could do a lot more experiments and throw a lot more stuff in there.
yeah.
that's great.
um
oh the other thing that i did was um i compiled the h t k stuff for the linux boxes.
so we have this big thing that we got from i b m.
which is a five processor machine.
really fast.
but it's running linux.
so you can now run your experiments on that machine.
and you can run five at a time.
and it runs uh as fast as you know uh five different machines.
uhhuh.
uhhuh.
so um
i've forgotten now what the name of that machine is.
but i can i can send email around about it.
yeah.
and so we've got it.
now h t k's compiled for both the linux and for um the sparcs.
um you have to make you have to make sure that in your dot c s h r c um it detects whether you're running on the linux or a a sparc and points to the right executables.
uh
and you may not have had that in your dot c s h r c before if you were always just running the sparc.
so
um
uh i can i can tell you exactly what you need to do to get all of that to work.
uhhuh.
but it'll it really increases what we can run on.
huh cool.
so together with the fact that we've got these faster linux boxes and that it takes less time to do these um we should be able to crank through a lot more experiments.
uhhuh.
so
huh.
so after i did that then what i wanted to do was try increasing the number of mixtures.
just to see um see how how that affects performance.
yeah.
so
yeah in fact you could do something like keep exactly the same procedure and then add a fifth thing onto it.
uhhuh.
exactly.
that had more.
yeah.
right.
right.
so at at the middle where the arrows are showing that's you're adding one more mixture per state?
uhhuh.
or
uh
let's see.
uh
it goes from
this uh try to go it backwards this at this point it's two mixtures per state.
so this just adds one.
except that uh actually for the silence model it's six mixtures per state.
uhhuh.
okay.
uh so it goes to two.
um
and i think what happens here is
might be between uh shared uh shared variances or something.
yeah i think that's what it is.
or
uh
yeah it's uh
shoot!
i i i can't remember now what happens at that first one.
uh i have to look it up and see.
oh okay.
um
there because they start off with uh an initial model.
which is just this global model.
and then they split it to the individuals.
and so it may be that that's what's happening here.
i i i have to look it up and see.
i i don't exactly remember.
okay.
okay.
so that's it.
all right.
so what else?
um
yeah.
there was a conference call this tuesday.
um
i don't know yet the what happened tuesday.
but the points that they were supposed to discuss is still uh things like the weights.
uh
oh this is a conference call for uh uh aurora participant sort of thing.
for
yeah.
i see.
yeah.
huh
do you know who was who was since we weren't in on it uh do you know who was in from o g i?
was was was hynek involved?
or was it sunil?
i have no idea.
or
oh you don't know.
huh i just
okay.
yeah.
all right.
um
yeah.
so the points were the the weights how to weight the different error rates that are obtained from different language and and conditions.
um
it's not clear that they will keep the same kind of weighting.
right now it's a weighting on on improvement.
uhhuh.
some people are arguing that it would be better to have weights on
uh
well to to combine error rates before computing improvement.
uh and the fact is that for right now for the english they have weights.
they they combine error rates.
but for the other languages they combine improvement.
so it's not very consistent.
uhhuh.
um
yeah.
the um
yeah.
and so
well this is a point.
and right now actually there is a thing also uh that happens with the current weight is that a very non significant improvement on the well matched case result in huge differences in in the final number.
uhhuh.
huh.
and so perhaps they will change the weights to
yeah.
how should that be done?
i mean it it seems like there's a simple way.
uhhuh.
uh this seems like an obvious mistake or something.
they're
well i mean the fact that it's inconsistent is an obvious mistake.
but the but um the other thing
i don't know.
i haven't thought it through.
but one one would think that each it
it's like if you say what's the what's the best way to do an average.
an arithmetic average or a geometric average?
uhhuh.
it depends what you want to show.
uhhuh.
each each one is going to have a different characteristic.
yeah.
so
well it seems like they should do like the percentage improvement or something.
rather than the absolute improvement.
that's what they do.
well they are doing that.
yeah.
no that is relative.
but the question is do you average the relative improvements or do you average the error rates and take the relative improvement maybe of that?
yeah.
yeah.
and the thing is it's not just a pure average because there are these weightings.
oh.
it's a weighted average.
um
yeah.
and so when you average the the relative improvement it tends to to give a lot of of um importance to the well matched case.
because the baseline is already very good.
and
um
why don't they not look at improvements but just look at your your scores?
it's
you know figure out how to combine the scores.
uhhuh.
with a weight or whatever.
and then give you a score.
here's your score.
and then they can do the same thing for the baseline system.
and here's its score.
and then you can look at
uhhuh.
well that's what he's seeing as one of the things they could do.
it's just when you when you get all done i think that they
yeah.
i i i wasn't there.
but i think they started off this process with the notion that you should be significantly better than the previous standard.
uhhuh.
and um
so they said how much is significantly better.
what do you
and and so they said well you know you should have half the errors or something that you had before.
uhhuh.
huh.
uhhuh.
yeah.
so it's
uh
huh.
but it does seem like
it does seem like it's more logical to combine them first.
and then do the
combine error rates.
yeah.
and then
yeah.
yeah.
well
but there is this this is this still this problem of weights.
when when you combine error rate it tends to give more importance to the difficult cases.
oh yeah?
and some people think that
well they have different um opinions about this.
some people think that it's more important to look at to have ten percent relative improvement on well matched case than to have fifty percent on the mismatched.
and other people think that it's more important to improve a lot on the mismatch.
and
it sounds like they don't really have a good idea about what the final application is going to be.
so
fff!
huh.
well you know the the thing is that if you look at the numbers on the on the more difficult cases um if you really believe that was going to be the predominant use none of this would be good enough.
yeah.
huh.
uhhuh.
yeah.
nothing anybody's
whereas you sort of with some reasonable error recovery could imagine in the better cases that these these systems working.
so
um
i think the hope would be that it would uh it would work well for the good cases.
and uh it would have reasonable soft degradation as you got to worse and worse conditions.
um
yeah.
i i guess what i'm
i mean i i was thinking about it in terms of if i were building the final product.
and i was going to test to see which front end i'd i wanted to use.
i would try to weight things depending on the exact environment that i was going to be using the system in.
if i
but but no.
well no.
well no.
i mean it isn't the operating theater.
i mean they they they don't they don't really know i think.
yeah.
i mean i
so if if they don't know doesn't that suggest the way for them to go?
uh
you assume everything's equal.
i mean i mean you
well i mean i i think one thing to do is to just not rely on a single number.
to maybe have two or three numbers.
yeah.
right.
you know?
and and and say here's how much you uh you improve the uh the the relatively clean case.
and here's
or or well matched case.
and here's how here's how much you
uhhuh.
uh
so not try to combine them.
so
yeah.
uh actually it's true.
yeah.
uh i had forgotten this.
uh but uh well matched is not actually clean.
what it is is just that
the training and testing.
uh the training and testing are similar.
huh.
so
i guess what you would do in practice is you'd try to get as many uh examples of similar sort of stuff as you could.
yeah.
and then
uh
so the argument for that being the the the more important thing is that you're going to try and do that but you want to see how badly it deviates from that when when when the uh it's a little different.
so
um
so you should weight those other conditions very you know really small.
but
no.
that's a that's a that's an
i mean that's more of an information kind of thing.
that's an
well that's an argument for it.
but let me give you the opposite argument.
uhhuh.
the opposite argument is you're never really going to have a good sample of all these different things.
i mean are you going to have uh uh examples with the windows open?
half open?
full open?
going seventy sixty fifty forty miles an hour.
on what kind of roads.
uhhuh.
with what passing you.
with
uh i mean
uhhuh.
i i i think that you could make the opposite argument that the well matched case is a fantasy.
uhhuh.
uhhuh.
you know?
so
i think the thing is is that if you look at the well matched case versus the you know the the medium and the and the and then the mismatched case um we're seeing really really big differences in performance.
right?
and and you wouldn't like that to be the case.
you wouldn't like that as soon as you step outside
you know a lot of the the cases it's is
well that'll teach them to roll their window up.
i mean in these cases if you go from the the uh
i mean i don't remember the numbers right off.
but if you if you go from the well matched case to the medium it's not an enormous difference in the in the the training testing situation.
and and and it's a really big performance drop.
uhhuh.
you know?
so
um
yeah i mean the reference one for instance this is back old on uh on italian uh was like six percent error for the well matched.
and eighteen for the medium matched.
and sixty for the for highly mismatched.
uh
and you know with these other systems we we helped it out quite a bit.
but still there's there's something like a factor of two or something between well matched and medium matched.
and so i think that if what you're if the goal of this is to come up with robust features it does mean
so you could argue in fact that the well matched is something you shouldn't be looking at at all.
that that the goal is to come up with features that will still give you reasonable performance.
you know with again gentle degradation.
um even though the the testing condition is not the same as the training.
huh.
so you know i i could argue strongly that something like the medium mismatch which is you know not pathological but
i mean what was the the medium mismatch condition again?
um it's
yeah.
medium mismatch is everything with the far microphone.
but trained on like low noisy condition.
like low speed.
and or stopped car.
and tested on high speed conditions i think.
like on a highway.
and
right.
so it's still the same same microphone in both cases.
so
same microphone.
but
yeah.
but uh it's there's a mismatch between the car conditions.
and that's
uh you could argue that's a pretty realistic situation.
yeah.
uhhuh.
and uh i'd almost argue for weighting that highest.
but the way they have it now it's i guess it's it's
they they compute the relative improvement first.
and then average that with a weighting?
yeah.
and so then the that that makes the highly matched the really big thing.
uhhuh.
um
so since they have these three categories it seems like the reasonable thing to do is to go across the languages and to come up with an improvement for each of those.
uhhuh.
just say okay in the in the highly matched case this is what happens.
in the the uh this other medium if this happens.
in the highly mismatched that happens.
uhhuh.
and uh
you should see uh a gentle degradation through that.
huh.
um
but
i don't know.
yeah.
i think that that
i i
i gather that in these meetings it's it's really tricky to make anything make any policy change.
because everybody has has uh their own opinion.
and
uhhuh.
i don't know.
yeah.
yeah.
uh so
yeah.
yeah.
but there is probably a a big change that will be made.
is that the the baseline they want to have a new baseline perhaps.
which is um m f c c.
but with a voice activity detector.
and apparently uh some people are pushing to still keep this fifty percent number.
so they want to have at least fifty percent improvement on the baseline.
uhhuh.
but which would be a much better baseline.
uhhuh.
and if we look at the result that sunil sent just putting the v a d in the baseline improved like more than twenty percent.
uhhuh.
which would mean then then mean that fifty percent on this new baseline is like well more than sixty percent improvement on
so nobody would be there probably right?
right now nobody would be there.
but
good.
yeah.
work to do.
uhhuh.
so whose v a d?
is is is this a
uh they didn't decide yet.
i guess this was one point of the conference call also.
but
huh
so i don't know.
um
but
yeah.
oh.
oh i i think that would be good.
i mean it's not that the design of the v a d isn't important.
but it's just that it it it does seem to be uh a lot of work to do a good job on on that.
and as well as being a lot of work to do a good job on the feature design.
yeah.
so
yeah.
if we can cut down on that maybe we can make some progress.
yeah.
huh.
but i guess perhaps
i don't know.
yeah.
uh yeah.
someone told that perhaps it's not fair to do that because the um to make a good v a d you don't have enough to with the the features that are the baseline features.
so
huh
you need more features.
so you really need to put more more in the in in the front end.
yeah.
so
um
sure.
wait a minute.
but
i i'm confused.
yeah.
what do you mean?
yeah.
so so you
if
yeah.
but
well let's say for
see m f c c for instance doesn't have anything in it uh related to the pitch.
so just just for example.
so suppose you've that what you really want to do is put a good pitch detector on there.
and if it gets an unambiguous
oh.
oh i see.
uhhuh.
if it gets an unambiguous result then you're definitely in a in a in a in a uh region with speech.
so there's this assumption that the the voice activity detector can only use the m f c c?
uh
that's not clear.
but this
well for the baseline.
yeah.
so so if you use other features then
but it's just a question of what is your baseline.
right?
i
what is it that you're supposed to do better than.
yeah.
and so
i don't
having the baseline be the m f c c's means that people could choose to pour their their effort into trying to do a really good v a d.
but they seem like two separate issues.
or
right?
i mean
they're sort of separate.
unfortunately there's coupling between them.
which is part of what i think stephane is getting to is that you can choose your features in such a way as to improve the v a d.
yeah.
and you also can choose your features in such a way as to prove improve recognition.
but it seems like you should do both.
they may not be the same thing.
right?
you should do both.
and and i i think that this still makes i still think this makes sense as a baseline.
it's just saying as a baseline we know
huh.
you know we had the m f c c's before.
lots of people have done voice activity detectors.
uhhuh.
you might as well pick some voice activity detector and make that the baseline.
just like you picked some version of h t k and made that the baseline.
yeah.
right.
and then let's try and make everything better.
um
and if one of the ways you make it better is by having your features be better features for the v a d then that's so be it.
but
uhhuh.
uh uh uh at least you have a starting point that's
um
because some of the some of the people didn't have a v a d at all i guess.
right?
and and
yeah.
then they they looked pretty bad.
and and in fact what they were doing wasn't so bad at all.
uhhuh.
but
uhhuh.
yeah it seems like you should try to make your baseline as good as possible.
um
and if it turns out that you can't improve on that well i mean then you know nobody wins and you just use m f c c.
right?
yeah.
i mean it seems like
uh
it should include sort of the current state of the art that you want are trying to improve.
and m f c c's you know or p l p or something it seems like reasonable baseline for the features.
and anybody doing this task uh is going to have some sort of voice activity detection at some level in some way.
they might use the whole recognizer to do it but rather than a separate thing.
but but they'll have it on some level.
so
um
it seems like whatever they choose they shouldn't you know purposefully brain damage a part of the system to make a worse baseline.
or
you know?
well i think people just
it wasn't that they purposely brain damaged it.
i think people hadn't really thought through about the uh the v a d issue.
huh.
uhhuh.
and and then when the the the proposals actually came in and half of them had v a d's and half of them didn't.
and the half that did did well.
and the half that didn't did poorly.
uhhuh.
so it's
uhhuh.
um
uh
yeah.
so
we'll see what happen with this.
and
yeah.
so what happened since um last week is
well from o g i these experiments on putting v a d on the baseline.
and these experiments also are using uh some kind of noise compensation.
so spectral subtraction.
and putting on line normalization um just after this.
so i think spectral subtrac tion l d a filtering and on line normalization.
so which is similar to the proposal one but with spectral subtraction in addition.
and it seems that on line normalization doesn't help further when you have spectral subtraction.
is this related to the issue that you brought up a couple of meetings ago with the the musical tones?
i
and
i have no idea.
because the issue i brought up was with a very simple spectral subtraction approach.
huh.
and the one that they use at o g i is one from from the proposed the the the aurora uh proposals.
which might be much better.
so yeah.
i asked sunil for more information about that.
but uh
i don't know yet.
um
and what's happened here is that we
so we have this kind of new um reference system which use a nice a a clean downsampling upsampling.
which use a new filter that's much shorter.
and which also cuts the frequency below sixty four hertz.
right.
which was not done on our first proposal.
when you say we have that does sunil have it now too?
or
no.
no.
okay.
because we're still testing.
so we have the result for uh just the features.
okay.
and we are currently testing with putting the neural network in the k l t.
um it seems to improve on the well matched case.
um but it's a little bit worse on the mismatch and highly mismatched.
i mean when we put the neural network.
and with the current weighting i think it's it will be better.
because the well matched case is better.
huh.
but how much worse since the weighting might change?
how how much worse is it on the other conditions?
when you say it's a little worse.
it's like uh fff fff um ten percent relative.
yeah.
okay.
um
uhhuh.
but it has the uh
the latencies are much shorter.
that's
when i say it's worse it's not it's when i i uh compare proposal two to proposal one.
so
uh putting neural network compared to not having any neural network.
uhhuh.
i mean this new system is is is better.
because it has um this sixty four hertz cut off.
uh clean downsampling.
and
um what else?
uh yeah a good v a d.
we put the good v a d.
so
yeah i don't know i i uh uh
but the latencies
but you've got the latency shorter now.
latency is short.
isn't it?
is
yeah.
yeah.
and
so it's better than the system that we had before.
yeah.
mainly because of the sixty four hertz and the good v a d.
okay.
and then i took this system and huh uh i we put the old filters also.
so we have this good system with good v a d.
with the short filter and with the long filter.
and
um
with the short filter it's not worse.
so
well is it?
okay.
so that's that's all fine.
it's in
but what you're saying is that when you do these
yes uh
so let me try to understand.
when when you do these same improvements to proposal one
uhhuh.
that uh on the
things are somewhat better uh in proposal two for the well matched case.
and somewhat worse for the other two cases.
yeah.
so does uh
when you say uh
so
the now that these other things are in there is it the case maybe that the additions of proposal two over proposal one are less important?
yeah.
probably yeah.
i get it.
um
so yeah.
uh
yeah but it's a good thing anyway to have shorter delay.
then we tried um to do something like proposal two.
but having um using also m s g features.
so there is this k l t part which use just the standard features.
uhhuh.
right.
and then two two neural networks.
uhhuh.
huh
and it doesn't seem to help.
um however we just have one result.
which is the italian mismatch.
so
uh
we have to wait for that to fill the whole table.
but
okay.
there was a start of some effort on something related to voicing or something?
is that
yeah.
um yeah.
so basically we try to uh find good features that could be used for voicing detection.
uh but it's still uh on the
um
oh well i have the picture.
we basically we are still playing with matlab to to look at at what happened.
what sorts of
yeah.
and
what sorts of features are you looking at?
we have some
so we would be looking at um the variance of the spectrum of the excitation.
uh um this this and this.
something like this.
which is should be high for voiced sounds.
uh
wait a minute.
i what does that mean?
we
the variance of the spectrum of excitation.
yeah.
so the
so basically the spectrum of the excitation for a purely periodic signal
okay.
yeah
what what you're calling the excitation as i recall is you're subtracting the the um the mel mel mel filter uh spectrum from the f f t spectrum.
that's right.
yeah.
right.
so
uhhuh.
yeah.
so we have the mel filter bank.
we have the f f t.
so we just
so it's it's not really an excitation.
but it's something that hopefully tells you something about the excitation.
no.
yeah that's right.
yeah.
yeah.
um
yeah.
we have here some histogram.
but they have a lot of overlap.
yeah.
but it's it's still
yeah.
so
well.
for unvoiced portion we have something that has a mean around o point three.
and for voiced portion the mean is o point fifty nine.
but the variance seem quite high.
how do you know
so
huh
how did you get your voiced and unvoiced truth data?
we used uh timit.
and we used canonical mappings between the phones .
yeah.
we uh use timit on this.
and
for
yeah.
but if we look at it in one sentence it apparently it's good.
i think.
yeah.
but
yeah.
uh so it's noisy timit.
that's right.
it's noisy timit.
yeah.
yeah.
it seems quite robust to noise.
so when we take we draw its parameters across time for a clean sentence and then the same noisy sentence it's very close.
uhhuh.
yeah so there are there is this.
there could be also the um something like the maximum of the auto correlation function.
or
is this a a a trained system?
which
or is it a system where you just pick some thresholds?
how does it work?
right now we just are trying to find some features.
uhhuh.
and
uh
yeah hopefully i think what we want to have is to put these features in some kind of
um
well to to obtain a statistical model on these features.
and to or just to use a neural network.
and hopefully these features would help.
because it seems like what you said about the mean of the the voiced and the unvoiced that seemed pretty encouraging.
uhhuh.
well yeah except the variance was big.
right?
yeah.
except the variance is quite high.
right?
well
well i i don't know that i would trust that so much.
yeah.
because you're doing these canonical mappings from timit labelings.
right?
uhhuh.
so
really that's sort of a cartoon picture about what's voiced and unvoiced.
so that could be giving you a lot of variance.
i mean
yeah.
it it may be that that you're finding something good.
and that the variance is sort of artificial because of how you're getting your truth.
uhhuh.
yeah.
but another way of looking at it might be that
i mean what we are coming up with feature sets after all.
so another way of looking at it is that um the mel mel spectrum mel cepstrum any of these variants um give you the smooth spectrum.
it's the spectral envelope.
by going back to the f f t you're getting something that is more like the raw data.
so the question is what characterization
and you're playing around with this.
another way of looking at it is what characterization of the difference between the raw data and this smooth version is something that you're missing that could help?
so i mean looking at different statistical measures of that difference.
coming up with some things and just trying them out.
and seeing if you add them onto the feature vector does that make things better or worse in noise.
where you're really just
the way i'm looking at it is not so much you're trying to find the best the world's best voiced unvoiced uh uh classifier.
uhhuh.
huh.
but it's more that you know uh uh try some different statistical characterizations of that difference back to the raw data.
right.
and and
right.
maybe there's something there that the system can use.
yeah.
yeah but more obvious is that
yeah.
the the more obvious is that that
well using the the f f t um you just it gives you just information about if it's voiced or not voiced mainly i mean.
but so
yeah.
this is why we we started to look by having sort of voiced phonemes.
well that's the
what i'm arguing is
yeah.
i mean uh what i'm arguing is that that that's you gives you your intuition.
and
uhhuh.
but in in reality it's you know there's all of this this overlap and so forth.
oh sorry.
and but what i'm saying is that may be okay.
because what you're really getting is not actually voiced versus unvoiced.
both for the the reason of the overlap and and then uh you know structural reasons.
uh uh like the one that chuck said.
that that in fact well the data itself is that you're working with is not perfect.
yeah.
uhhuh.
so what i'm saying is maybe that's not a killer.
because you're just getting some characterization.
one that's driven by your intuition about voiced unvoiced certainly.
uhhuh.
but it's just some characterization of something back in the in the in the almost raw data rather than the smooth version.
uhhuh.
and your intuition is driving you towards particular kinds of uh statistical characterizations of um what's missing from the spectral envelope.
uhhuh.
um obviously you have something about the excitation.
um
and what is it about the excitation.
and you know and you're not getting the excitation anyway you know.
so
so i i would almost take a
uh especially if if these trainings and so forth are faster i would almost just take a uh a scattershot at a few different ways of of characterizing that difference.
and uh you could have one of them but and and see you know which of them helps.
uhhuh.
so is the idea that you're going to take whatever features you develop and and just add them onto the future vector?
okay.
or what's the use of the the voiced unvoiced detector?
uh i guess we don't know exactly yet.
but um
yeah.
it's not part of a v a d system that you're doing?
no.
uh no.
oh.
okay.
no.
no the idea was i guess to to use them as as features.
features.
i see.
uh
yeah it could be uh it could be a neural network that does voiced and unvoiced detection.
uhhuh.
but it could be in the also the big neural network that does phoneme classification.
uhhuh.
huh
but each one of the mixture components
yeah.
i mean you have uh uh variance only.
so it's kind of like you're just multiplying together these um probabilities from the individual features within each mixture.
so it's
so
uh
i think it's a neat thing.
it seems you know
uh it seems like a good idea.
yeah.
um
yeah.
i mean i know that um people doing some robustness things a ways back were were just doing just being gross and just throwing in the f f t.
and actually it wasn't wasn't wasn't so bad.
uh so it would
and and you know that it's got to hurt you a little bit to not have a a spectral uh a a smooth spectral envelope.
so there must be something else that you get in return for that.
that uh
uhhuh.
uh
so
so how does
uh maybe i'm going in too much detail.
but how exactly do you make the difference between the f f t and the smoothed spectral envelope?
uh how is that uh
um we just
how did we do it up again?
uh we distend the we have the twenty three coefficient after the mel filter.
uhhuh.
and we extend these coefficient between the all the frequency range.
uhhuh.
and the interpolation between the point is give for the triangular filter the value of the triangular filter.
and of this way we obtained this this model speech.
so you essentially take the values that that you get from the triangular filter and extend them.
to sort of like a rectangle that's at that value.
yeah.
uhhuh.
yeah i think we have linear interpolation.
so we have we have one point for one energy for each filter bank.
huh yeah it's linear.
huh.
oh.
yeah.
which is the energy that's centered on on on the triangle.
at the
at the center of the filter.
so you you end up with a vector that's the same length as the f f t vector.
yeah.
yeah.
that's right.
and then you just uh compute differences.
yeah.
i have here one example if you if you want see something like that.
and
then we compute the difference.
yeah.
uhhuh.
uh sum the differences?
okay.
so
and i think the variance is computed only from like two hundred hertz to one to fifteen hundred.
oh okay.
uhhuh.
two two fifteen hundred.
uhhuh.
no.
because
right.
two hundred and fifty thousand.
fifteen hundred.
because
yeah.
yeah.
two thousand and fifteen hundred.
above um it seems that
well some voiced sound can have also like a noisy part on high frequencies.
and
yeah.
but
no it's makes sense to look at low frequencies.
well it's just
so this is uh basically this is comparing an original version of the signal to a smoothed version of the same signal?
yeah.
right.
so so this is
i mean you could argue about whether it should be linear interpolation or or or or zeroeth order.
but but
uhhuh.
at any rate something like this is what you're feeding your recognizer typically.
like which of the
no.
uh so the mel cepstrum is the is the is the cepstrum of this this uh spectrum or log spectrum.
so this is
yeah.
yeah.
right right.
whatever it
you're subtracting in in in power domain or log domain?
in log domain.
log domain.
yeah.
okay.
so it's sort of like division when you do the yeah the spectra.
yeah.
uh yeah.
it's the ratio?
um
yeah.
but anyway.
um
and that's
so what's uh what's the intuition behind this kind of a thing?
i i don't know really know the signal processing well enough to understand what what is that doing.
so
yeah.
yeah.
i guess that makes sense.
what happen if what we have have what we would like to have is some spectrum of the excitation signal.
yeah.
which is for voiced sound ideally.
a a pulse train.
uhhuh.
and for unvoiced it's something that's more flat.
uhhuh.
right.
and the way to do this is that
well we have the we have the f f t because it's computed in in the in the system.
and we have the mel filter banks.
uhhuh.
uhhuh.
and so if we if we like remove the mel filter bank from the f f t we have something that's close to the excitation signal.
okay.
it's something that's like a a train of a pulse train for voiced sound.
yeah.
oh okay.
yeah.
and that's that should be flat for
yeah.
i see.
so do you have a picture that
is this for a voiced segment?
it's
this picture?
yeah.
what does it look like for unvoiced?
yeah.
you have several some unvoiced?
the
no unvoiced i don't have.
for unvoiced.
oh.
yeah.
i'm sorry.
so you know all
but
yeah.
yeah.
yeah.
this is the between
this is another voiced example.
no.
yeah.
but it's this.
oh yeah.
this is
but between the frequency that we are considered for the excitation.
right.
uhhuh.
for the difference.
and this is the difference.
this is the difference.
yeah.
okay.
so of course it's around zero.
yeah.
sure looks
but
huh.
well.
huh.
no.
yeah.
it is
because we begin uh in fifteen point the fifteen point.
so
does does the periodicity of this signal say something about the the
fifteen
so it's
pitch.
yeah.
the pitch.
it's the pitch.
okay.
yeah.
yeah.
uhhuh.
that's like fundamental frequency.
uhhuh.
okay.
so i mean
i see.
i mean to first order what you'd what you're doing
i mean ignore all the details and all the ways which is that these are complete lies.
uhhuh.
uh the the you know what you're doing in feature extraction for speech recognition is you have uh in your head a a a a simplified production model for speech.
yeah.
uhhuh.
in which you have a periodic or aperiodic source that's driving some filters.
this is the the auto correlation the r zero energy.
do you have the mean
do you have the mean for the auto correlation?
uh first order for speech recognition you say i don't care about the source.
for
yeah.
i have the mean.
well i mean for the the energy.
right.
right?
right.
and so you just want to find out what the filters are.
yeah.
the filters roughly act like a um a uh an overall resonant you know some resonances and so forth that that's processing excitation.
here.
they should be more close.
uh no.
this is this.
more close is this.
and this.
uhhuh.
uhhuh.
yeah.
so they are
this is there is less difference.
uhhuh.
so if you look at the spectral envelope just the very smooth properties of it you get something closer to that.
this is less it's less robust.
less robust.
yeah.
oh yeah.
and the notion is if you have the full spectrum with all the little nitty gritty details that that has the effect of both.
yeah.
uhhuh.
and it would be a multiplication in in frequency domain.
so that would be like an addition in log power spectrum domain.
uhhuh.
uhhuh.
and so this is saying well if you really do have that sort of vocal tract envelope and you subtract that off what you get is the excitation.
and i call that lies because you don't really have that.
you just have some kind of signal processing trickery to get something that's kind of smooth.
it's not really what's happening in the vocal tract.
yeah.
so you're not really getting the vocal excitation.
right.
that's why i was going to the why i was referring to it in a more a more uh uh conservative way.
when i was saying well it's
yeah.
it's the excitation.
but it's not really the excitation.
it's whatever it is that's different between
oh.
so so standing back from that you sort of say there's this very detailed representation.
this moved in the
yeah.
uhhuh.
you go to a smooth representation.
you go to a smooth representation because this typically generalizes better.
uhhuh.
um
but whenever you smooth you lose something.
so the question is have you lost something you can you use.
right.
um probably you wouldn't want to go to the extreme of just saying okay our feature set will be the f f t.
because we really think we do gain something in robustness from going to something smoother.
uhhuh.
but maybe there's something that we missed.
yeah.
so what is it?
and then you go back to the intuition that
well you don't really get the excitation.
but you get something related to it.
uhhuh.
uhhuh.
and it and as you can see from those pictures you do get something that shows some periodicity uh in frequency.
huh?
you know.
and and and also in time.
so
that's that's really neat.
so
so you don't have one for unvoiced picture?
uh not here.
oh.
no i have
uhhuh.
yeah.
but not here.
but presumably you'll see something that won't have this kind of uh uh uh regularity in frequency uh in the
but
yeah well.
not here.
i would i would like to see those pictures.
well so.
yeah.
i can't see you now.
yeah.
yeah.
yeah.
uhhuh.
i don't have.
and so you said this is pretty
doing this kind of thing is pretty robust to noise?
it seems.
yeah.
pfft.
huh.
um
oops.
the mean is different with it.
because the the histogram for the the
no no no.
but the kind of robustness to noise
oh!
so if if you take this frame uh from the noisy utterance and the same frame from the clean utterance
huh.
you end up with a similar difference?
yeah.
we end up with
over here?
yeah.
okay.
i have here the same frame for the clean speech.
cool.
oh that's clean.
the same
oh okay.
but they are a difference.
because here the f f t is only with two hundred fifty six point.
yeah.
that's
and this is with five hundred twelve.
oh.
okay.
yeah.
this is kind of interesting also.
because if we use the standard uh frame length of of like twenty five milliseconds um what happens is that for low pitched voiced because of the frame length you don't really have you don't clearly see this periodic structure.
uhhuh.
because of the first lobe of of each each of the harmonics.
so this one is a longer uh
so this is like yeah fifty milliseconds or something like that.
fifty
yeah.
yeah but it's the same frame.
and
oh it's that time frequency trade off thing.
yeah.
right?
i see.
yeah.
so yeah.
uhhuh.
oh oh so this is this the difference here?
no.
this is the signal.
for that?
this is the signal.
i see that.
oh yeah.
the frame.
oh that's the the original.
this is the the original frame.
yeah.
so with a short frame basically you have only two periods.
yeah.
and it's not not enough to to have this kind of neat things.
uhhuh.
uhhuh.
yeah.
but
and here
no well.
yeah.
so probably we'll have to use like long long frames.
uhhuh.
uhhuh.
huh.
oh.
that's interesting.
huh.
yeah maybe.
well i mean it looks better.
but i mean the thing is if if uh if you're actually asking you know if you actually uh need to do place along an f f t it may be it may be pushing things.
yeah.
and and uh
would you would you want to do this kind of uh difference thing after you do spectral subtraction?
uh maybe.
no.
maybe we can do that.
huh
huh.
the spectral subtraction is being done
at what level is it being done?
at the level of f f t bins?
or at the level of uh mel spectrum or something?
um
i guess it depends.
i mean how are they doing it?
how they're doing it.
yeah.
um
i guess ericsson is on the um filter bank.
f f t filter bank.
no?
yeah.
it's on the filter bank.
so
so yeah.
probably
so in that case it might not make much difference at all.
it yeah.
seems like you'd want to do it on the f f t bins.
maybe.
i mean certainly it'd be better.
i mean if you were going to
uh for for this purpose that is.
yeah.
uhhuh.
yeah.
uhhuh.
okay.
huh.
what else?
uh yeah.
that's all.
so we'll perhaps try to convince o g i people to use the new the new filters.
and
yeah.
okay.
uh has has anything happened yet on this business of having some sort of standard uh source?
uh
or
not yet.
but i i will call them.
and
okay.
now they are i think they have more time.
because they have this
well eurospeech deadline is over.
when is the next um aurora deadline?
and
it's um in june.
june.
yeah.
early june?
late june?
middle june?
i don't know.
huh.
huh.
okay.
um
and he's been doing all the talking.
but but these he's he's uh
yeah.
this is this by the way a bad thing.
we're trying to get um more female voices in this record as well.
so
make make sure carmen talks as well.
uh but has he pretty much been talking about what you're doing also?
and
oh i i am doing this.
yeah yeah.
yes.
i don't know.
i'm sorry.
but
i think that for the recognizer for the meeting recorder that it's better that i don't speak.
yeah well.
because
you know uh we'll get we'll get to uh spanish voices sometime.
and we do we want to recognize uh you too.
after the after uh the result for the t i digits on the meeting record there will be foreigns people.
yeah but
oh no.
we like we we're we're
we are we're in the uh bourlard-hermansky-morgan uh frame of mind.
yeah we like high error rates.
it's
yeah.
that way there's lots of work to do.
so it's
uh
anything to
um not much is new.
talk about?
so when i talked about what i'm planning to do last time i said i was um going to use avendano's method of um using a transformation um to map from long analysis frames which are used for removing reverberation to short analysis frames for feature calculation.
and he has a trick for doing that involving viewing the d f t as a matrix.
um
but uh um i decided not to do that after all.
because i i realized to use it i'd need to have these short analysis frames get plugged directly into the feature computation somehow.
uhhuh.
and right now i think our feature computation is set to up to um take um audio as input in general.
so i decided that i i'll do the reverberation removal on the long analysis windows.
and then just re synthesize audio.
and then send that.
this is in order to use the s r i system or something.
um
right?
or
or even if i'm using our system i was thinking it might be easier to just re synthesize the audio.
yeah?
because then i could just feacalc as is.
and i wouldn't have to change the code.
oh okay.
yeah.
i mean it's
um
certainly in a short short term this just sounds easier.
uhhuh.
yeah.
i mean longer term if it's if it turns out to be useful one one might want to do something else.
right that's true.
but
uh uh i mean in in other words you you may be putting other kinds of errors in from the re synthesis process.
but
from the re synthesis.
um
yeah.
o okay.
i don't know anything about re synthesis.
uh how likely do you think that is?
uh it depends what you what you do.
i mean it's it's it's uh
um
don't know.
but anyway it sounds like a reasonable way to go for a for an initial thing.
and we can look at at exactly what you end up doing.
and and then figure out if there's some something that could be be hurt by the end part of the process.
okay.
okay.
so that's
that yeah that's it that's it.
that was it huh.
okay.
okay.
uhhuh.
um anything to add?
um
well i've been continuing reading.
i went off on a little tangent this past week.
um
looking at uh uh modulation spectrum stuff.
um
and and learning a bit about what what um what it is.
and uh the importance of it in speech recognition.
and i found some some uh neat papers um historical papers from um kanedera hermansky and arai.
yeah.
and they they did a lot of experiments where where um they take speech and um they modify the uh
they they they measure the relative importance of having different um portions of the modulation spectrum intact.
and they find that the the spectrum between one and sixteen hertz in the modulation is uh is important for speech recognition.
yeah.
um
sure.
i mean this sort of goes back to earlier stuff by drullman.
yeah.
and and uh the the m s g features were sort of built up with this notion.
right.
but i guess i thought you had brought this up in the context of um targets somehow.
right.
but
um
it's not i mean they're sort of not in the same kind of category as say a phonetic target or a syllabic target.
huh.
uhhuh.
or a
um i was thinking more like using them as as the inputs to to the detectors.
or a feature or something.
oh i see.
yeah.
well that's sort of what m s g does.
yeah.
right?
uhhuh.
so it's
but but uh
yeah.
yeah.
anyway we'll talk more about it later.
okay.
we can talk more about it later.
yeah.
yeah.
yeah.
yeah.
so maybe
should we do digits?
let's do digits.
let you you start.
oh okay.
